## Singular Value Decomposition para Matrizes Quadradas e Retangulares

### Introdução
Como vimos anteriormente [^1, 7], a **Singular Value Decomposition (SVD)** é uma ferramenta poderosa para diagonalizar mapas lineares, mesmo quando não é possível fazê-lo com uma única base ortonormal. Este capítulo expande o conceito de SVD para matrizes retangulares, demonstrando como qualquer matriz real $m \times n$ pode ser decomposta em termos de matrizes ortogonais e uma matriz diagonal. Este capítulo se baseia nos conceitos de **autovalores**, **autovetores**, **matrizes ortogonais**, **matrizes unitárias** e **decomposição polar** apresentados anteriormente [^1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16].

### Conceitos Fundamentais
O **Teorema da Decomposição em Valores Singulares** [^13, 14] (Teorema 22.7) generaliza o Teorema 22.5 [^8] para matrizes retangulares:

> Para cada matriz real $m \times n$, $A$, existem duas matrizes ortogonais $U$ ($n \times n$) e $V$ ($m \times m$) e uma matriz diagonal $m \times n$, $D$, tal que $A = VDU^T$, onde $D$ é da forma:

$$
D =
\begin{pmatrix}
\sigma_1 & & & & & \\
& \sigma_2 & & & & \\
& & \ddots & & & \\
& & & \sigma_r & & \\
& & & & 0 & \\
& & & & & \ddots \\
& & & & & & 0
\end{pmatrix}
$$
ou

$$
D =
\begin{pmatrix}
\sigma_1 & & & & 0 & \cdots & 0 \\
& \sigma_2 & & & 0 & \cdots & 0 \\
& & \ddots & & \vdots & & \vdots \\
& & & \sigma_r & 0 & \cdots & 0 \\
& & & & 0 & \cdots & 0 \\
& & & & \vdots & & \vdots \\
& & & & 0 & \cdots & 0
\end{pmatrix}
$$
onde $\sigma_1, \dots, \sigma_r$ são os *valores singulares* de $A$, ou seja, as raízes quadradas (positivas) dos autovalores não nulos de $A^T A$ e $A A^T$, e $\sigma_{r+1} = \dots = \sigma_p = 0$, onde $p = \min(m, n)$. As colunas de $U$ são autovetores de $A^T A$, e as colunas de $V$ são autovetores de $A A^T$ [^13].

**Prova**:

A prova segue a mesma linha do Teorema 22.5 [^8]. Como $A^T A$ é simétrica e positiva semidefinida, existe uma matriz ortogonal $U$ ($n \times n$) tal que:

$$A^T A = U \Sigma^2 U^T$$

onde $\Sigma = \text{diag}(\sigma_1, \dots, \sigma_r, 0, \dots, 0)$, onde $\sigma_1^2, \dots, \sigma_r^2$ são os autovalores não nulos de $A^T A$, e onde $r$ é o posto de $A$. Note que $r \leq \min\{m, n\}$, e $AU$ é uma matriz $m \times n$. Segue que:

$$U^T A^T A U = (AU)^T (AU) = \Sigma^2$$

Seja $f_j \in \mathbb{R}^m$ a $j$-ésima coluna de $AU$ para $j = 1, \dots, n$. Então:

$$\langle f_i, f_j \rangle = \sigma_i^2 \delta_{ij}, \quad 1 \leq i, j \leq r$$
e

$$f_j = 0, \quad r+1 \leq j \leq n$$

Definimos $(v_1, \dots, v_r)$ por:

$$v_j = \sigma_j^{-1} f_j, \quad 1 \leq j \leq r$$

Então:

$$\langle v_i, v_j \rangle = \delta_{ij}, \quad 1 \leq i, j \leq r$$

Completamos $(v_1, \dots, v_r)$ para uma base ortonormal $(v_1, \dots, v_r, v_{r+1}, \dots, v_m)$ (por exemplo, usando Gram-Schmidt). Agora, como $f_j = \sigma_j v_j$ para $j = 1, \dots, r$, temos:

$$\langle v_i, f_j \rangle = \sigma_j \langle v_i, v_j \rangle = \sigma_j \delta_{ij}, \quad 1 \leq i \leq m, 1 \leq j \leq r$$
e como $f_j = 0$ para $j = r+1, \dots, n$, temos:

$$\langle v_i, f_j \rangle = 0, \quad 1 \leq i \leq m, r+1 \leq j \leq n$$

Seja $V$ a matriz cujas colunas são $v_1, \dots, v_m$. Então $V$ é uma matriz ortogonal $m \times m$ e, se $m \geq n$, definimos:

$$
D =
\begin{pmatrix}
\Sigma \\
O_{m-n}
\end{pmatrix}
$$
onde $O_{m-n}$ é uma matriz de zeros de dimensão $(m-n) \times n$. Caso contrário, se $n \geq m$, definimos:

$$
D =
\begin{pmatrix}
\Sigma & O_{n-m}
\end{pmatrix}
$$
onde $O_{n-m}$ é uma matriz de zeros de dimensão $m \times (n-m)$. Em ambos os casos, as equações acima provam que:

$$V^T A U = D$$

o que implica $A = V D U^T$, como requerido. $\blacksquare$

A equação $A = VDU^T$ implica que [^9]:

$$A^T A = U D^T D U^T$$
$$A A^T = V D D^T V^T$$

o que mostra que $A^T A$ e $A A^T$ têm os mesmos autovalores não nulos, que as colunas de $U$ são autovetores de $A^T A$, e que as colunas de $V$ são autovetores de $A A^T$ [^9].

**Observações Adicionais:**

*   A matriz $D$ é *diagonal*, mesmo sendo $m \times n$, pois seus elementos não nulos estão na diagonal principal [^16].
*   A SVD pode ser calculada usando o comando `svd(A)` no MATLAB [^10, 16]. No entanto, o MATLAB usa a convenção $A = UDV^T$ [^16].

**Decomposição SVD para Matrizes Complexas**

Para uma matriz complexa $A$ ($n \times n$), a SVD é dada por [^10]:

$$A = VDU^*$$

onde $U$ e $V$ são matrizes unitárias, e $D$ é uma matriz diagonal consistindo de entradas reais não negativas $\sigma_1, \dots, \sigma_n$, onde $\sigma_1 \geq \dots \geq \sigma_r$ são os valores singulares de $A$, ou seja, as raízes quadradas positivas dos autovalores não nulos de $A^*A$ e $AA^*$, e $\sigma_{r+1} = \dots = \sigma_n = 0$ [^10].

### Conclusão
A SVD para matrizes retangulares fornece uma generalização poderosa da SVD para matrizes quadradas [^8, 13, 14]. Ela permite decompor qualquer matriz em termos de matrizes ortogonais e uma matriz diagonal contendo os valores singulares. Esta decomposição é fundamental em diversas aplicações, incluindo compressão de dados, solução de sistemas lineares sobredeterminados e análise de dados [^5]. Como visto, a SVD está intimamente relacionada com os conceitos de autovalores, autovetores e a decomposição polar, fornecendo uma visão unificada sobre a estrutura de mapas lineares [^1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16].

### Referências
[^1]: Capítulo 22 (completo).
[^2]: Seção 22.1.
[^3]: Proposição 22.1.
[^4]: Definição 22.1.
[^5]: Definição 22.2.
[^6]: Proposição 22.2.
[^7]: Seção 22.2.
[^8]: Teorema 22.5.
[^9]: Exemplo 22.4.
[^10]: Definição 22.3.
[^11]: Seção 22.3.
[^12]: Definição 22.4.
[^13]: Seção 22.4.
[^14]: Teorema 22.7.
[^15]: Exemplo 22.7.
[^16]: Seção 22.5.
<!-- END -->