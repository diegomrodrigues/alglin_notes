## Decomposição em Valores Singulares para Matrizes Quadradas

### Introdução
Este capítulo aprofunda a decomposição em valores singulares (SVD) para matrizes quadradas e retangulares, expandindo sobre a base estabelecida nos capítulos anteriores. A SVD é uma ferramenta poderosa para diagonalizar qualquer mapa linear, utilizando duas bases ortonormais [^1]. Exploraremos a existência e as propriedades dessas decomposições, com foco em matrizes reais e complexas. Em particular, investigaremos o teorema fundamental que garante a existência de matrizes ortogonais $U$ e $V$, e uma matriz diagonal $D$ tal que $A = VDU^T$ para matrizes reais, e $A = UH$ para matrizes complexas, onde $U$ é unitária e $H$ é hermitiana semidefinida positiva [^10].

### Conceitos Fundamentais

**Decomposição em Valores Singulares (SVD)**
A SVD é uma fatoração de uma matriz real ou complexa. Para uma matriz real $A_{n \times n}$, o teorema da decomposição em valores singulares (Teorema 22.5) afirma que existem matrizes ortogonais $U$ e $V$ e uma matriz diagonal $D$ tal que $A = VDU^T$ [^8]. A matriz $D$ contém os valores singulares de $A$ na sua diagonal. Os valores singulares são as raízes quadradas (positivas) dos autovalores não nulos de $A^TA$ e $AA^T$ [^8].

**Teorema 22.5 (Decomposição em Valores Singulares)**
*Para cada matriz real $n \times n$, $A$, existem duas matrizes ortogonais $U$ e $V$ e uma matriz diagonal $D$ tal que $A = VDU^T$, onde $D$ é da forma:*
$$
D = \begin{pmatrix}
\sigma_1 & & & \\
& \sigma_2 & & \\
& & \ddots & \\
& & & \sigma_n
\end{pmatrix}
$$
*onde $\sigma_1, \dots, \sigma_n$ são os valores singulares de $A$, ou seja, as raízes quadradas (positivas) dos autovalores não nulos de $A^TA$ e $AA^T$ e $\sigma_{r+1} = \dots = \sigma_n = 0$. As colunas de $U$ são autovetores de $A^TA$, e as colunas de $V$ são autovetores de $AA^T$ [^8].*

**Prova do Teorema 22.5**
A prova do Teorema 22.5 envolve o seguinte raciocínio: como $A^TA$ é uma matriz simétrica (e, de fato, uma matriz semidefinida positiva), existe uma matriz ortogonal $U$ tal que
$$
A^TA = UD^2U^T,
$$
onde $D = \text{diag}(\sigma_1, \dots, \sigma_r, 0, \dots, 0)$, e $\sigma_1^2, \dots, \sigma_r^2$ são os autovalores não nulos de $A^TA$, e $r$ é o posto de $A$ [^8]. Assim, $\sigma_1, \dots, \sigma_r$ são os valores singulares de $A$.

Seja $f_j$ a *j*-ésima coluna de $AU$ para $j = 1, \dots, n$. Então,
$$
(f_i, f_j) = \sigma_i^2 \delta_{ij}, \quad 1 \leq i, j \leq r,
$$
e
$$
f_j = 0, \quad r+1 \leq j \leq n.
$$
Definimos $v_1, \dots, v_r$ por
$$
v_j = \sigma_j^{-1} f_j, \quad 1 \leq j \leq r,
$$
então
$$
(v_i, v_j) = \delta_{ij}, \quad 1 \leq i, j \leq r,
$$
Completamos então $(v_1, \dots, v_r)$ numa base ortonormal $(v_1, \dots, v_r, v_{r+1}, \dots, v_n)$. Como $f_j = \sigma_j v_j$ para $j = 1, \dots, r$, temos
$$
(v_i, f_j) = \sigma_j (v_i, v_j) = \sigma_j \delta_{i,j}, \quad 1 \leq i \leq n, 1 \leq j \leq r,
$$
e como $f_j = 0$ para $j = r+1, \dots, n$, temos
$$
(v_i, f_j) = 0, \quad 1 \leq i \leq n, r+1 \leq j \leq n.
$$
Se $V$ é a matriz cujas colunas são $v_1, \dots, v_n$, então $V$ é ortogonal e as equações acima provam que $V^T AU = D$, o que implica $A = VDU^T$, como requerido [^9]. $\blacksquare$

**Observação:** A equação $A = VDU^T$ implica que $A^TA = UD^2U^T$ e $AA^T = VD^2V^T$, o que mostra que $A^TA$ e $AA^T$ têm os mesmos autovalores, que as colunas de $U$ são autovetores de $A^TA$, e que as colunas de $V$ são autovetores de $AA^T$ [^9].

**SVD para Matrizes Complexas**
O conceito de SVD também se estende a matrizes complexas. Para uma matriz complexa $A$, existem matrizes unitárias $U$ e $V$ e uma matriz diagonal $D$ com entradas reais não negativas, tal que $A = VDU^*$, onde $U^*$ denota a transposta conjugada de $U$ [^10]. Os elementos diagonais de $D$ são os valores singulares de $A$, que são as raízes quadradas dos autovalores de $A^*A$ e $AA^*$.

**Forma Polar**
Uma noção intimamente relacionada com a SVD é a forma polar de uma matriz. Para cada matriz real $n \times n$, $A$, existe uma matriz ortogonal $R$ e uma matriz simétrica semidefinida positiva $S$ tal que $A = RS$. Se $A$ é invertível, então $R$ e $S$ são únicos [^11].

Para o caso complexo, a decomposição polar afirma que, para cada matriz complexa $n \times n$, $A$, existe uma matriz unitária $U$ e uma matriz hermitiana semidefinida positiva $H$ tal que $A = UH$ [^11].

**Relação entre SVD e Forma Polar**
Dada uma decomposição SVD $A = VDU^T$, podemos obter a forma polar definindo $R = VU^T$ e $S = UDU^T$ [^11]. Então, $R$ é ortogonal, $S$ é simétrica semidefinida positiva, e $A = RS$. Reciprocamente, dada uma forma polar $A = R_1S$, onde $R_1$ é ortogonal e $S$ é simétrica semidefinida positiva, existe uma matriz ortogonal $R_2$ e uma matriz diagonal semidefinida positiva $D$ tal que $S = R_2DR_2^T$ [^12]. Assim, $A = R_1R_2DR_2^T = VDU^T$, onde $V = R_1R_2$ e $U = R_2$ são ortogonais.

### Conclusão
Neste capítulo, exploramos a decomposição em valores singulares (SVD) para matrizes quadradas, demonstrando o teorema fundamental que garante a existência dessa decomposição. Vimos como a SVD está relacionada com a forma polar de uma matriz, e como esses conceitos se estendem a matrizes complexas. A SVD é uma ferramenta essencial em diversas áreas, incluindo álgebra linear numérica, processamento de sinais e análise de dados [^5].

### Referências
[^1]: Capítulo 22, Singular Value Decomposition and Polar Form
[^5]: Page 5, The singular value decomposition (SVD).
[^8]: Page 8, Theorem 22.5. (Singular value decomposition)
[^9]: Page 9, Example 22.4.
[^10]: Page 10, (4) The SVD also applies to complex matrices.
[^11]: Page 11, 22.3 Polar Form for Square Matrices
[^12]: Page 12, Example 22.6.

<!-- END -->