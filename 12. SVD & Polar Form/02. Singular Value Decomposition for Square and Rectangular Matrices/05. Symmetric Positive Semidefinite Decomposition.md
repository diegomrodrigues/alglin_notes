## Decomposição de Matrizes Simétricas e Semidefinidas Positivas

### Introdução
Este capítulo explora a decomposição de matrizes simétricas e semidefinidas positivas, um conceito fundamental na álgebra linear e análise de dados. Especificamente, focaremos na decomposição $A = V\Sigma V^T$, onde $A$ é uma matriz simétrica e semidefinida positiva, $V$ é uma matriz ortogonal e $\Sigma$ é uma matriz diagonal com autovalores não negativos [^3, ^4, ^8]. Esta decomposição é um caso especial da decomposição em valores singulares (SVD), que é uma ferramenta poderosa para analisar e manipular matrizes [^1, ^5].

### Conceitos Fundamentais

**Matrizes Simétricas e Semidefinidas Positivas:**
Uma matriz $A$ é simétrica se $A = A^T$. Uma matriz simétrica $A$ é semidefinida positiva se $x^T A x \geq 0$ para todo vetor $x$. Uma propriedade importante é que os autovalores de uma matriz simétrica são reais [^1]. Além disso, os autovalores de uma matriz simétrica semidefinida positiva são não negativos [^1, ^3, ^4].

**Decomposição Espectral:**
O teorema espectral afirma que uma matriz simétrica $A$ pode ser diagonalizada por uma matriz ortogonal $V$. Isso significa que existe uma matriz ortogonal $V$ tal que $A = V\Sigma V^T$, onde $\Sigma$ é uma matriz diagonal contendo os autovalores de $A$ na diagonal [^8]. As colunas de $V$ são os autovetores ortonormais de $A$ [^8].

**Autovalores e Autovetores:**
Um autovetor $v$ de uma matriz $A$ é um vetor não nulo tal que $Av = \lambda v$, onde $\lambda$ é um escalar chamado autovalor [^1]. Os autovalores e autovetores desempenham um papel crucial na decomposição de matrizes.

**Decomposição $A = V\Sigma V^T$:**
Se $A$ é uma matriz simétrica e semidefinida positiva, então $A$ pode ser decomposta como $A = V\Sigma V^T$, onde:
- $V$ é uma matriz ortogonal cujas colunas são os autovetores ortonormais de $A$.
- $\Sigma$ é uma matriz diagonal cujos elementos diagonais são os autovalores não negativos de $A$.

**Prova:**
A prova se baseia no teorema espectral e na propriedade de que os autovalores de uma matriz simétrica semidefinida positiva são não negativos.
1.  Como $A$ é simétrica, pelo teorema espectral, existe uma matriz ortogonal $V$ tal que $A = V\Sigma V^T$, onde $\Sigma$ é uma matriz diagonal contendo os autovalores de $A$ [^8].
2.  Como $A$ é semidefinida positiva, todos os autovalores em $\Sigma$ são não negativos [^1, ^3, ^4].
3.  Portanto, a decomposição $A = V\Sigma V^T$ é válida, onde $V$ é uma matriz ortogonal e $\Sigma$ é uma matriz diagonal com autovalores não negativos. $\blacksquare$

**Exemplo:**
Considere a matriz $A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$. Esta matriz é simétrica e semidefinida positiva (verificação: $u^T A u > 0$ para todo $u \neq 0$). Os autovalores de $A$ são $\lambda_1 = 3$ e $\lambda_2 = 1$, e os autovetores correspondentes são $v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$ e $v_2 = \begin{bmatrix} -1 \\ 1 \end{bmatrix}$. Normalizando os autovetores, obtemos $V = \begin{bmatrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{bmatrix}$ e $\Sigma = \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix}$. Portanto, $A = V\Sigma V^T$.

**Singular Value Decomposition (SVD):**
A decomposição $A = V\Sigma V^T$ é um caso especial da SVD, onde a matriz $A$ é quadrada e simétrica [^1, ^5]. Em geral, a SVD de uma matriz $A$ é dada por $A = U\Sigma V^T$, onde $U$ e $V$ são matrizes ortogonais e $\Sigma$ é uma matriz diagonal com valores singulares não negativos [^5, ^8]. No caso de uma matriz simétrica e semidefinida positiva, $U = V$ e os valores singulares são iguais aos autovalores [^10].

### Conclusão

A decomposição de uma matriz simétrica e semidefinida positiva como $A = V\Sigma V^T$ é uma ferramenta poderosa na álgebra linear e análise de dados. Ela permite a diagonalização da matriz e facilita a análise de suas propriedades. Esta decomposição é um caso especial da SVD e está relacionada ao teorema espectral. O conhecimento desta decomposição é fundamental para diversas aplicações, como análise de componentes principais (PCA), compressão de dados e solução de sistemas lineares [^5, ^16].

### Referências
[^1]: Capítulo 22, Singular Value Decomposition and Polar Form.
[^3]: Proposition 22.1.
[^4]: Proposition 22.3.
[^5]: Singular Value Decomposition (SVD).
[^8]: Theorem 22.5.
[^10]: Remark (3).
[^16]: The SVD has applications to data compression, for instance in image processing.
<!-- END -->