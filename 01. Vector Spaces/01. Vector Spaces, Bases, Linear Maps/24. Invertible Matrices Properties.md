## Capítulo 3.6: Invertibilidade de Matrizes Quadradas e suas Propriedades Fundamentais

### Introdução

Expandindo os conceitos de espaços vetoriais, bases e transformações lineares introduzidos anteriormente, este capítulo foca em uma propriedade crucial das matrizes quadradas: a **invertibilidade**. Uma matriz quadrada $A$ é dita **invertível** ou **nonsingular** se existir uma outra matriz, denotada por $A^{-1}$, tal que seu produto resulte na matriz identidade $I_n$ [^14]. Formalmente, $A^{-1}$ é a **inversa** de $A$ se satisfaz a condição $AA^{-1} = A^{-1}A = I_n$ [^13, ^19]. A matriz identidade $I_n$ é uma matriz quadrada de dimensão $n$ com 1s na diagonal principal e 0s nas demais posições [^10, ^38]. Uma matriz que não possui inversa é chamada de **singular** [^15, ^20]. A invertibilidade está intrinsecamente ligada à capacidade de resolver sistemas de equações lineares da forma $Ax = b$ de maneira única [^17] e reflete propriedades fundamentais da transformação linear associada à matriz $A$ [^7]. Este capítulo explorará a definição formal de invertibilidade, suas condições equivalentes, e suas implicações, baseando-se exclusivamente nas discussões anteriores sobre combinações lineares, independência linear e matrizes.

### Conceitos Fundamentais

#### Definição Formal e Existência da Inversa

A definição central estabelece que uma matriz quadrada $A$ de dimensão $n$ é invertível se existe uma matriz $B$ tal que $AB = BA = I_n$ [^19]. Esta matriz $B$, quando existe, é única e é denotada por $A^{-1}$ [^13, ^19]. Uma questão fundamental é determinar sob quais condições essa inversa existe. A Proposição 3.13 estabelece um resultado importante sobre a existência de inversas laterais:

> **Proposição 3.13:** Se uma matriz quadrada $A \in M_n(K)$ possui uma inversa à esquerda, isto é, uma matriz $B$ tal que $BA = I_n$, ou uma inversa à direita, isto é, uma matriz $C$ tal que $AC = I_n$, então $A$ é de fato invertível. Além disso, $B = A^{-1}$ e $C = A^{-1}$ [^21].

*Prova.* Suponha que existe $B$ tal que $BA = I_n$. Considere a equação $Ax = 0$, onde $x \in K^n$ é um vetor coluna [^39]. Multiplicando ambos os lados à esquerda por $B$, obtemos $BAx = B0$, o que implica $I_n x = 0$, ou seja, $x = 0$ [^40]. Isso demonstra que a única solução para $Ax = 0$ é o vetor nulo. Como $Ax = x_1A^1 + \dots + x_nA^n$ [^8, ^40], a condição $Ax = 0 \implies x = 0$ é equivalente à independência linear das colunas $(A^1, \dots, A^n)$ de $A$ [^31]. Como vimos na Proposição 3.14 (a ser discutida em breve, mas cuja prova parcial está aqui implícita), a independência linear das colunas de uma matriz quadrada $A$ implica que $A$ é invertível [^28, ^29]. De forma mais direta, se as colunas de $A$ são linearmente independentes, elas formam uma base para $K^n$ [^40]. Consequentemente, para cada vetor da base canônica $e_j$ (onde $e_j$ tem 1 na $j$-ésima posição e 0 nas demais), existe um único vetor coluna $x^j$ tal que $Ax^j = e_j$ [^40]. Construindo a matriz $X = (x^1 \dots x^n)$ cujas colunas são $x^j$, temos $AX = I_n$ [^40]. Assim, $A$ possui uma inversa à direita $X$. Tendo uma inversa à esquerda $B$ e uma inversa à direita $X$, por uma propriedade básica (associatividade ou Proposição 2.2 referenciada no texto [^25]), temos $B = B(AX) = (BA)X = I_n X = X$. Portanto, $B=X$, e $A$ é invertível com $A^{-1} = B = X$ [^25].
Agora, suponha que existe $C$ tal que $AC = I_n$. Podemos aplicar o raciocínio anterior à matriz $C$, com $A$ atuando como sua inversa à direita. Pela primeira parte da prova, isso implica que $C$ é invertível e sua inversa é $A$. Como a inversa de $C$ (que é $A$) também é invertível, concluímos que $A$ é invertível e $A^{-1} = (C^{-1})^{-1} = C$ [^26, ^40]. $\blacksquare$

Esta proposição é poderosa, pois garante que a verificação de apenas uma das condições ($AB=I_n$ ou $BA=I_n$) é suficiente para assegurar a invertibilidade de uma matriz quadrada.

#### Critérios de Invertibilidade

A invertibilidade de uma matriz quadrada $A$ pode ser caracterizada por várias condições equivalentes, fundamentais tanto na teoria quanto na prática. O contexto fornecido destaca duas equivalências principais.

**Invertibilidade e Independência Linear das Colunas:** A relação mais direta explorada no contexto é entre a invertibilidade e a independência linear das colunas da matriz.

> **Proposição 3.14:** Uma matriz quadrada $A \in M_n(K)$ é invertível se, e somente se, suas colunas $(A^1, \dots, A^n)$ são linearmente independentes [^28].

*Prova.* Se $A$ é invertível, ela possui uma inversa à esquerda $A^{-1}$ [^13]. Como mostrado na prova da Proposição 3.13, a existência de uma inversa à esquerda $B=A^{-1}$ tal que $BA=I_n$ implica que a única solução para $Ax=0$ é $x=0$, o que por sua vez significa que as colunas $(A^1, \dots, A^n)$ são linearmente independentes [^22, ^29, ^40].
Reciprocamente, assuma que as colunas $(A^1, \dots, A^n)$ de $A$ são linearmente independentes [^29]. Como $A$ tem $n$ colunas linearmente independentes em $K^n$, essas colunas formam uma base para $K^n$ (pelo Teorema 3.11, qualquer conjunto de $n$ vetores linearmente independentes em um espaço de dimensão $n$ é uma base [^34]). Como as colunas formam uma base, a transformação linear $x \mapsto Ax$ é uma bijeção de $K^n$ para $K^n$. Em particular, existe uma (única) matriz $X$ tal que $AX = I_n$ (como construído na prova da Proposição 3.13 [^40]). Pela Proposição 3.13, a existência de uma inversa à direita $X$ implica que $A$ é invertível [^21, ^29]. $\blacksquare$

Esta proposição estabelece uma ponte direta entre a álgebra das matrizes (invertibilidade) e a geometria dos vetores coluna (independência linear), um conceito central no Capítulo 3 [^1].

**Invertibilidade e a Solução Nula:** Outro critério fundamental conecta a invertibilidade à natureza das soluções do sistema homogêneo $Ax=0$.

> **Proposição 3.15:** Uma matriz quadrada $A \in M_n(K)$ é invertível se, e somente se, para qualquer $x \in K^n$, a equação $Ax = 0$ implica que $x = 0$ [^30].

*Prova.* A condição "$Ax = 0$ implica $x = 0$" é, por definição, a condição de que as colunas $(A^1, \dots, A^n)$ de $A$ são linearmente independentes [^31], pois $Ax = x_1A^1 + \dots + x_nA^n$ [^8, ^41]. Pela Proposição 3.14, a independência linear das colunas de $A$ é equivalente à invertibilidade de $A$ [^28, ^31]. Portanto, $A$ é invertível se, e somente se, $Ax=0$ implica $x=0$ [^41]. $\blacksquare$

Este critério é particularmente útil para verificar a invertibilidade sem calcular explicitamente a inversa ou o determinante. Ele conecta a invertibilidade ao **kernel** (ou nullspace) da transformação linear representada por $A$; a matriz é invertível se e somente se o kernel contém apenas o vetor nulo [^44].

O contexto também menciona a equivalência com o determinante não nulo [^2, ^16]: *Uma matriz quadrada é invertível sse suas colunas são linearmente independentes sse seu determinante é não nulo* [^16]. Embora o desenvolvimento detalhado das propriedades do determinante não esteja presente no contexto fornecido além do cálculo inicial [^2], esta é uma terceira caracterização fundamental da invertibilidade.

#### Aplicações e Propriedades Adicionais

**Solução de Sistemas Lineares:** Uma das aplicações primárias de matrizes invertíveis é na resolução de sistemas lineares.

> *Em resumo, se A é uma matriz quadrada invertível, então o sistema linear Ax = b possui a solução única x = A⁻¹b* [^17].

Isso decorre diretamente da definição da inversa: se $Ax=b$, multiplicando à esquerda por $A^{-1}$ obtemos $A^{-1}(Ax) = A^{-1}b$, que simplifica para $(A^{-1}A)x = A^{-1}b$, e finalmente $I_n x = x = A^{-1}b$ [^11, ^13]. É importante notar, como mencionado no contexto, que calcular $A^{-1}$ diretamente para resolver o sistema pode ser computacionalmente caro e métodos como a eliminação Gaussiana são frequentemente preferidos na prática [^18].

**Inversa do Produto:** A invertibilidade se comporta de maneira previsível em relação ao produto de matrizes. Se $A$ e $B$ são duas matrizes $n \times n$ invertíveis, então o produto $AB$ também é invertível, e sua inversa é dada por:
$$ (AB)^{-1} = B^{-1}A^{-1} $$
Esta propriedade pode ser verificada diretamente multiplicando $AB$ por $B^{-1}A^{-1}$ (usando a associatividade da multiplicação de matrizes [^42]) e é mencionada no contexto [^27, ^40]. Note a inversão da ordem, análoga à propriedade da transposta de um produto.

### Conclusão

A invertibilidade é uma propriedade definidora para matrizes quadradas, indicando a existência de uma inversa única $A^{-1}$ tal que $AA^{-1} = A^{-1}A = I_n$ [^13, ^19]. Este capítulo demonstrou, com base no material apresentado, que a invertibilidade de uma matriz $A$ é rigorosamente equivalente à independência linear de suas colunas [^16, ^28] e à condição de que o sistema homogêneo $Ax=0$ admita apenas a solução trivial $x=0$ [^30]. Estas equivalências são fundamentais e conectam a álgebra de matrizes com a estrutura geométrica dos espaços vetoriais e as propriedades das transformações lineares. A existência da inversa garante a solução única $x = A^{-1}b$ para sistemas lineares $Ax=b$ [^17], embora considerações práticas possam favorecer outros métodos de solução [^18]. A invertibilidade também implica que a transformação linear $x \mapsto Ax$ é um **isomorphism** de $K^n$ para si mesmo, mapeando bases para bases e possuindo propriedades exploradas nas Proposições 3.18 e 3.21 [^34, ^35, ^36, ^45, ^46]. Em suma, a invertibilidade encapsula noções de não-degenerescência, bases e soluções únicas no framework da álgebra linear.

### Referências

[^1]: Página 3: "Now if the vectors u, v, w are linearly independent..."
[^2]: Página 3: "...compute a numerical quantity det(u, v, w), called the determinant... and to check that it is nonzero. In our case, it turns out that det(u, v, w) = 15, which confirms that u, v, w are linearly independent."
[^3]: Página 3: "...matrix consisting of the three columns u, v, w, A = (u v w)..."
[^4]: Página 4: "...so our linear system is expressed by... or more concisely as Ax = b."
[^5]: Página 4: "Now what if the vectors u, v, w are linearly dependent?... our system has no solution."
[^6]: Página 5: "In summary, a 3 × 3 linear system may have a unique solution, no solution, or an infinite number of solutions, depending on the linear independence (and dependence)..."
[^7]: Página 5: "The point of view where our linear system is expressed in matrix form as Ax = b stresses the fact that the map x → Ax is a linear transformation."
[^8]: Página 5: "...we defined the product Ax as the linear combination Ax = x1A¹ + x2A² + x3A³..."
[^9]: Página 7: "Suppose that A is an n×n matrix... Suppose we can find an n × n matrix B such that BA = In." (Note: context uses BAi = ei, which leads to BA=In)
[^10]: Página 7: Defines the identity matrix `In`.
[^11]: Página 7: "If Ax = b, then multiplying both sides on the left by B, we get B(Ax) = Bb... x = Bb."
[^12]: Página 7: "What is not obvious is that BA = In implies AB = In, but this is indeed provable."
[^13]: Página 7: "The matrix B is usually denoted A⁻¹ and called the inverse of A. It can be shown that it is the unique matrix such that AA⁻¹ = A⁻¹A = In."
[^14]: Página 7: "If a square matrix A has an inverse, then we say that it is invertible or nonsingular..."
[^15]: Página 7: "...otherwise we say that it is singular."
[^16]: Página 7: "We will show later that a square matrix is invertible iff its columns are linearly independent iff its determinant is nonzero."
[^17]: Página 7: "In summary, if A is a square invertible matrix, then the linear system Ax = b has the unique solution x = A⁻¹b."
[^18]: Página 7: "In practice, this is not a good way to solve a linear system because computing A⁻¹ is too expensive."
[^19]: Página 39: "Definition 3.16. For any square matrix A of dimension n, if a matrix B such that AB = BA = In exists, then it is unique, and it is called the inverse of A. The matrix B is also denoted by A⁻¹."
[^20]: Página 39: "An invertible matrix is also called a nonsingular matrix, and a matrix that is not invertible is called a singular matrix."
[^21]: Página 39: "Proposition 3.13. If a square matrix A ∈ Mn(K) has a left inverse... or a right inverse... then A is actually invertible. Furthermore, B = A⁻¹ and C = A⁻¹."
[^22]: Página 40: Proof of Prop 3.13: "Suppose that there is a matrix B such that BA = In. This implies that the columns A¹, ..., Aⁿ of A are linearly independent..."
[^23]: Página 40: Proof of Prop 3.13: "...they form a basis of Kⁿ."
[^24]: Página 40: Proof of Prop 3.13: "...there is a unique column vector... These equations yield the matrix equation AX = In..."
[^25]: Página 40: Proof of Prop 3.13: "...by Proposition 2.2, we have X = B..."
[^26]: Página 40: Proof of Prop 3.13: "Let us now assume that there is a matrix C such that AC = In... conclude that A is invertible and that its inverse is equal to C."
[^27]: Página 40: "Using Proposition 2.3... if A and B are two n × n invertible matrices, then AB is also invertible and (AB)⁻¹ = B⁻¹A⁻¹."
[^28]: Página 40: "Proposition 3.14. A square matrix A ∈ Mn(K) is invertible iff its columns (A¹, ..., Aⁿ) are linearly independent."
[^29]: Página 40: Proof of Prop 3.14. References Prop 3.13 and links linear independence to invertibility.
[^30]: Página 41: "Proposition 3.15. A square matrix A ∈ Mn(K) is invertible iff for any x ∈ Kⁿ, the equation Ax = 0 implies that x = 0."
[^31]: Página 41: Proof of Prop 3.15: "the condition Ax = 0 implies x = 0 is equivalent to the linear independence of the columns (A¹, ..., Aⁿ) of A. By Proposition 3.14, the matrix A is invertible."
[^32]: Página 41: Definition 3.17 defines the elementary matrices Eij.
[^33]: Página 23: Definition 3.3 defines linear independence for a family (ui)i∈I.
[^34]: Página 49: Definition 3.21 defines isomorphism and inverse map f⁻¹.
[^35]: Página 49: Proposition 3.21(1): Left inverse implies isomorphism.
[^36]: Página 50: Proposition 3.21(2): Right inverse implies isomorphism.
[^37]: Página 50: Proof of Prop 3.21 relies on Prop 3.18 and Thm 3.11.
[^38]: Página 38: Definition 3.14 defines the identity matrix In.
[^39]: Página 39: Proof of Prop 3.13 uses Ax = 0 where λ is the column vector x.
[^40]: Página 40: Detailed proof steps for Prop 3.13.
[^41]: Página 41: Proof steps for Prop 3.15.
[^42]: Página 42: Proposition 3.16 states matrix multiplication is associative and bilinear.
[^43]: Página 43: Definition 3.18 defines linear maps.
[^44]: Página 44: Definition 3.19 defines Image (Im f) and Kernel (Ker f). Proposition 3.17 states f is injective iff Ker f = (0).
[^45]: Página 45: Proposition 3.18 discusses unique linear map extension from basis and properties (injectivity/surjectivity linkage to basis properties).
[^46]: Página 46: Further discussion linking basis, linear independence, and properties of the linear map f.

<!-- END -->