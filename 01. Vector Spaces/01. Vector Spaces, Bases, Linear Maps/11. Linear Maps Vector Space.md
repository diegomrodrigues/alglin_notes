## Capítulo 3.7.1: O Espaço Vetorial das Aplicações Lineares Hom(E, F)

### Introdução

Nos capítulos anteriores, exploramos os conceitos fundamentais de **espaços vetoriais** [^14], **bases** [^29] e **aplicações lineares** [^43]. Vimos que uma aplicação linear $f: E \to F$ é uma função que preserva a estrutura do espaço vetorial, satisfazendo $f(x + y) = f(x) + f(y)$ e $f(\lambda x) = \lambda f(x)$ para todos os vetores $x, y \in E$ e escalares $\lambda \in K$ [^43]. Uma propriedade fundamental é que aplicações lineares transformam combinações lineares em combinações lineares [^43]. Introduzimos também a noção do conjunto de todas as aplicações lineares de E para F, denotado por **Hom(E, F)** ou $\mathbf{L(E; F)}$ [^50]. Este capítulo aprofunda a análise deste conjunto, demonstrando que ele próprio forma um espaço vetorial e investigando sua dimensão, conectando assim as noções de aplicações lineares e espaços vetoriais de uma maneira mais profunda.

### Conceitos Fundamentais

#### Definição do Espaço Vetorial Hom(E, F)

Conforme introduzido na Definição 3.22 [^50], o conjunto $\text{Hom}(E, F)$ de todas as aplicações lineares entre dois espaços vetoriais E e F sobre um corpo K pode ser dotado de uma estrutura de espaço vetorial. As operações são definidas ponto a ponto, inspiradas na estrutura análoga para espaços de funções gerais vista no Exemplo 3.1 (item 8) [^16].

> **Definição 3.22 (Reformulada):** Seja $E$ e $F$ dois espaços vetoriais sobre um corpo $K$. O conjunto $\text{Hom}(E, F)$ é um espaço vetorial sob as seguintes operações de adição e multiplicação por escalar:
> 1.  **Adição:** Para quaisquer $f, g \in \text{Hom}(E, F)$, a soma $f+g$ é a aplicação definida por:
>     $$ (f+g)(x) = f(x) + g(x) \quad \text{para todo } x \in E $$
> 2.  **Multiplicação por Escalar:** Para qualquer $f \in \text{Hom}(E, F)$ e qualquer escalar $\lambda \in K$, o produto $\lambda f$ é a aplicação definida por:
>     $$ (\lambda f)(x) = \lambda f(x) \quad \text{para todo } x \in E $$
> [^50]

Primeiramente, é necessário verificar que $f+g$ e $\lambda f$, como definidas acima, são de fato aplicações lineares, ou seja, pertencem a $\text{Hom}(E, F)$.
Sejam $f, g \in \text{Hom}(E, F)$ e $\lambda \in K$. Para quaisquer $x, y \in E$ e $\mu \in K$:\n$(f+g)(x+y) = f(x+y) + g(x+y) = (f(x)+f(y)) + (g(x)+g(y)) = (f(x)+g(x)) + (f(y)+g(y)) = (f+g)(x) + (f+g)(y)$.\n$(f+g)(\mu x) = f(\mu x) + g(\mu x) = \mu f(x) + \mu g(x) = \mu (f(x)+g(x)) = \mu (f+g)(x)$.\nPortanto, $f+g \in \text{Hom}(E, F)$.\nSimilarmente, para $\lambda f$:\n$(\lambda f)(x+y) = \lambda f(x+y) = \lambda (f(x)+f(y)) = \lambda f(x) + \lambda f(y) = (\lambda f)(x) + (\lambda f)(y)$.\n$(\lambda f)(\mu x) = \lambda f(\mu x) = \lambda (\mu f(x)) = (\lambda \mu) f(x)$. Utilizando a comutatividade da multiplicação no corpo $K$ [^50], temos $(\lambda \mu) f(x) = (\mu \lambda) f(x) = \mu (\lambda f(x)) = \mu ((\lambda f)(x))$.\nPortanto, $\lambda f \in \text{Hom}(E, F)$. As operações estão bem definidas.

#### Verificação dos Axiomas de Espaço Vetorial

Para estabelecer que $\text{Hom}(E, F)$ é um espaço vetorial, devemos verificar os axiomas listados na Definição 3.1 [^14].\n(V0) **Grupo Abeliano Aditivo:** A adição de funções é associativa e comutativa porque a adição em F é. O elemento neutro é a **aplicação nula** $0: E \to F$, definida por $0(x) = 0_F$ para todo $x \in E$ (onde $0_F$ é o vetor nulo em F). A aplicação nula é linear: $0(x+y) = 0_F = 0_F + 0_F = 0(x) + 0(y)$ e $0(\lambda x) = 0_F = \lambda 0_F = \lambda 0(x)$. Para toda $f \in \text{Hom}(E, F)$, $(f+0)(x) = f(x) + 0(x) = f(x) + 0_F = f(x)$, logo $f+0 = f$. O elemento inverso aditivo de $f$ é a aplicação $-f$, definida por $(-f)(x) = -f(x)$. Como $f$ é linear, $-f$ também é: $(-f)(x+y) = -(f(x+y)) = -(f(x)+f(y)) = (-f(x)) + (-f(y)) = (-f)(x) + (-f)(y)$, e $(-f)(\lambda x) = -(f(\lambda x)) = -(\lambda f(x)) = \lambda(-f(x)) = \lambda((-f)(x))$. Além disso, $(f+(-f))(x) = f(x) + (-f)(x) = f(x) - f(x) = 0_F = 0(x)$, logo $f+(-f)=0$.\n(V1) **Distributividade Escalar sobre Adição Vetorial:** Seja $\alpha \in K$ e $f, g \in \text{Hom}(E, F)$. Temos $(\alpha(f+g))(x) = \alpha((f+g)(x)) = \alpha(f(x)+g(x)) = \alpha f(x) + \alpha g(x) = (\alpha f)(x) + (\alpha g)(x) = (\alpha f + \alpha g)(x)$. Logo, $\alpha(f+g) = \alpha f + \alpha g$.\n(V2) **Distributividade Vetorial sobre Adição Escalar:** Sejam $\alpha, \beta \in K$ e $f \in \text{Hom}(E, F)$. Temos $((\alpha+\beta)f)(x) = (\alpha+\beta)f(x) = \alpha f(x) + \beta f(x) = (\alpha f)(x) + (\beta f)(x) = (\alpha f + \beta f)(x)$. Logo, $(\alpha+\beta)f = \alpha f + \beta f$.\n(V3) **Associatividade da Multiplicação Escalar:** Sejam $\alpha, \beta \in K$ e $f \in \text{Hom}(E, F)$. Temos $((\alpha * \beta)f)(x) = (\alpha * \beta)f(x) = \alpha(\beta f(x)) = \alpha((\beta f)(x)) = (\alpha(\beta f))(x)$. Logo, $(\alpha * \beta)f = \alpha(\beta f)$.\n(V4) **Identidade Multiplicativa:** Seja $1$ o elemento identidade multiplicativo de $K$. Temos $(1 \cdot f)(x) = 1 \cdot f(x) = f(x)$. Logo, $1 \cdot f = f$.\n\nTodos os axiomas de espaço vetorial [^14] são satisfeitos. Portanto, $\text{Hom}(E, F)$ é um espaço vetorial sobre $K$.

#### Dimensão de Hom(E, F)

Um resultado central sobre $\text{Hom}(E, F)$ diz respeito à sua dimensão quando E e F são espaços vetoriais de dimensão finita [^50]. Vamos estabelecer este resultado utilizando as ferramentas desenvolvidas.

**Teorema:** Sejam E e F espaços vetoriais sobre K, com $\dim(E) = n$ e $\dim(F) = m$, onde $n, m$ são inteiros finitos. Então, $\text{Hom}(E, F)$ é um espaço vetorial de dimensão finita e $\dim(\text{Hom}(E, F)) = mn$.

**Prova:** Seja $\mathcal{B}_E = (u_1, \dots, u_n)$ uma base de E [^29] e $\mathcal{B}_F = (v_1, \dots, v_m)$ uma base de F [^29]. De acordo com a Proposição 3.18 [^45], uma aplicação linear $f \in \text{Hom}(E, F)$ é unicamente determinada pela especificação dos vetores $f(u_j) \in F$ para cada $j = 1, \dots, n$.\nComo $\mathcal{B}_F$ é uma base de F, cada vetor imagem $f(u_j)$ pode ser escrito unicamente como uma combinação linear dos vetores de $\mathcal{B}_F$ (Proposição 3.12 [^35]):\n$$ f(u_j) = \sum_{i=1}^m a_{ij} v_i $$\npara um conjunto único de escalares $a_{ij} \in K$, onde $1 \le i \le m$ e $1 \le j \le n$.\nEsses $m \times n$ escalares $a_{ij}$ definem completamente a aplicação linear $f$. Podemos organizar esses escalares em uma matriz $A = (a_{ij})$ de $m$ linhas e $n$ colunas, $A \in M_{m,n}(K)$ [^37].\nConsideremos a aplicação $\Phi: \text{Hom}(E, F) \to M_{m,n}(K)$ que mapeia cada $f \in \text{Hom}(E, F)$ para a sua matriz $A=(a_{ij})$ correspondente, definida por $f(u_j) = \sum_{i=1}^m a_{ij} v_i$.\nA Proposição 3.18 [^45] garante que para cada matriz $A \in M_{m,n}(K)$, existe uma única aplicação linear $f$ tal que $\Phi(f) = A$. Portanto, $\Phi$ é uma bijeção.\nVamos verificar que $\Phi$ é uma aplicação linear. Sejam $f, g \in \text{Hom}(E, F)$ com matrizes associadas $A = (a_{ij})$ e $B = (b_{ij})$, respectivamente. Então $f(u_j) = \sum_i a_{ij} v_i$ e $g(u_j) = \sum_i b_{ij} v_i$.\nPara a soma $f+g$:\n$$ (f+g)(u_j) = f(u_j) + g(u_j) = \sum_{i=1}^m a_{ij} v_i + \sum_{i=1}^m b_{ij} v_i = \sum_{i=1}^m (a_{ij} + b_{ij}) v_i $$\nA matriz associada a $f+g$ é $(a_{ij} + b_{ij})$, que é precisamente a soma das matrizes $A+B$ conforme a Definição 3.13 [^37]. Logo, $\Phi(f+g) = \Phi(f) + \Phi(g)$.\nPara a multiplicação por escalar $\lambda f$:\n$$ (\lambda f)(u_j) = \lambda f(u_j) = \lambda \sum_{i=1}^m a_{ij} v_i = \sum_{i=1}^m (\lambda a_{ij}) v_i $$\nA matriz associada a $\lambda f$ é $(\lambda a_{ij})$, que é precisamente a matriz $\lambda A$ [^38]. Logo, $\Phi(\lambda f) = \lambda \Phi(f)$.\nComo $\Phi$ é uma bijeção linear (um isomorfismo de espaços vetoriais), $\text{Hom}(E, F)$ e $M_{m,n}(K)$ são isomórficos. Sabemos que o espaço vetorial $M_{m,n}(K)$ tem uma base formada pelas matrizes $E_{ij}$ (com 1 na posição $(i,j)$ e 0 nas demais) [^41]. Existem $mn$ tais matrizes. Portanto, $\dim(M_{m,n}(K)) = mn$.\nConclui-se que $\dim(\text{Hom}(E, F)) = \dim(M_{m,n}(K)) = mn$.\n$\blacksquare$\n

#### Caso Especial: Espaço Dual

Um caso particular importante de $\text{Hom}(E, F)$ ocorre quando $F=K$, o corpo de escalares.\nConforme a Definição 3.26 [^52], o espaço $\text{Hom}(E, K)$ é chamado de **espaço dual** de E, denotado por $E^*$. Os elementos de $E^*$ são chamados de **formas lineares** ou **covectores**.\nSe $\dim(E) = n$ (finita), então, pelo teorema anterior, $\dim(E^*) = \dim(\text{Hom}(E, K)) = n \times \dim(K)$. Como $\dim(K)=1$ (qualquer escalar não nulo forma uma base) [^35], temos $\dim(E^*) = n \times 1 = n$.\nEste resultado, $\dim(E^*) = \dim(E)$, está em concordância com o Teorema 3.23 [^54], que estabelece a existência de uma base dual $(u_1^*, \dots, u_n^*)$ para $E^*$ correspondente a cada base $(u_1, \dots, u_n)$ de E.

#### Endomorfismos

Quando $E=F$, as aplicações lineares $f: E \to E$ são chamadas **endomorfismos** [^50]. O espaço vetorial $\text{Hom}(E, E)$, também denotado por $\text{End}(E)$ [^50], tem dimensão $\dim(\text{End}(E)) = n \times n = n^2$, se $\dim(E)=n$. Além da estrutura de espaço vetorial, $\text{End}(E)$ possui uma estrutura de anel (não comutativo, em geral) devido à operação de composição de aplicações lineares, como mencionado após a Proposição 3.16 [^42] para matrizes e aplicável aqui via isomorfismo.

### Conclusão

Demonstramos que o conjunto $\text{Hom}(E, F)$ de todas as aplicações lineares entre dois espaços vetoriais E e F sobre um corpo K herda uma estrutura natural de espaço vetorial. As operações de adição e multiplicação por escalar são definidas ponto a ponto, e a validade dos axiomas de espaço vetorial [^14] depende crucialmente da estrutura de F e das propriedades das aplicações lineares [^43]. No caso de dimensão finita, $\dim(E)=n$ e $\dim(F)=m$, provamos que $\text{Hom}(E, F)$ é isomórfico ao espaço das matrizes $M_{m,n}(K)$ [^37], resultando em $\dim(\text{Hom}(E, F)) = mn$. Este resultado quantifica o "tamanho" do conjunto de transformações lineares possíveis entre dois espaços de dimensão finita e reforça a conexão íntima entre aplicações lineares e matrizes. O caso especial do espaço dual $E^* = \text{Hom}(E, K)$ [^52] ilustra que um espaço vetorial e seu dual têm a mesma dimensão finita [^54]. A estrutura de $\text{Hom}(E, F)$ é fundamental para estudos mais avançados em álgebra linear e análise funcional.

### Referências

[^1]: The dimension of Hom(E, F) is related to the dimensions of E and F. (Do enunciado do problema)
[^5]: The point of view where our linear system is expressed in matrix form as Ax = b stresses the fact that the map x → Ax is a linear transformation. This means that A(λx) = λ(Ax) for all x ∈ R3×1 and all λ ∈ R and that A(u + v) = Au + Av, for all u, v ∈ R3×1. We can view the matrix A as a way of expressing a linear map from R3×1 to R3×1... (p. 53)
[^6]: Thus we have defined a multiplication operation on matrices, namely if A = (aik) is a m × n matrix and if B = (bjk) if n × p matrix, then their product AB is the m × n matrix whose entry on the ith row and the jth column is given by the inner product of the ith row of A by the jth column of B, (AB)ij = ∑k=1^n aik bkj. (p. 54)
[^7]: If a square matrix A has an inverse, then we say that it is invertible or nonsingular... We will show later that a square matrix is invertible iff its columns are linearly independent iff its determinant is nonzero. (p. 55)
[^8]: A major result of linear algebra states that every m × n matrix A can be written as A = VΣU^T, where V is an m × m orthogonal matrix, U is an n × n orthogonal matrix, and Σ is an m × n matrix whose only nonzero entries are nonnegative diagonal entries σ1 ≥ σ2 ≥ ... ≥ σp, where p = min(m, n), called the singular values of A. The factorization A = VΣU^T is called a singular decomposition of A, or SVD. (p. 56)
[^14]: Definition 3.1. Given a field K (with addition + and multiplication *), a vector space over K (or K-vector space) is a set E (of vectors) together with two operations +: E × E → E (called vector addition), and ·: K × E → E (called scalar multiplication) satisfying the following conditions for all α, β ∈ K and all u, v ∈ E; (VO) E is an abelian group w.r.t. +, with identity element 0; (V1) α· (u + v) = (α· u) + (α·υ); (V2) (α + β)· u = (a • u) + (β· u); (V3) (α* β)· u = α · (β· u); (V4) 1 · u = น. (p. 62)
[^16]: Example 3.1. ... 8. A very important example of vector space is the set of linear maps between two vector spaces to be defined in Section 11.1. Here is an example that will prepare us for the vector space of linear maps. Let X be any nonempty set and let E be a vector space. The set of all functions f : X → E can be made into a vector space as follows: Given any two functions f : X → E and g: X → E, let (f + g): X → E be defined such that (f+g)(x) = f(x) + g(x) for all x ∈ X, and for every λ ∈ R, let λf : X → E be defined such that (λf)(x) = λf(x) for all x ∈ X. The axioms of a vector space are easily verified. (p. 64)
[^23]: Definition 3.3. Let E be a vector space. A vector v ∈ E is a linear combination of a family (ui)i∈I of elements of E iff there is a family (λi)i∈I of scalars in K such that v = ∑i∈I λi ui. ... We say that a family (ui)i∈I is linearly independent iff for every family (λi)i∈I of scalars in K, ∑i∈I λi ui = 0 implies that λi = 0 for all i ∈ I. Equivalently, a family (ui)i∈I is linearly dependent iff there is some family (λi)i∈I of scalars in K such that ∑i∈I λi ui = 0 and λj ≠ 0 for some j ∈ I. (p. 71)
[^25]: Definition 3.4. Let E be a vector space. A subset F of E is a linear subspace (or subspace) of E iff F is nonempty and λu + μυ ∈ F for all u, v ∈ F, and all λ, μ ∈ Κ. (p. 73)
[^27]: Proposition 3.5. Given any vector space E, if S is any nonempty subset of E, then the smallest subspace (S) (or Span(S)) of E containing S is the set of all (finite) linear combinations of elements from S. (p. 75)
[^29]: Definition 3.6. Given a vector space E and a subspace V of E, a family (vi)i∈I of vectors vi ∈ V spans V or generates V iff for every v ∈ V, there is some family (λi)i∈I of scalars in K such that v = ∑i∈I λi vi. ... A family (ui)i∈I that spans V and is linearly independent is called a basis of V. (p. 77)
[^30]: Theorem 3.7. Given any finite family S = (ui)i∈I generating a vector space E and any linearly independent subfamily L = (uj)j∈J of S (where J ⊆ I), there is a basis B of E such that L ⊆ B ⊆ S. (p. 78)
[^34]: Theorem 3.11. Let E be a finitely generated vector space. Any family (ui)i∈I generating E contains a subfamily (uj)j∈J which is a basis of E. Any linearly independent family (ui)i∈I can be extended to a family (uj)j∈J which is a basis of E (with I ⊆ J). Furthermore, for every two bases (ui)i∈I and (vj)j∈J of E, we have |I| = |J| = n for some fixed integer n ≥ 0. (p. 82)
[^35]: Definition 3.8. When a vector space E is not finitely generated, we say that E is of infinite dimension. The dimension of a finitely generated vector space E is the common dimension n of all of its bases and is denoted by dim(E). Clearly, if the field K itself is viewed as a vector space, then every family (a) where a ∈ K and a ≠ 0 is a basis. Thus dim(K) = 1. Note that dim({0}) = 0. ... Proposition 3.12. Given a vector space E, let (ui)i∈I be a family of vectors in E. Let v ∈ E, and assume that v = ∑i∈I λi ui. Then the family (λi)i∈I of scalars such that v = ∑i∈I λi ui is unique iff (ui)i∈I is linearly independent. (p. 83)
[^36]: Definition 3.10. If (ui)i∈I is a basis of a vector space E, for any vector v ∈ E, if (xi)i∈I is the unique family of scalars in K such that v = ∑i∈I xi ui, each xi is called the component (or coordinate) of index i of v with respect to the basis (ui)i∈I. Definition 3.11. Given a field K and any (nonempty) set I, let K(I) be the subset of the cartesian product K^I consisting of all families (λi)i∈I with finite support of scalars in K. ... the family (ei)i∈I of vectors ei, defined such that (ei)j = 0 if j ≠ i and (ei)i = 1, is clearly a basis of the vector space K(I). (p. 84)
[^37]: Definition 3.12. If K = R or K = C, an m × n-matrix over K is a family (aij)1≤i≤m, 1≤j≤n of scalars in K, represented by an array... The set of all m × n-matrices is denoted by Mm,n(K) or Mm,n. ... Definition 3.13. Given two m × n matrices A = (aij) and B = (bij), we define their sum A + B as the matrix C = (cij) such that cij = aij + bij; (p. 85)
[^38]: For any matrix A = (aij), we let -A be the matrix (-aij). Given a scalar λ ∈ K, we define the matrix λA as the matrix C = (cij) such that cij = λaij; ... Given an m × n matrices A = (aik) and an n× p matrices B = (bkj), we define their product AB as the m × p matrix C = (cij) such that cij = ∑k=1^n aik bkj... Definition 3.14. The square matrix In of dimension n containing 1 on the diagonal and 0 everywhere else is called the identity matrix. ... Definition 3.15. Given an m × n matrix A = (aij), its transpose A^T = (a^T_ji), is the n × m-matrix such that a^T_ji = aij, for all i, 1 ≤ i ≤ m, and all j, 1 ≤ j ≤ n. (p. 86)
[^39]: Definition 3.16. For any square matrix A of dimension n, if a matrix B such that AB = BA = In exists, then it is unique, and it is called the inverse of A. The matrix B is also denoted by A⁻¹. An invertible matrix is also called a nonsingular matrix, and a matrix that is not invertible is called a singular matrix. ... Proposition 3.13. If a square matrix A ∈ Mn(K) has a left inverse... or a right inverse... then A is actually invertible. (p. 87)
[^40]: Proposition 3.14. A square matrix A ∈ Mn(K) is invertible iff its columns (A¹,..., Aⁿ) are linearly independent. (p. 88)
[^41]: Proposition 3.15. A square matrix A ∈ Mn(K) is invertible iff for any x ∈ Kⁿ, the equation Ax = 0 implies that x = 0. ... It is immediately verified that the set Mm,n(K) of m × n matrices is a vector space under addition of matrices and multiplication of a matrix by a scalar. Definition 3.17. The m × n-matrices Eij = (ehk), are defined such that eij = 1, and ehk = 0, if h ≠ i or k ≠ j; ... Thus, the family (Eij)1≤i≤m,1≤j≤n is a basis of the vector space Mm,n(K), which has dimension mn. (p. 89)
[^42]: Proposition 3.16. (1) Given any matrices A ∈ Mm,n(K), B∈ Mn,p(K), and C∈ Mp,q(K), we have (AB)C = A(BC); ... (2) Given any matrices A, B ∈ Mm,n(K), and C, D ∈ Mn,p(K), for all λ ∈ K, we have (A + B)C = AC + BC, A(C + D) = AC + AD, (λA)C = λ(AC), A(λC) = λ(AC), so that matrix multiplication ·: Mm,n(K) × Mn,p(K) → Mm,p(K) is bilinear. ... show that Mn(K) is a ring with unit In (in fact, an associative algebra). This is a noncommutative ring with zero divisors... (p. 90)
[^43]: Definition 3.18. Given two vector spaces E and F, a linear map between E and F is a function f: E → F satisfying the following two conditions: f(x + y) = f(x) + f(y) for all x, y ∈ E; f(λx) = λf(x) for all λ ∈ K, x ∈ E. ... The basic property of linear maps is that they transform linear combinations into linear combinations. Given any finite family (ui)i∈I of vectors in E, given any family (λi)i∈I of scalars in K, we have f(∑i∈I λi ui) = ∑i∈I λi f(ui). (p. 91)
[^44]: Definition 3.19. Given a linear map f : E → F, we define its image (or range) Im f = f(E), as the set Im f = {y ∈ F | (∃x ∈ E)(y = f(x))}, and its Kernel (or nullspace) Ker f = f⁻¹(0), as the set Ker f = {x ∈ E | f(x) = 0}. Proposition 3.17. Given a linear map f: E → F, the set Im f is a subspace of F and the set Ker f is a subspace of E. The linear map f : E → F is injective iff Ker f = (0). (p. 92)
[^45]: Definition 3.20. Given a linear map f: E → F, the rank rk(f) of f is the dimension of the image Im f of f. Proposition 3.18. Given any two vector spaces E and F, given any basis (ui)i∈I of E, given any other family of vectors (vi)i∈I in F, there is a unique linear map f: E → F such that f(ui) = vi for all i ∈ I. Furthermore, f is injective iff (vi)i∈I is linearly independent, and f is surjective iff (vi)i∈I generates F. (p. 93)
[^48]: Definition 3.21. A linear map f: E → F is an isomorphism iff there is a linear map g: F → E, such that g ◦ f = idE and f ◦ g = idF. (p. 96)
[^49]: Proposition 3.21. Let E be a vector space of finite dimension n ≥ 1 and let f : E → E be any linear map. The following properties hold: (1) If f has a left inverse g... then f is an isomorphism and f⁻¹ = g. (2) If f has a right inverse h... then f is an isomorphism and f⁻¹ = h. (p. 97)
[^50]: Definition 3.22. The set of all linear maps between two vector spaces E and F is denoted by Hom(E, F) or by L(E; F)... The set Hom(E, F) is a vector space under the operations defined in Example 3.1, namely (f+g)(x) = f(x) + g(x) for all x ∈ E, and (λf)(x) = λf(x) for all x ∈ E. The point worth checking carefully is that λf is indeed a linear map, which uses the commutativity of * in the field K... When E and F have finite dimensions, the vector space Hom(E, F) also has finite dimension, as we shall see shortly. Definition 3.23. When E = F, a linear map f: E → E is also called an endomorphism. The space Hom(E, E) is also denoted by End(E). (p. 98)
[^51]: Proposition 3.22. Given any vector space E and any subspace M of E, the relation ≡M is an equivalence relation with the following two congruential properties: ... (p. 99)
[^52]: Definition 3.26. Given a vector space E, the vector space Hom(E, K) of linear maps from E to the field K is called the dual space (or dual) of E. The space Hom(E, K) is also denoted by E*, and the linear maps in E* are called the linear forms, or covectors. The dual space E** of the space E* is called the bidual of E. (p. 100)
[^54]: Definition 3.27. Given a vector space E and any basis (ui)i∈I for E, by Proposition 3.18, for every i ∈ I, there is a unique linear form u*i such that u*i(uj) = δij ... The linear form u*i is called the coordinate form of index i w.r.t. the basis (ui)i∈I. ... Theorem 3.23. (Existence of dual bases) Let E be a vector space of dimension n. ... For every basis (u1,..., Un) of E, the family of coordinate forms (u*1,..., u*n) is a basis of E* (called the dual basis of (u1,..., Un)). (p. 102)
[^55]: In particular, Theorem 3.23 shows a finite-dimensional vector space and its dual E* have the same dimension. (p. 103)
[^56]: Summary ... The vector space of linear maps HomK(E, F). (p. 104)

<!-- END -->