## Capítulo 3.4: Independência Linear e Subespaços

### Introdução

Como introduzido na Seção 3.1, a análise de sistemas de equações lineares está intrinsecamente ligada ao estudo de combinações lineares de vetores [^1], [^2]. Vimos que um sistema da forma $x_1u + x_2v + x_3w = b$ questiona se o vetor $b$ pode ser expresso como uma **combinação linear** dos vetores $u, v, w$ [^2]. A natureza da solução deste sistema – única, inexistente ou infinita – depende crucialmente de uma propriedade fundamental dos vetores geradores: a **independência linear** [^5]. Especificamente, a independência linear dos vetores $u, v, w$ garante que a representação de qualquer vetor no espaço gerado por eles, se existir, é única [^2], [^3]. Esta seção aprofunda o conceito de independência linear, formalizando-o para famílias gerais de vetores e explorando suas consequências, incluindo a definição de subespaços vetoriais. Expandiremos a noção inicial onde a independência linear de $u, v, w$ significa que $x_1u + x_2v + x_3w = 0$ apenas se $x_1 = x_2 = x_3 = 0$ [^2], para uma definição mais geral aplicável a famílias indexadas de vetores, finitas ou infinitas, com suporte finito para os escalares [^22], [^23].

### Conceitos Fundamentais

#### Definição Formal (Famílias Indexadas)

Antes de definir formalmente a independência linear, é crucial adotar a abordagem de **famílias indexadas** de vetores, $(u_i)_{i \\in I}$, em vez de conjuntos, como discutido na Seção 3.3 [^16], [^17]. Esta abordagem permite lidar com vetores repetidos, o que é essencial, por exemplo, ao considerar as colunas de uma matriz [^24]. Assumimos, a menos que especificado de outra forma, que todas as famílias de escalares $(\\lambda_i)_{i \\in I}$ consideradas têm **suporte finito**, significando que apenas um número finito de escalares $\\lambda_i$ são não-nulos [^23].

> **Definição 3.3.** Seja $E$ um espaço vetorial sobre um corpo $K$. Dizemos que uma família de vetores $(u_i)_{i \\in I}$ em $E$ é **linearmente independente** se, para toda família de escalares $(\\lambda_i)_{i \\in I}$ em $K$ com suporte finito, a condição
> $$ \\sum_{i \\in I} \\lambda_i u_i = 0 $$
> implica que $\\lambda_i = 0$ para todo $i \\in I$ [^23].

Equivalentemente, a família $(u_i)_{i \\in I}$ é **linearmente dependente** se existe uma família de escalares $(\\lambda_i)_{i \\in I}$ em $K$ com suporte finito tal que
$$ \\sum_{i \\in I} \\lambda_i u_i = 0 \\quad \\text{e} \\quad \\lambda_j \\neq 0 \\text{ para algum } j \\in I $$
[^23]. Por convenção, a família vazia $(I = \\emptyset)$ é considerada linearmente independente [^23].

#### Interpretação e Consequências

Desdobrando a Definição 3.3, uma família $(u_i)_{i \\in I}$ é linearmente dependente se e somente se uma das seguintes condições ocorre:
1.  $I$ consiste em um único elemento $i$, e $u_i = 0$.
2.  $|I| \\ge 2$ e algum vetor $u_j$ na família pode ser expresso como uma combinação linear dos outros vetores na família [^24]. Isto é, existem escalares $(\\lambda_i)_{i \\in I - \\{j\\}}$ tais que $u_j = \\sum_{i \\in I - \\{j\\}} \\alpha_i u_i$. De fato, se $\\sum_{i \\in I} \\lambda_i u_i = 0$ com $\\lambda_j \\neq 0$, então $u_j = \\sum_{i \\in I - \\{j\\}} (-\\lambda_j^{-1}\\lambda_i) u_i$ [^24].

Uma consequência importante da definição é que se uma família $(u_i)_{i \\in I}$ com $I$ não vazio é linearmente independente, então $u_i \\neq 0$ para todo $i \\in I$. Além disso, se $|I| \\ge 2$, devemos ter $u_i \\neq u_j$ para todos $i, j \\in I$ com $i \\neq j$. Caso contrário, se $u_i = u_j$ para $i \\neq j$, teríamos uma dependência linear não trivial $1 \\cdot u_i + (-1) \\cdot u_j = 0$ [^24]. Portanto, uma família linearmente independente não trivial é, na verdade, um conjunto de vetores distintos e não nulos [^24].

A importância da independência linear reside fundamentalmente na garantia da unicidade da representação vetorial. Como vimos na Seção 3.1, se os vetores $u, v, w$ são linearmente independentes, então qualquer vetor $z$ que pode ser escrito como $z = x_1u + x_2v + x_3w$ tem essa representação de forma única [^2], [^3]. Generalizamos este resultado:

> **Proposição 3.12.** Dado um espaço vetorial $E$, seja $(u_i)_{i \\in I}$ uma família de vetores em $E$. Seja $v \\in E$, e assuma que $v = \\sum_{i \\in I} \\lambda_i u_i$. A família de escalares $(\\lambda_i)_{i \\in I}$ tal que $v = \\sum_{i \\in I} \\lambda_i u_i$ é **única** se e somente se $(u_i)_{i \\in I}$ é linearmente independente [^35].

*Prova.* Suponha que $(u_i)_{i \\in I}$ é linearmente independente. Se $(\\mu_i)_{i \\in I}$ é outra família de escalares tal que $v = \\sum_{i \\in I} \\mu_i u_i$, então subtraindo as duas representações obtemos $\\sum_{i \\in I} (\\lambda_i - \\mu_i) u_i = 0$. Pela independência linear, devemos ter $\\lambda_i - \\mu_i = 0$ para todo $i \\in I$, ou seja, $\\lambda_i = \\mu_i$ para todo $i \\in I$, provando a unicidade. Reciprocamente, suponha que a representação seja única. Se a família $(u_i)_{i \\in I}$ fosse linearmente dependente, existiria uma família de escalares $(\\alpha_i)_{i \\in I}$, não todos nulos, tal que $\\sum_{i \\in I} \\alpha_i u_i = 0$. Então, para $v = \\sum_{i \\in I} \\lambda_i u_i$, teríamos também $v = v + 0 = \\sum_{i \\in I} \\lambda_i u_i + \\sum_{i \\in I} \\alpha_i u_i = \\sum_{i \\in I} (\\lambda_i + \\alpha_i) u_i$. Como nem todos os $\\alpha_i$ são zero, a família $(\\lambda_i + \\alpha_i)_{i \\in I}$ é diferente de $(\\lambda_i)_{i \\in I}$, contradizendo a unicidade da representação. Portanto, $(u_i)_{i \\in I}$ deve ser linearmente independente [^35]. $\\blacksquare$

#### Dependência Linear

A dependência linear indica redundância na família de vetores. Se $(u_i)_{i \\in I}$ é linearmente dependente, então existe uma combinação linear não trivial que resulta no vetor nulo. Como exemplo, considere quaisquer dois escalares distintos $\\lambda, \\mu \\neq 0$ no corpo $K$. A família $(\\lambda, \\mu)$ é linearmente dependente no espaço vetorial $K$, pois podemos tomar escalares $x_1 = \\mu$ e $x_2 = -\\lambda$ (ambos não nulos), tais que $x_1 \\lambda + x_2 \\mu = \\mu\\lambda - \\lambda\\mu = 0$ [^24]. Outro exemplo concreto foi dado na Seção 3.1 com os vetores $u = (1, 2, 1)$, $v = (2, 1, -1)$ e $w = (-1, 1, 2)$. Foi mostrado que $u - v = w$, ou seja, $u - v - w = 0$, o que constitui uma *nontrivial linear dependence* [^4]. Neste caso, a família $(u, v, w)$ é linearmente dependente.

A dependência linear tem implicações diretas na solução de sistemas lineares $Ax=b$. Se as colunas de $A$, que correspondem aos vetores $u, v, w$ no exemplo inicial, são linearmente dependentes, o sistema não pode ter uma solução única. Ele terá ou nenhuma solução (se $b$ não pertencer ao espaço gerado pelas colunas linearmente independentes) ou infinitas soluções (se $b$ pertencer a esse espaço) [^4], [^5]. Por exemplo, com $w = u - v$, o sistema $x_1u + x_2v + x_3w = b$ torna-se $(x_1 + x_3)u + (x_2 - x_3)v = b$. Se $b$ for uma combinação linear de $u$ e $v$ (que são linearmente independentes neste exemplo), haverá infinitas soluções parametrizadas por $x_3$ [^5].

#### Independência Linear e Sistemas Lineares

A conexão entre independência linear e sistemas lineares pode ser elegantemente expressa usando matrizes. Seja $A$ uma matriz $n \\times n$ cujas colunas são $A^1, ..., A^n$. O sistema $Ax=0$ corresponde à equação vetorial $x_1A^1 + \\dots + x_nA^n = 0$.

> **Proposição 3.14.** Uma matriz quadrada $A \\in M_n(K)$ é invertível se e somente se suas colunas $(A^1, ..., A^n)$ são linearmente independentes [^40].

> **Proposição 3.15.** Uma matriz quadrada $A \\in M_n(K)$ é invertível se e somente se para qualquer $x \\in K^n$, a equação $Ax = 0$ implica que $x = 0$ [^41].

A condição $Ax = 0$ implica $x = 0$ é precisamente a definição de independência linear para as colunas de $A$ [^41]. Portanto, a invertibilidade de uma matriz quadrada $A$ está diretamente ligada à independência linear de suas colunas. Quando as colunas são linearmente independentes, $A$ é invertível e o sistema $Ax=b$ tem uma solução única dada por $x = A^{-1}b$ [^7].

#### Métodos de Verificação

Para determinar se um conjunto de vetores em $\\mathbb{R}^n$, representado como colunas de uma matriz quadrada $A$, é linearmente independente, um método teórico é calcular o determinante de $A$. Os vetores são linearmente independentes se e somente se $\\det(A) \\neq 0$ [^3], [^7]. No exemplo inicial com $u=(1,2,1), v=(2,1,-1), w=(-1,1,-2)$, foi calculado $\\det(u, v, w) = 15 \\neq 0$, confirmando que $u, v, w$ são linearmente independentes [^3]. No entanto, para sistemas maiores, métodos computacionais como a eliminação Gaussiana (que pode ser usada para calcular decomposições LU), decomposição QR ou decomposição em valores singulares (SVD) são preferíveis [^3].

#### Relação com Bases

O conceito de independência linear é um dos dois pilares na definição de uma **base** de um espaço vetorial.

> **Definição 3.6.** Uma família $(u_i)_{i \\in I}$ de vetores em um subespaço $V$ de $E$ é uma **base** de $V$ se ela gera $V$ (i.e., $V = \\text{Span}((u_i)_{i \\in I})$) e é linearmente independente [^29].

A Proposição 3.8 estabelece caracterizações equivalentes de uma base, afirmando que uma família $B = (v_i)_{i \\in I}$ é uma base de $E$ se e somente se é uma família linearmente independente maximal ou se é uma família geradora minimal [^30]. O Lema 3.6 mostra como construir incrementalmente famílias linearmente independentes: se $(u_i)_{i \\in I}$ é linearmente independente e $v$ não é uma combinação linear de $(u_i)_{i \\in I}$, então a família estendida $(u_i)_{i \\in I} \\cup_k \\{v\\}$ (onde $k \\notin I$) também é linearmente independente [^29]. Este lema é fundamental para provar o Teorema 3.7, que garante que qualquer família linearmente independente $L$ contida numa família geradora finita $S$ pode ser estendida para uma base $B$ tal que $L \\subseteq B \\subseteq S$ [^30]. Isso solidifica a importância da independência linear na construção de representações eficientes e não redundantes de espaços vetoriais.

### Conclusão

A **independência linear** é um conceito central na álgebra linear, permeando a teoria de espaços vetoriais, bases, dimensão, sistemas lineares e transformações lineares. Ela formaliza a noção de não redundância em um conjunto (ou família) de vetores, garantindo que nenhum vetor possa ser expresso como combinação linear dos demais. Essa propriedade é essencial para a **unicidade da representação** de vetores em termos de uma base [^35], um resultado fundamental que conecta a estrutura algébrica à representação coordenada. A independência linear das colunas de uma matriz quadrada determina sua **invertibilidade** [^40] e, consequentemente, a existência e unicidade de soluções para sistemas lineares associados [^7]. Além disso, a independência linear é um critério definidor para **bases** [^29], que são as "pedras angulares" para descrever espaços vetoriais de dimensão finita. Finalmente, transformações lineares injetivas preservam a independência linear, mapeando bases para famílias linearmente independentes [^45], o que destaca a interação profunda entre a estrutura do espaço e as funções que a preservam. O estudo da independência linear, portanto, fornece ferramentas essenciais para a análise e compreensão de fenômenos lineares em diversas áreas da matemática, ciência e engenharia.

### Referências

[^1]: Chapter 3, Section 3.1, Page 49.
[^2]: Chapter 3, Section 3.1, Page 50.
[^3]: Chapter 3, Section 3.1, Page 51.
[^4]: Chapter 3, Section 3.1, Page 52.
[^5]: Chapter 3, Section 3.1, Page 53.
[^6]: Chapter 3, Section 3.1, Page 54.
[^7]: Chapter 3, Section 3.1, Page 55.
[^8]: Chapter 3, Section 3.1, Page 56.
[^9]: Chapter 3, Section 3.1, Page 57.
[^10]: Chapter 3, Section 3.1, Page 58.
[^11]: Chapter 3, Section 3.1, Page 59.
[^12]: Chapter 3, Section 3.1, Page 60.
[^13]: Chapter 3, Section 3.2, Page 61.
[^14]: Chapter 3, Section 3.2, Page 62.
[^15]: Chapter 3, Section 3.2, Page 63.
[^16]: Chapter 3, Section 3.2, Page 64.
[^17]: Chapter 3, Section 3.3, Page 65.
[^18]: Chapter 3, Section 3.3, Page 66.
[^19]: Chapter 3, Section 3.3, Page 67.
[^20]: Chapter 3, Section 3.3, Page 68.
[^21]: Chapter 3, Section 3.3, Page 69.
[^22]: Chapter 3, Section 3.4, Page 70.
[^23]: Chapter 3, Section 3.4, Page 71.
[^24]: Chapter 3, Section 3.4, Page 72.
[^25]: Chapter 3, Section 3.4, Page 73.
[^26]: Chapter 3, Section 3.4, Page 74.
[^27]: Chapter 3, Section 3.4, Page 75.
[^28]: Chapter 3, Section 3.4, Page 76.
[^29]: Chapter 3, Section 3.5, Page 77.
[^30]: Chapter 3, Section 3.5, Page 78.
[^31]: Chapter 3, Section 3.5, Page 79.
[^32]: Chapter 3, Section 3.5, Page 80.
[^33]: Chapter 3, Section 3.5, Page 81.
[^34]: Chapter 3, Section 3.5, Page 82.
[^35]: Chapter 3, Section 3.5, Page 83.
[^36]: Chapter 3, Section 3.5, Page 84.
[^37]: Chapter 3, Section 3.6, Page 85.
[^38]: Chapter 3, Section 3.6, Page 86.
[^39]: Chapter 3, Section 3.6, Page 87.
[^40]: Chapter 3, Section 3.6, Page 88.
[^41]: Chapter 3, Section 3.6, Page 89.
[^42]: Chapter 3, Section 3.6, Page 90.
[^43]: Chapter 3, Section 3.7, Page 91.
[^44]: Chapter 3, Section 3.7, Page 92.
[^45]: Chapter 3, Section 3.7, Page 93.
[^46]: Chapter 3, Section 3.7, Page 94.
[^47]: Chapter 3, Section 3.7, Page 95.
[^48]: Chapter 3, Section 3.7, Page 96.
[^49]: Chapter 3, Section 3.7, Page 97.
[^50]: Chapter 3, Section 3.7, Page 98.
[^51]: Chapter 3, Section 3.8, Page 99.
[^52]: Chapter 3, Section 3.9, Page 100.
[^53]: Chapter 3, Section 3.9, Page 101.
[^54]: Chapter 3, Section 3.9, Page 102.
[^55]: Chapter 3, Section 3.10, Page 103.
[^56]: Chapter 3, Section 3.11, Page 104.

<!-- END -->