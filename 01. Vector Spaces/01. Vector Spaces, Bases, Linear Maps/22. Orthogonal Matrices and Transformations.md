## Matrizes Ortogonais: Definição e Propriedades Fundamentais

### Introdução

No contexto mais amplo de espaços vetoriais, bases e mapas lineares, introduzido no Capítulo 3 [^16] [^17] [^11], certas classes de matrizes e transformações lineares exibem propriedades geométricas e algébricas particularmente importantes. Expandindo os conceitos de transformações lineares [^10] [^11] e operações matriciais [^4] [^5], este capítulo foca em uma classe especial de matrizes quadradas conhecidas como **matrizes ortogonais**. Conforme mencionado brevemente em [^1], estas matrizes estão intrinsecamente ligadas a transformações lineares que preservam o comprimento [^8], desempenhando um papel crucial em diversas áreas, incluindo fatorizações matriciais como a decomposição QR e a decomposição em valores singulares (SVD) [^14] [^15]. Exploraremos a definição formal, as propriedades fundamentais e a interpretação geométrica das matrizes ortogonais.

### Conceitos Fundamentais

#### Definição Formal

Iniciamos com a definição formal apresentada no contexto. Uma matriz quadrada $Q$ de ordem $n \\times n$ é denominada **matriz ortogonal** se satisfizer a seguinte condição [^1]:
$$ QQ^T = Q^TQ = I_n $$
Aqui, $Q^T$ denota a **transposta** da matriz $Q$, definida como a matriz $n \\times m$ $(a^T_{ji})$ obtida a partir de uma matriz $m \\times n$ $A = (a_{ij})$ trocando suas linhas por colunas, ou seja, $a^T_{ji} = a_{ij}$ [^2]. $I_n$ representa a **matriz identidade** de ordem $n$, uma matriz quadrada com entradas 1 na diagonal principal e 0 nas demais posições [^3]. A condição $QQ^T = Q^TQ = I_n$ estabelece uma relação fundamental entre uma matriz ortogonal e sua transposta.

#### Relação com a Inversa

Uma consequência direta e importante da definição de matriz ortogonal é a relação entre sua transposta e sua inversa. Recordemos que a inversa de uma matriz quadrada $A$, denotada por $A^{-1}$, é a única matriz tal que $AA^{-1} = A^{-1}A = I_n$ [^12]. Comparando esta definição com a condição para uma matriz ortogonal $Q$, $Q^TQ = I_n$ e $QQ^T = I_n$ [^1], concluímos imediatamente que a inversa de uma matriz ortogonal $Q$ é igual à sua transposta [^13]:
> $Q^{-1} = Q^T$

Esta propriedade simplifica enormemente os cálculos envolvendo inversas de matrizes ortogonais, já que a transposição é uma operação computacionalmente muito mais simples do que a inversão geral de matrizes.

#### Preservação do Produto Interno e da Norma

As matrizes ortogonais possuem uma propriedade geométrica fundamental: elas preservam o **produto interno** (inner product) [^9]. O produto interno usual entre dois vetores $x = (x_1, ..., x_n)$ e $y = (y_1, ..., y_n)$ em $\\mathbb{R}^n$, denotado por $x \\cdot y$ ou $\\langle x, y \\rangle$, é definido como $x \\cdot y = \\sum_{i=1}^n x_i y_i$ [^6]. Em notação matricial, considerando $x$ e $y$ como vetores coluna (elementos de $\\mathbb{R}^{n \\times 1}$ [^16]), o produto interno pode ser escrito como $x^T y$.

**Lema 1:** Seja $Q$ uma matriz $n \\times n$. $Q$ é ortogonal se e somente se preserva o produto interno, i.e., $(Qx) \\cdot (Qy) = x \\cdot y$ para todos $x, y \\in \\mathbb{R}^n$.

*Prova.* Suponha que $Q$ é ortogonal, então $Q^T Q = I_n$ [^1]. Calculamos o produto interno entre $Qx$ e $Qy$ usando a notação matricial:\
$$ (Qx) \\cdot (Qy) = (Qx)^T (Qy) $$
Utilizando a propriedade da transposta de um produto de matrizes, $(AB)^T = B^T A^T$, temos $(Qx)^T = x^T Q^T$. Portanto,
$$ (Qx)^T (Qy) = (x^T Q^T) (Qy) = x^T (Q^T Q) y $$
Como $Q$ é ortogonal, $Q^T Q = I_n$ [^1]. Logo,
$$ x^T (Q^T Q) y = x^T I_n y = x^T y = x \\cdot y $$
Assim, $(Qx) \\cdot (Qy) = x \\cdot y$.
Reciprocamente, suponha que $(Qx) \\cdot (Qy) = x \\cdot y$ para todos $x, y \\in \\mathbb{R}^n$. Reescrevendo em notação matricial, $x^T Q^T Q y = x^T y$ para todos $x, y$. Isso implica $x^T (Q^T Q - I_n) y = 0$ para todos $x, y$. Escolhendo $x = e_i$ e $y = e_j$ (vetores da base canônica), onde $(e_k)_l = \\delta_{kl}$, obtemos que a entrada $(i, j)$ da matriz $Q^T Q - I_n$ é zero. Como isso vale para todos $i, j$, temos $Q^T Q - I_n = 0$, ou seja, $Q^T Q = I_n$. Como $Q$ é quadrada, isso também implica $QQ^T = I_n$ (um resultado mencionado como "provable" em [^12] para inversas, que se aplica aqui dado que $Q^T$ atua como inversa), concluindo que $Q$ é ortogonal. $\\blacksquare$

Como corolário direto da preservação do produto interno, as matrizes ortogonais preservam a **norma Euclidiana** (ou $\\ell^2$-norm), que generaliza o conceito de comprimento de um vetor [^7]. A norma Euclidiana de um vetor $x$ é definida como $||x||_2 = \\sqrt{x \\cdot x}$ [^7].

**Corolário 1:** Se $Q$ é uma matriz ortogonal, então $||Qx||_2 = ||x||_2$ para todo $x \\in \\mathbb{R}^n$.

*Prova.* Usando o Lema 1 com $y = x$, temos:
$$ ||Qx||_2^2 = (Qx) \\cdot (Qx) = x \\cdot x = ||x||_2^2 $$
Como a norma é não-negativa, extraindo a raiz quadrada, obtemos $||Qx||_2 = ||x||_2$. $\\blacksquare$

Esta propriedade de preservação do comprimento é a razão pela qual as transformações lineares associadas a matrizes ortogonais são descritas como transformações que preservam o comprimento [^8]. Geometricamente, elas correspondem a isometrias lineares, que incluem rotações e reflexões [^9].

#### Orthonormalidade das Colunas e Linhas

A condição $Q^T Q = I_n$ [^1] tem uma interpretação importante em termos das colunas de $Q$. Seja $Q = [q_1 | q_2 | \\dots | q_n]$, onde $q_j \\in \\mathbb{R}^{n \\times 1}$ é a $j$-ésima coluna de $Q$. A matriz transposta $Q^T$ terá as transpostas dessas colunas como suas linhas:
$$ Q^T = \\begin{pmatrix} - & q_1^T & - \\\\ - & q_2^T & - \\\\ & \\vdots & \\\\ - & q_n^T & - \\end{pmatrix} $$
O produto $Q^T Q$ é calculado como:
$$ (Q^T Q)_{ij} = (\\text{i-ésima linha de } Q^T) \\times (\\text{j-ésima coluna de } Q) = q_i^T q_j $$
A condição $Q^T Q = I_n$ significa que $(Q^T Q)_{ij} = \\delta_{ij}$, onde $\\delta_{ij}$ é o delta de Kronecker (1 se $i=j$, 0 se $i \\neq j$), como visto na estrutura da matriz identidade [^3]. Portanto, temos:
$$ q_i^T q_j = \\begin{cases} 1 & \\text{se } i = j \\\\ 0 & \\text{se } i \\neq j \\end{cases} $$
Isso significa que as colunas $q_1, ..., q_n$ de $Q$ formam um conjunto **ortonormal** de vetores: são mutuamente ortogonais ($q_i^T q_j = 0$ para $i \\neq j$, o que corresponde a um ângulo de $\\pi/2$ entre eles [^9]) e cada coluna tem norma Euclidiana 1 ($||q_i||_2^2 = q_i^T q_i = 1$ [^7]).

De maneira análoga, a condição $QQ^T = I_n$ [^1] implica que as linhas de $Q$ também formam um conjunto ortonormal de vetores (considerados como vetores linha [^17]).

#### Aplicações em Fatorizações

O contexto menciona explicitamente o uso de matrizes ortogonais em fatorizações matriciais importantes [^14] [^15].
1.  **Decomposição QR:** Qualquer matriz $A$ (não necessariamente quadrada) pode ser fatorada como $A=QR$, onde $Q$ é uma matriz com colunas ortonormais (e ortogonal, se $A$ for quadrada e invertível) e $R$ é uma matriz triangular superior. Esta decomposição é utilizada, por exemplo, na solução de sistemas lineares e problemas de mínimos quadrados. A menção em [^14] sugere que a fatorização de $A$ usando matrizes ortogonais é uma técnica para resolver $Ax=b$.
2.  **Decomposição em Valores Singulares (SVD):** Toda matriz $m \\times n$ $A$ pode ser decomposta como $A = U \\Sigma V^T$, onde $U$ é uma matriz ortogonal $m \\times m$, $V$ é uma matriz ortogonal $n \\times n$, e $\\Sigma$ é uma matriz $m \\times n$ diagonal (no sentido de ter entradas não-nulas apenas em posições $(i,i)$) com valores não-negativos na diagonal, chamados valores singulares [^15]. A SVD tem vastas aplicações, incluindo compressão de dados [^15], redução de dimensionalidade (PCA) [^15], e a definição da pseudo-inversa $A^+$ para resolver sistemas lineares $Ax=b$ mesmo quando não há solução exata [^15].

### Conclusão

As **matrizes ortogonais**, definidas pela condição $QQ^T = Q^TQ = I_n$ [^1], representam uma classe fundamental de matrizes em álgebra linear. Sua propriedade definidora implica diretamente que a inversa de uma matriz ortogonal é simplesmente sua transposta, $Q^{-1} = Q^T$ [^13]. Geometricamente, elas correspondem a **transformações lineares** [^10] [^11] que preservam o **produto interno** [^9] e, consequentemente, a **norma Euclidiana** (comprimento) dos vetores [^7] [^8]. Esta preservação de comprimento as caracteriza como generalizações de rotações e reflexões no espaço Euclidiano [^9]. Algebricamente, as colunas (e linhas) de uma matriz ortogonal formam uma **base ortonormal**. Sua importância é ainda destacada por seu papel central em fatorizações matriciais poderosas como a **decomposição QR** [^14] e a **SVD** [^15], ferramentas essenciais em diversas aplicações computacionais e teóricas.

### Referências

[^1]: page 56: An n× n matrix Q such that QQᵀ = QᵀQ = Iₙ is called an orthogonal matrix.
[^2]: page 56: Given an m × n matrix A = (ακι), the n × m matrix Aᵀ = (aᵀji) whose ith row is the ith column of A, which means that aᵀji = aij for i = 1,...,n and j = 1, . . ., m, is called the transpose of A.
[^3]: page 55: If we form the n x n matrix In = ... called the identity matrix, whose ith column is e_i...
[^4]: page 54: Thus we have defined a multiplication operation on matrices, namely if A = (aik) is a m × n matrix and if B = (bjk) if n × p matrix, then their product AB is the m × n matrix whose entry on the ith row and the jth column is given by the inner product of the ith row of A by the jth column of B, (AB)ij = ∑(k=1 to n) aik * bkj.
[^5]: page 86: Given an m × n matrices A = (aik) and an n× p matrices B = (bkj), we define their product AB as the m × p matrix C = (cij) such that cij = ∑(k=1 to n) aik * bkj.
[^6]: page 54: More generally, given any two vectors x = (x1,...,xn) and y = (y1, ………, yn) ∈ R", their inner product denoted x·y, or (x, y), is the number x · y = (x1 x2 ... xn) * (y1 y2 ... yn)^T = ∑(i=1 to n) xi * yi.
[^7]: page 54: First, we quantity ||x||₂ = √x · x = (x₁² + ... + xn²)¹/² is a generalization of the length of a vector, called the Euclidean norm, or l²-norm.
[^8]: page 56: Geometrically, they correspond to linear transformation that preserve length.
[^9]: page 54: The (square) matrices Q that preserve the inner product, in the sense that (Qx, Qy) = (x, y) for all x, y ∈ R”, also play a very important role. They can be thought of as generalized rotations. In particular, if x. y = 0 then the vectors x and y make the angle π/2, that is, they are orthogonal.
[^10]: page 53: This means that A(λx) = λ(Ax) for all x ∈ R³ˣ¹ and all λ ∈ R and that A(u + v) = Au + Av, for all u, v ∈ R³ˣ¹.\
[^11]: page 91: Definition 3.18. Given two vector spaces E and F, a linear map between E and F is a function f: E → F satisfying the following two conditions: f(x + y) = f(x) + f(y) for all x, y ∈ E; f(λx) = λf(x) for all λ ∈ K, x ∈ E.
[^12]: page 55: The matrix B is usually denoted A⁻¹ and called the inverse of A. It can be shown that it is the unique matrix such that AA⁻¹ = A⁻¹A = In.
[^13]: page 56: Equivalently, the inverse Q⁻¹ of an orthogonal matrix Q is equal to its transpose Qᵀ.
[^14]: page 56: Ax = b make use of a factorization of A (QR decomposition, SVD decomposition), using orthogonal matrices defined next.
[^15]: page 56: A major result of linear algebra states that every m × n matrix A can be written as A = UΣVᵀ, where V is an m × m orthogonal matrix, U is an n × n orthogonal matrix, and Σ is an m × n matrix whose only nonzero entries are nonnegative diagonal entries σ₁ ≥ σ₂ ≥ ... ≥ σρ, where p = min(m, n), called the singular values of A. The factorization A = UΣVᵀ is called a singular decomposition of A, or SVD. The SVD can be used to “solve” a linear system Ax = b... A⁺ can be computed from an SVD A = UΣVᵀ of A. Indeed, A⁺ = VΣ⁺Uᵀ... Another important application of the SVD is principal component analysis (or PCA)... data compression... low-rank decomposition...
[^16]: page 50: The set of all vectors with three components is denoted by R³ˣ¹. The reason for using the notation R³ˣ¹ rather than the more conventional notation R³ is that the elements of R³ˣ¹ are column vectors... we will denote the set of column vectors with n components by Rⁿˣ¹.\
[^17]: page 50: On the other hand, R³ = R × R × R consists of all triples of the form (x₁, x₂, x₃), with x₁, x₂, x₃ ∈ R, and these are row vectors.

<!-- END -->