## Definição Axiomática e Propriedades Fundamentais de Espaços Vetoriais

### Introdução

Como introduzido na Seção 3.1 [^1, ^2], a manipulação de vetores e escalares surge naturalmente na resolução de sistemas de equações lineares e em problemas de otimização linear. Observamos operações como adição de vetores e multiplicação de vetor por escalar em $\mathbb{R}^{n \times 1}$ [^1, ^2]. Para unificar e generalizar o estudo dessas estruturas \'lineares\', a noção de **espaço vetorial** emergiu no início do século XX como um framework conveniente e unificador [^13]. Este capítulo foca na definição axiomática formal de um espaço vetorial sobre um corpo K e explora suas propriedades fundamentais, estabelecendo a base para conceitos subsequentes como bases e transformações lineares, que serão abordados em seções posteriores. O objetivo é fornecer uma compreensão rigorosa desta estrutura algébrica fundamental.

### Conceitos Fundamentais

A definição formal de um espaço vetorial é baseada em um conjunto de axiomas que especificam as propriedades das operações de adição de vetores e multiplicação por escalar.

> **Definição 3.1.** Dado um corpo K (com adição + e multiplicação *), um **espaço vetorial** sobre K (ou K-espaço vetorial) é um conjunto E (de **vetores**) juntamente com duas operações +: E × E → E (chamada **adição vetorial**),¹ e ·: K × E → E (chamada **multiplicação escalar**) satisfazendo as seguintes condições para todos α, β ∈ K e todos u, v ∈ E;
> (V0) E é um **grupo abeliano** em relação a +, com elemento identidade 0;²
> (V1) $\alpha \cdot (u + v) = (\alpha \cdot u) + (\alpha \cdot v)$; (Distributividade da multiplicação escalar sobre a adição vetorial)
> (V2) $(\alpha + \beta) \cdot u = (\alpha \cdot u) + (\beta \cdot u)$; (Distributividade da multiplicação escalar sobre a adição de escalares)
> (V3) $(\alpha * \beta) \cdot u = \alpha \cdot (\beta \cdot u)$; (Compatibilidade da multiplicação escalar com a multiplicação do corpo)
> (V4) $1 \cdot u = u$. (Elemento identidade para multiplicação escalar)
> Em (V3), * denota a multiplicação no corpo K [^14]. Dado $a \in K$ e $v \in E$, o elemento $a \cdot v$ também é denotado por $av$. O corpo K é frequentemente chamado de corpo de **escalares** [^14]. A menos que especificado de outra forma ou que estejamos lidando com vários corpos diferentes, assumiremos que todos os K-espaços vetoriais são definidos em relação a um corpo K fixo. Assim, nos referiremos a um K-espaço vetorial simplesmente como um espaço vetorial. Na maioria dos casos, o corpo K será o corpo $\mathbb{R}$ dos reais [^14].
> ¹O símbolo + está sobrecarregado, denotando tanto a adição no corpo K quanto a adição de vetores em E. Geralmente, o contexto torna claro qual + é pretendido [^14].
> ²O símbolo 0 também está sobrecarregado, representando tanto o zero em K (um escalar) quanto o elemento identidade de E (o **vetor nulo**). A confusão raramente surge, mas pode-se preferir usar **0** para o vetor nulo [^14].

**Consequências Imediatas dos Axiomas**

A partir dos axiomas, algumas propriedades importantes podem ser deduzidas imediatamente. De (V0), um espaço vetorial sempre contém o **vetor nulo** 0, e, portanto, é não vazio [^14]. Da propriedade de grupo abeliano (V0) e do axioma (V1), podemos derivar $\alpha \cdot 0 = 0$. De fato, $\alpha \cdot 0 = \alpha \cdot (0+0) = \alpha \cdot 0 + \alpha \cdot 0$, e somando o inverso aditivo de $\alpha \cdot 0$ a ambos os lados, obtemos $0 = \alpha \cdot 0$. Similarmente, de (V1), obtemos $\alpha \cdot (-v) = -(\alpha \cdot v)$ [^14]. Do axioma (V2), podemos derivar $0 \cdot v = 0$. De fato, $0 \cdot v = (0+0) \cdot v = 0 \cdot v + 0 \cdot v$, e somando o inverso aditivo de $0 \cdot v$ a ambos os lados, obtemos $0 = 0 \cdot v$. Similarmente, de (V2), obtemos $(-\alpha) \cdot v = -(\alpha \cdot v)$ [^14].

Outra consequência importante dos axiomas é formalizada na seguinte proposição:

> **Proposição 3.1.** Para qualquer $u \in E$ e qualquer $\lambda \in K$, se $\lambda \neq 0$ e $\lambda \cdot u = 0$, então $u = 0$ [^14].

*Prova.* Como $\lambda \neq 0$ e K é um corpo, $\lambda$ possui um inverso multiplicativo $\lambda^{-1} \in K$. Partindo de $\lambda \cdot u = 0$, multiplicamos ambos os lados pela esquerda por $\lambda^{-1}$ para obter $\lambda^{-1} \cdot (\lambda \cdot u) = \lambda^{-1} \cdot 0$ [^14]. Pela propriedade $\alpha \cdot 0 = 0$ demonstrada acima, o lado direito é 0 [^14]. Usando o axioma (V3) e depois (V4) no lado esquerdo, temos $\lambda^{-1} \cdot (\lambda \cdot u) = (\lambda^{-1} * \lambda) \cdot u = 1 \cdot u = u$ [^14]. Portanto, deduzimos que $u = 0$ [^14]. $\blacksquare$

**Discussão sobre o Axioma V4**

Pode-se questionar se o axioma (V4), $1 \cdot u = u$, é realmente necessário ou se pode ser derivado dos outros axiomas (V0)-(V3). A resposta é **não** [^15]. Considere, por exemplo, $E = \mathbb{R}^n$ com a adição vetorial usual, mas defina a multiplicação escalar · : $\mathbb{R} \times \mathbb{R}^n \to \mathbb{R}^n$ por $\lambda \cdot (x_1, ..., x_n) = (0, ..., 0)$ para todo $\lambda \in \mathbb{R}$ e $(x_1, ..., x_n) \in \mathbb{R}^n$ [^15]. É fácil verificar que os axiomas (V0) a (V3) são satisfeitos. No entanto, (V4) falha, pois $1 \cdot u = 0 \neq u$ para $u \neq 0$ [^15]. Exemplos menos triviais que mostram a independência de (V4) podem ser construídos usando o conceito de base, que ainda não foi definido [^15].

**Exemplos de Espaços Vetoriais**

A definição axiomática abrange uma vasta gama de estruturas matemáticas. Alguns exemplos fundamentais são listados abaixo, referenciando o Example 3.1 do contexto [^15, ^16].

> **Exemplo 3.1.** Vários conjuntos com operações apropriadas formam espaços vetoriais.
> 1. Os corpos $\mathbb{R}$ e $\mathbb{C}$ são espaços vetoriais sobre $\mathbb{R}$ [^15]. O próprio corpo K pode ser visto como um espaço vetorial sobre si mesmo, onde a adição de vetores é a adição do corpo e a multiplicação escalar é a multiplicação do corpo [^15].
> 2. Para qualquer $n \ge 1$, os grupos $\mathbb{R}^n$ (conjunto de n-tuplas $x = (x_1, ..., x_n)$) e $\mathbb{C}^n$ são espaços vetoriais sobre $\mathbb{R}$ [^13, ^15]. A adição é definida componente a componente: $(x_1, ..., x_n) + (y_1, ..., y_n) = (x_1+y_1, ..., x_n+y_n)$ [^13]. A multiplicação escalar é dada por $\lambda \cdot (x_1, ..., x_n) = (\lambda x_1, ..., \lambda x_n)$ para qualquer $\lambda \in \mathbb{R}$ [^13, ^15]. Além disso, $\mathbb{C}^n$ também é um espaço vetorial sobre $\mathbb{C}$ com a mesma definição de adição e multiplicação escalar, mas com $\lambda \in \mathbb{C}$ [^15]. Conectando com a motivação inicial [^1, ^2], o conjunto de vetores coluna $\mathbb{R}^{n \times 1}$ também forma um espaço vetorial sobre $\mathbb{R}$ com operações análogas [^2]. Frequentemente, $\mathbb{R}^n$ e $\mathbb{R}^{n \times 1}$ são identificados através de um isomorfismo natural [^2].
> 3. O anel $\mathbb{R}[X]_n$ de polinômios com coeficientes reais de grau no máximo $n$ é um espaço vetorial sobre $\mathbb{R}$. Similarmente, $\mathbb{C}[X]_n$, o anel de polinômios com coeficientes complexos de grau no máximo $n$, é um espaço vetorial sobre $\mathbb{C}$ [^15]. A multiplicação escalar $\lambda \cdot P(X)$ de um polinômio $P(X) = a_m X^m + ... + a_1 X + a_0$ (com $m \le n$) pelo escalar $\lambda$ é definida como $\lambda \cdot P(X) = (\lambda a_m) X^m + ... + (\lambda a_1) X + (\lambda a_0)$ [^15].
> 4. Os anéis $\mathbb{R}[X]$ (todos os polinômios com coeficientes reais) e $\mathbb{C}[X]$ (todos os polinômios com coeficientes complexos) são espaços vetoriais sobre $\mathbb{R}$ e $\mathbb{C}$, respectivamente, com a mesma definição de multiplicação escalar [^15].
> 5. O anel de matrizes quadradas $n \times n$, $M_n(\mathbb{R})$, e, mais geralmente, o anel de matrizes $m \times n$, $M_{m,n}(\mathbb{R})$, são espaços vetoriais sobre $\mathbb{R}$ [^15]. A adição de matrizes e a multiplicação por escalar são definidas entrada por entrada, como será formalizado na Seção 3.6 [^37, ^38].
> 6. O anel $C(a, b)$ de funções contínuas $f: (a, b) \to \mathbb{R}$ é um espaço vetorial sobre $\mathbb{R}$ [^15]. A adição de funções $(f+g)$ é definida por $(f+g)(x) = f(x) + g(x)$ e a multiplicação escalar $(\lambda f)$ por $(\lambda f)(x) = \lambda f(x)$, para todo $x \in (a, b)$ [^15].
> 7. De forma mais geral, seja X um conjunto não vazio e E um espaço vetorial sobre K. O conjunto de todas as funções $f: X \to E$ forma um espaço vetorial sobre K [^16]. As operações são definidas ponto a ponto: dados $f, g: X \to E$ e $\lambda \in K$, define-se $(f+g): X \to E$ por $(f+g)(x) = f(x) + g(x)$ e $(\lambda f): X \to E$ por $(\lambda f)(x) = \lambda f(x)$ para todo $x \in X$ [^16]. A verificação dos axiomas é direta [^16]. Este é um exemplo muito importante, pois o conjunto de transformações lineares entre dois espaços vetoriais, Hom(E, F), é um caso particular desta construção [^16].

### Conclusão

Este capítulo estabeleceu a **definição axiomática de um espaço vetorial** sobre um corpo K [^14]. Vimos como esta estrutura algébrica abstrata, definida por um conjunto de vetores e duas operações (adição vetorial e multiplicação escalar) que satisfazem os axiomas (V0)-(V4) [^14], captura as propriedades essenciais de \'linearidade\' observadas em diversos contextos matemáticos, como $\mathbb{R}^n$ [^15], espaços de polinômios [^15] e espaços de funções [^15, ^16]. As consequências imediatas dos axiomas, como as propriedades do vetor nulo e a Proposição 3.1 [^14], fornecem as ferramentas iniciais para trabalhar com esses espaços. A generalidade desta definição permite um tratamento unificado de muitos problemas em álgebra linear, análise e áreas aplicadas. Os próximos capítulos explorarão conceitos cruciais como independência linear, bases, dimensão e transformações lineares, todos construídos sobre a fundação estabelecida aqui.

### Referências
[^1]: Chapter 3 Vector Spaces, Bases, Linear Maps 3.1 Motivations: Linear Combinations, Linear Independence and Rank In linear optimization problems, we often encounter systems of linear equations. For example, consider the problem of solving the following system of three linear equations in the three variables X1,X2, X3 ∈ R: X1 + 2x2 X3 = 1 2X1+ X2 + X3 = 2 X1 2x2 - 2x3 = 3. One way to approach this problem is introduce the “vectors” u, v, w, and b, given by u = (1, 2, 1), v = (2, 1, -2), w = (-1, 1, -2) and b = (1, 2, 3) and to write our linear system as X₁U + X2V + x3w = b. In the above equation, we used implicitly the fact that a vector z can be multiplied by a scalar λ∈ R, where λz = λ(z1, z2, z3) = (λz1, λz2, λz3) and two vectors y and and z can be added, where y + z = (y1, y2, y3) + (z1, z2, z3) = (y1 + z1, y2 + z2, y3 + z3).
[^2]: 50 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS Also, given a vector x = (x1, x2, x3), we define the additive inverse -x of x (pronounced minus x) as -x = (-x1, -x2, -x3). Observe that -x = (-1)x, the scalar multiplication of x by -1. The set of all vectors with three components is denoted by R3×1. The reason for using the notation R3×1 rather than the more conventional notation R³ is that the elements of R3×1 are column vectors; they consist of three rows and a single column, which explains the superscript 3 × 1. On the other hand, R³ = R ×R× R consists of all triples of the form (X1,X2,X3), with X1,X2,X3 ∈ R, and these are row vectors. However, there is an obvious bijection between R3×1 and R³ and they are usually identified. For the sake of clarity, in this introduction, we will denote the set of column vectors with n components by Rn×1. An expression such as X₁U + X2V + x3W where u, v, w are vectors and the xis are scalars (in R) is called a linear combination. Using this notion, the problem of solving our linear system X₁U + X2V + x3w = b. is equivalent to determining whether b can be expressed as a linear combination of u,υ, ω.
[^13]: 61 3.2. VECTOR SPACES nonnegative real number ||A|| which behaves a lot like the absolute value |x| of a real number x. Then our goal is to find some low-rank matrix A\' that minimizes the norm ||A – A\'||², over all matrices A\' of rank at most k, for some given k ≪ min{m, n}. Some advantages of a low-rank approximation are: 1. Fewer elements are required to represent A; namely, k(m + n) instead of mn. Thus less storage and fewer operations are needed to reconstruct A. 2. Often, the process for obtaining the decomposition exposes the underlying structure of the data. Thus, it may turn out that “most" of the significant data are concentrated along some directions called principal directions. Low-rank decompositions of a set of data have a multitude of applications in engineering, including computer science (especially computer vision), statistics, and machine learning. As we will see later in Chapter 23, the singular value decomposition (SVD) provides a very satisfactory solution to the low-rank approximation problem. Still, in many cases, the data sets are so large that another ingredient is needed: randomization. However, as a first step, linear algebra often yields a good initial solution. We will now be more precise as to what kinds of operations are allowed on vectors. In the early 1900, the notion of a vector space emerged as a convenient and unifying framework for working with “linear” objects and we will discuss this notion in the next few sections. 3.2 Vector Spaces For every n ≥ 1, let Rn be the set of n-tuples x = (x1,...,xn). Addition can be extended to Rn as follows: (X1,...,xn) + (y1,..., Yn) = (X1 + Y1, ..., Xn + Yn). We can also define an operation •: R × Rn → R″ as follows: λ. (X1,...,xn) = (x1,..., λχη). The resulting algebraic structure has some interesting properties, those of a vector space. However, keep in mind that vector spaces are not just algebraic objects; they are also geometric objects. Vector spaces are defined as follows.
[^14]: 62 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS Definition 3.1. Given a field K (with addition + and multiplication *), a vector space over K (or K-vector space) is a set E (of vectors) together with two operations +: E × E → E (called vector addition),¹ and ·: K × E → E (called scalar multiplication) satisfying the following conditions for all α, β ∈ K and all u, v ∈ E; (VO) E is an abelian group w.r.t. +, with identity element 0;² (V1) α· (u + v) = (α· u) + (α·υ); (V2) (α + β)· u = (a • u) + (β· u); (V3) (α* β)· u = α · (β· u); (V4) 1 · u = น. In (V3), * denotes multiplication in the field K. Given a ∈ K and v ∈ E, the element α·υ is also denoted by av. The field K is often called the field of scalars. Unless specified otherwise or unless we are dealing with several different fields, in the rest of this chapter, we assume that all K-vector spaces are defined with respect to a fixed field K. Thus, we will refer to a K-vector space simply as a vector space. In most cases, the field K will be the field R of reals. From (VO), a vector space always contains the null vector 0, and thus is nonempty. From (V1), we get a.0 = 0, and a· (-v) = −(α·υ). From (V2), we get 0· υ = 0, and (-α)· υ = − (α·υ). Another important consequence of the axioms is the following fact: Proposition 3.1. For any u ∈ E and any > ∈ K, if > ≠ 0 and 1 · u = 0, then u = 0. Proof. Indeed, since 入 ≠ 0, it has a multiplicative inverse 入¯¹, so from λ· u = 0, we get 1-1. (λ· u) = λ−1 · 0. However, we just observed that 入¯¹ · 0 = 0, and from (V3) and (V4), we have 1-1. (λ. κ) = (λ¯¹1) · u = 1 · u = u, and we deduce that u = 0. ¹The symbol + is overloaded, since it denotes both addition in the field K and addition of vectors in E. It is usually clear from the context which + is intended. ² The symbol 0 is also overloaded, since it represents both the zero in K (a scalar) and the identity element of E (the zero vector). Confusion rarely arises, but one may prefer using 0 for the zero vector.
[^15]: 63 3.2. VECTOR SPACES Remark: One may wonder whether axiom (V4) is really needed. Could it be derived from the other axioms? The answer is no. For example, one can take E = Rn and define • : R × R″ → R" by λ. (X1,...,xn) = (0, . . ., 0) for all (x1,...,xn) ∈ R″ and all λ∈ R. Axioms (V0)–(V3) are all satisfied, but (V4) fails. Less trivial examples can be given using the notion of a basis, which has not been defined yet. The field K itself can be viewed as a vector space over itself, addition of vectors being addition in the field, and multiplication by a scalar being multiplication in the field. Example 3.1. 1. The fields R and Care vector spaces over R. 2. The groups Rn and Cn are vector spaces over R, with scalar multiplication given by (x1,...,xn) = (x1,..., λχη), for any 入∈ R and with (x1,...,xn) ∈ R″ or (x1,...,xn) ∈ C", and Cn is a vector space over C with scalar multiplication as above, but with λ∈ C. 3. The ring R[X]n of polynomials of degree at most n with real coefficients is a vector space over R, and the ring C[X]n of polynomials of degree at most n with complex coefficients is a vector space over C, with scalar multiplication λ· P(X) of a polynomial P(X) = amXm + am-1Xm-1 + ... + a₁X + ao (with aż ∈ R or aż ∈ C) by the scalar 入 (in R or C), with m ≤ n, given by λ· P(X) = λαmXm + Xam-1Xm−1 + ··· + λα₁X + λαο. 4. The ring R[X] of all polynomials with real coefficients is a vector space over R, and the ring C[X] of all polynomials with complex coefficients is a vector space over C, with the same scalar multiplication as above. 5. The ring of n × n matrices Mn(R) is a vector space over R. 6. The ring of m × n matrices Mm,n(R) is a vector space over R. 7. The ring C(a, b) of continuous functions f : (a,b) → R is a vector space over R, with the scalar multiplication Af of a function f: (a, b) → R by a scalar 入∈ R given by (f)(x) = f(x), for all x ∈ (a, b).\n[^16]: 64 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS 8. A very important example of vector space is the set of linear maps between two vector spaces to be defined in Section 11.1. Here is an example that will prepare us for the vector space of linear maps. Let X be any nonempty set and let E be a vector space. The set of all functions f : X → E can be made into a vector space as follows: Given any two functions f : X → E and g: X → E, let (f + g): X → E be defined such that (f+g)(x) = f(x) + g(x) for all x ∈ X, and for every λ ∈ R, let Af : X → E be defined such that (f)(x) = f(x) for all x ∈ X. The axioms of a vector space are easily verified. Let E be a vector space. We would like to define the important notions of linear combination and linear independence. Before defining these notions, we need to discuss a strategic choice which, depending how it is settled, may reduce or increase headaches in dealing with notions such as linear combinations and linear dependence (or independence). The issue has to do with using sets of vectors versus sequences of vectors.\n[^37]: 85 3.6. MATRICES Definition 3.12. If K = R or K = C, an m × n-matrix over K is a family (aij)1<i<m, 1<j<n of scalars in K, represented by an array ... In the special case where m = 1, we have a row vector, represented by (a11 ... a1n) and in the special case where n = 1, we have a column vector, represented by ... In these last two cases, we usually omit the constant index 1 (first index in case of a row, second index in case of a column). The set of all m × n-matrices is denoted by Mm,n(K) or Mm,n. An n×n-matrix is called a square matrix of dimension n. The set of all square matrices of dimension n is denoted by Mn(K), or Mn. Remark: As defined, a matrix A = (aij)1<i<m, 1<j<n is a family, that is, a function from {1,2,...,m} × {1,2,..., n} to K. As such, there is no reason to assume an ordering on the indices. Thus, the matrix A can be represented in many different ways as an array, by adopting different orders for the rows or the columns. However, it is customary (and usually convenient) to assume the natural ordering on the sets {1,2, . . ., m} and {1,2,..., n}, and to represent A as an array according to this ordering of the rows and columns. We define some operations on matrices as follows. Definition 3.13. Given two m × n matrices A = (aij) and B = (bij), we define their sum A + B as the matrix C = (cij) such that Cij = aij + bij; that is, ...\n[^38]: 86 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS For any matrix A = (aij), we let -A be the matrix (-ażj). Given a scalar λ ∈ K, we define the matrix AA as the matrix C = (cij) such that Cij = Xaij; that is ... Given an m × n matrices A = (aik) and an n× p matrices B = (bk j), we define their product AB as the m × p matrix C = (Cij) such that Cij = Σ aikbkj, for 1 ≤ i ≤ m, and 1 ≤ j ≤ p. In the product AB = C shown below ... note that the entry of index i and j of the matrix AB obtained by multiplying the matrices A and B can be identified with the product of the row matrix corresponding to the i-th row of A with the column matrix corresponding to the j-column of B: ... Definition 3.14. The square matrix In of dimension n containing 1 on the diagonal and 0 everywhere else is called the identity matrix. It is denoted by ... Definition 3.15. Given an m × n matrix A = (aij), its transpose AT = (a), is the n × m-matrix such that ai = aji, for all i, 1 ≤ i ≤ m, and all j, 1 ≤ j ≤ n. The transpose of a matrix A is sometimes denoted by At, or even by A. Note that the transpose AT of a matrix A has the property that the j-th row of AT is the j-th column of

<!-- END -->