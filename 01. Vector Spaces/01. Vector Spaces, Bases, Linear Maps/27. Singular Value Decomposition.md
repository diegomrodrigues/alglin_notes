## Capítulo 4: Singular Value Decomposition e Aplicações em Espaços Vetoriais

### Introdução

Expandindo os conceitos de espaços vetoriais, bases e transformações lineares apresentados no Capítulo 3, este capítulo aprofunda-se em uma das fatorações matriciais mais fundamentais e versáteis da álgebra linear: a **Singular Value Decomposition (SVD)**. Como vimos, sistemas de equações lineares $Ax = b$ são onipresentes e a capacidade de analisá-los e resolvê-los, mesmo que aproximadamente, é crucial [^p52_1]. A SVD fornece uma ferramenta poderosa não apenas para a análise teórica de transformações lineares, mas também para aplicações práticas robustas, incluindo a solução de sistemas lineares (mesmo aqueles sem solução exata) [^10], análise de dados e compressão. Uma transformação linear $x \mapsto Ax$ pode ser completamente caracterizada por uma matriz A [^p53_1], e a SVD decompõe essa matriz em componentes que revelam sua estrutura geométrica e algébrica intrínseca. Este capítulo detalhará a definição formal da SVD, explorando as propriedades das matrizes ortogonais [^p56_1] e dos valores singulares [^9] que a compõem. Subsequentemente, abordaremos aplicações-chave, como o cálculo da **pseudo-inversa** para encontrar soluções de mínimos quadrados [^2] [^6], a **Análise de Componentes Principais (PCA)** como uma técnica de análise de dados e redução de dimensionalidade [^3] [^7] [^17], e a **decomposição de baixo rank** para compressão de dados e revelação de estruturas subjacentes [^4] [^18].

### Conceitos Fundamentais

#### Definição da Singular Value Decomposition (SVD)

A Singular Value Decomposition é um resultado central da álgebra linear que afirma que qualquer matriz $A$ de dimensões $m \times n$ pode ser fatorada [^8].

> **Teorema (Singular Value Decomposition):** Qualquer matriz $A \in \mathbb{R}^{m \times n}$ pode ser decomposta na forma:
> $$ A = V \Sigma U^T $$
> onde:
> - $V$ é uma matriz ortogonal $m \times m$ [^1] [^5] [^8].
> - $U$ é uma matriz ortogonal $n \times n$ [^1] [^5] [^8].
> - $\Sigma$ é uma matriz $m \times n$ com entradas diagonais $(\Sigma)_{ii} = \sigma_i$ para $1 \le i \le p = \min(m, n)$, e todas as outras entradas $(\Sigma)_{ij}$ são zero [^8]. Os valores $\sigma_i$ são os **valores singulares** de $A$, são não-negativos e ordenados de forma decrescente: $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_p \ge 0$ [^9].

Lembremos que uma matriz $Q$ é ortogonal se $Q^T Q = Q Q^T = I$ [^p56_1]. Geometricamente, matrizes ortogonais representam transformações lineares que preservam comprimentos e ângulos (como rotações e reflexões) [^p54_1] [^p56_2]. A SVD, portanto, decompõe a transformação linear representada por $A$ em uma rotação/reflexão no espaço de domínio ($U^T$), um escalonamento ao longo dos eixos (dado pelos valores singulares em $\Sigma$), e uma rotação/reflexão no contra-domínio ($V$). Esta decomposição é fundamental para entender a geometria da transformação $x \mapsto Ax$ e tem vastas aplicações [^5].

#### Aplicações da SVD

##### Solução de Sistemas Lineares e a Pseudo-Inversa

Uma das aplicações mais importantes da SVD é na "solução" de sistemas lineares $Ax = b$, especialmente quando o sistema não possui solução única ou mesmo nenhuma solução, como em sistemas sobredeterminados ($m > n$) [^6] [^10]. Nestes casos, frequentemente buscamos uma solução aproximada que minimize alguma medida de erro [^11].

A abordagem clássica, utilizada por Legendre e Gauss, é encontrar um vetor $x$ que minimize a norma Euclidiana quadrática do erro, ou seja, $||Ax - b||_2^2$ [^11]. A SVD garante a existência de um vetor único $x^+$ com norma Euclidiana mínima que minimiza $||Ax - b||_2^2$ [^14]. Este vetor é dado pela expressão $x^+ = A^+ b$, onde $A^+$ é a **pseudo-inversa** (ou inversa de Moore-Penrose) de $A$ [^14].

A pseudo-inversa $A^+$ pode ser calculada diretamente a partir da SVD de $A = V \Sigma U^T$ [^2] [^6] [^12].

> **Definição (Pseudo-Inversa via SVD):** Dada a SVD de $A = V \Sigma U^T$, a pseudo-inversa $A^+$ é definida como:
> $$ A^+ = U \Sigma^+ V^T $$
> onde $\Sigma^+$ é obtida a partir da transposta de $\Sigma$ substituindo cada valor singular positivo $\sigma_i$ na diagonal pelo seu inverso $\sigma_i^{-1}$, e mantendo todas as entradas nulas [^13].

Portanto, a SVD fornece um método computacionalmente estável e teoricamente elegante para encontrar a melhor solução aproximada (no sentido de mínimos quadrados e norma mínima) para qualquer sistema linear $Ax = b$ [^2] [^6]. Vale notar que outras abordagens para encontrar soluções aproximadas ou regularizadas incluem a *ridge regression*, que minimiza $||Ax - b||_2^2 + K ||x||_2^2$ [^15], e o *lasso*, que minimiza $||Ax - b||_2^2 + K ||x||_1$ e tende a produzir soluções esparsas [^16].

##### Análise de Componentes Principais (PCA)

Outra aplicação proeminente da SVD é na **Análise de Componentes Principais (PCA)** [^3] [^17]. PCA é uma técnica fundamental em análise de dados e estatística, utilizada para identificar as direções (componentes principais) de maior variância em um conjunto de dados multidimensional [^7]. A SVD é a ferramenta matemática subjacente que permite calcular eficientemente esses componentes principais [^7] [^17]. Ao projetar os dados nesses componentes principais, a PCA permite a **redução de dimensionalidade**, mantendo o máximo de informação possível, e a **extração de características** (*feature extraction*), simplificando a estrutura dos dados para análise ou modelagem subsequente [^7]. A conexão entre SVD e PCA reside no fato de que os vetores singulares direitos (colunas de $U$) e os valores singulares ($\sigma_i$) estão diretamente relacionados às direções e variâncias dos componentes principais.

##### Decomposição de Baixo Rank

Em muitas aplicações do mundo real, como compressão de dados, os dados podem ser representados por uma matriz $A$ de grandes dimensões $m \times n$, mas que possui uma estrutura interna tal que seu rank efetivo é muito menor que $\min(m, n)$ [^18]. A **decomposição de baixo rank** (*low-rank decomposition*) visa explorar essa estrutura fatorando $A$ como um produto de duas matrizes menores, $A \approx BC$, onde $B$ é $m \times k$ e $C$ é $k \times n$, com $k \ll \min(m, n)$ [^4] [^18].

Os benefícios dessa aproximação são significativos:
1.  **Redução de Armazenamento:** Requer apenas $k(m + n)$ elementos para armazenar $B$ e $C$, em vez de $mn$ para $A$ [^p61_1].
2.  **Exposição da Estrutura:** O processo de decomposição frequentemente revela a estrutura latente ou as "direções principais" nos dados [^p61_2].

Geralmente, busca-se a melhor aproximação de baixo rank, ou seja, encontrar uma matriz $A'$ de rank no máximo $k$ que minimize a norma da diferença $||A - A'||^2$ (usualmente a norma de Frobenius, relacionada à norma Euclidiana quadrática mencionada anteriormente [^11]) [^19]. A SVD fornece uma solução ótima para este problema de aproximação de baixo rank [^20]. Especificamente, a melhor aproximação de rank $k$ para $A$ é obtida truncando a SVD de $A$, mantendo apenas os $k$ maiores valores singulares e os correspondentes vetores singulares em $V$ e $U$.

### Conclusão

A Singular Value Decomposition (SVD), $A = V \Sigma U^T$, emerge como uma ferramenta matemática de extraordinária importância, solidamente fundamentada nos conceitos de espaços vetoriais, bases e transformações lineares discutidos no Capítulo 3. Sua capacidade de decompor qualquer matriz $m \times n$ em matrizes ortogonais e uma matriz diagonal de valores singulares não apenas elucida a geometria da transformação linear associada, mas também habilita uma gama diversificada de aplicações avançadas. Demonstramos como a SVD fornece um método robusto para calcular a pseudo-inversa $A^+$, permitindo encontrar a solução de norma mínima e mínimos quadrados para sistemas lineares $Ax = b$, mesmo os mal-condicionados ou sem solução exata [^2] [^6] [^14]. Além disso, destacamos seu papel central na Análise de Componentes Principais (PCA) para análise exploratória de dados, redução de dimensionalidade e extração de características [^7] [^17], e na construção de aproximações de baixo rank ótimas para compressão de dados e identificação de estruturas latentes [^4] [^18] [^20]. A SVD representa, portanto, um pilar essencial tanto na teoria quanto na prática da álgebra linear moderna e suas aplicações em ciência de dados, engenharia e estatística.

### Referências

[^1]: SVD is expressed as A = VΣUᵀ, where V and U are orthogonal matrices, and Σ is a matrix with singular values on the diagonal.
[^2]: The pseudo-inverse A⁺, computed from SVD as A⁺ = UΣ⁺Vᵀ, provides an approximate solution to linear systems, minimizing the error ||Ax - b||₂.
[^3]: Principal Component Analysis (PCA) is an application of SVD used for data analysis.
[^4]: Low-rank decomposition factors a matrix A into B and C, where A ≈ BC, reducing storage and exposing underlying data structures.
[^5]: Singular Value Decomposition (SVD) decomposes a matrix A into A = VΣUᵀ, where V and U are orthogonal matrices, and Σ is a diagonal matrix containing the singular values of A; SVD is used for solving linear systems and dimensionality reduction.
[^6]: The pseudo-inverse A⁺ of a matrix A, computed using SVD, provides a least-squares solution to the linear system Ax = b, especially when the system is overdetermined (more equations than variables).
[^7]: Principal Component Analysis (PCA) is a technique that uses SVD to identify the most significant components in a dataset, allowing for dimensionality reduction and feature extraction.
[^8]: (p. 56) A major result of linear algebra states that every m × n matrix A can be written as A = VΣUᵀ, where V is an m × m orthogonal matrix, U is an n × n orthogonal matrix, and Σ is an m × n matrix whose only nonzero entries are nonnegative diagonal entries...
[^9]: (p. 56) ...nonnegative diagonal entries σ₁ ≥ σ₂ ≥ ... ≥ σₚ, where p = min(m, n), called the singular values of A.
[^10]: (p. 56) The SVD can be used to “solve” a linear system Ax = b where A is an m×n matrix, even when this system has no solution. This may happen when there are more equations that variables (m > n), in which case the system is overdetermined.
[^11]: (p. 56) Of course, there is no miracle, an unsolvable system has no solution. But we can look for a good approximate solution, namely a vector x that minimizes some measure of the error Ax - b. Legendre and Gauss used ||Ax - b||₂², which is the squared Euclidean norm of the error.
[^12]: (p. 56) Furthermore, x⁺ is given by the expression x⁺ = A⁺b, where A⁺ is the pseudo-inverse of A, and A⁺ can be computed from an SVD A = VΣUᵀ of A. Indeed, A⁺ = UΣ⁺Vᵀ...
[^13]: (p. 56) ...where Σ⁺ is the matrix obtained from Σ by replacing every positive singular value σᵢ by its inverse σᵢ⁻¹, leaving all zero entries intact, and transposing.
[^14]: (p. 56) This quantity is differentiable, and it turns out that there is a unique vector x⁺ of minimum Euclidean norm that minimizes ||Ax - b||₂². Furthermore, x⁺ is given by the expression x⁺ = A⁺b...
[^15]: (p. 56) Instead of searching for the vector of least Euclidean norm minimizing ||Ax - b||₂², we can add the penalty term K ||x||₂² (for some positive K > 0) to ||Ax - b||₂² and minimize the quantity ||Ax - b||₂² + K ||x||₂². This approach is called ridge regression.
[^16]: (p. 56) Another approach is to replace the penalty term K ||x||₂² by K ||x||₁, where ||x||₁ = |x₁| + ... + |xn| (the l¹-norm of x)... This approach known as lasso is popular in machine learning...\
[^17]: (p. 57) Another important application of the SVD is principal component analysis (or PCA), an important tool in data analysis.
[^18]: (p. 60) Another great example of a real-world problem where linear algebra proves to be very effective is the problem of data compression... Typically the data set is represented as an m × n matrix A... the goal of low-rank decomposition is to factor A as the product of two matrices B and C, where B is a m×k matrix and C is a k × n matrix, with k ≪ min{m, n}...\
[^19]: (p. 61) Then our goal is to find some low-rank matrix A' that minimizes the norm ||A – A'||², over all matrices A' of rank at most k, for some given k ≪ min{m, n}.
[^20]: (p. 61) As we will see later in Chapter 23, the singular value decomposition (SVD) provides a very satisfactory solution to the low-rank approximation problem.
[^p52_1]: (p. 52) ...so our linear system is expressed by... Ax = b.
[^p53_1]: (p. 53) The point of view where our linear system is expressed in matrix form as Ax = b stresses the fact that the map x → Ax is a linear transformation.
[^p54_1]: (p. 54) The (square) matrices Q that preserve the inner product... also play a very important role. They can be thought of as generalized rotations.
[^p56_1]: (p. 56) An n× n matrix Q such that Q Qᵀ = Qᵀ Q = In is called an orthogonal matrix.
[^p56_2]: (p. 56) Orthogonal matrices play an important role. Geometrically, they correspond to linear transformation that preserve length.
[^p61_1]: (p. 61) 1. Fewer elements are required to represent A; namely, k(m + n) instead of mn. Thus less storage and fewer operations are needed to reconstruct A.
[^p61_2]: (p. 61) 2. Often, the process for obtaining the decomposition exposes the underlying structure of the data. Thus, it may turn out that “most" of the significant data are concentrated along some directions called principal directions.

<!-- END -->