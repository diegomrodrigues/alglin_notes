{
  "topics": [
    {
      "topic": "Vector Spaces, Bases, Linear Maps",
      "sub_topics": [
        "A vector space over a field \\(K\\) is a set \\(E\\) with vector addition and scalar multiplication operations that satisfy specific axioms, including \\(E\\) being an abelian group under addition. Scalar multiplication is compatible with field multiplication and distributes over vector addition: \\(\\alpha \\cdot (u + v) = (\\alpha \\cdot u) + (\\alpha \\cdot v)\\), \\((\\alpha + \\beta) \\cdot u = (\\alpha \\cdot u) + (\\beta \\cdot u)\\), and \n\\((\\alpha * \\beta) \\cdot u = \\alpha \\cdot (\\beta \\cdot u)\\). The multiplicative identity in the field acts as an identity for scalar multiplication: \\(1 \\cdot u = u\\). Examples include the fields R and C, and the sets R^n and C^n.",
        "A subspace is a subset of a vector space that is itself a vector space, closed under vector addition and scalar multiplication, inheriting operations from the parent space. The intersection of any family of subspaces of a vector space \\(E\\) is a subspace. For any subspace \\(F\\) of a vector space \\(E\\), any linear combination of vectors in \\(F\\) is also in \\(F\\). The span of a set S, denoted Span(S), is the smallest subspace containing S, consisting of all finite linear combinations of elements from S.",
        "A linear combination of vectors \\(u, v, w\\) is an expression of the form \\(x_1u + x_2v + x_3w\\), where \\(x_i\\) are scalars. Solving linear systems can be framed as determining if a vector \\(b\\) can be expressed as a linear combination of other vectors. Linear combinations can be restricted to affine combinations (\\(\\sum \\lambda_i = 1\\)), positive combinations (\\(\\lambda_i \\geq 0\\)), or convex combinations (\\(\\sum \\lambda_i = 1\\) and \\(\\lambda_i \\geq 0\\)), each with specific geometric interpretations. A family of scalars (\\(\\lambda_i\\))_{i \\in I} has finite support if \\(\\lambda_i = 0\\) for all i outside a finite subset of I; this condition ensures that infinite sums of the form \\(\\sum \\lambda_i u_i\\) are well-defined. A vector \\(v\\) is a linear combination of a family \\((u_i)_{i \\in I}\\) if there exists a family of scalars \\((\\lambda_i)_{i \\in I}\\) such that \\(v = \\sum \\lambda_i u_i\\).",
        "Linear independence of vectors \\(u, v, w\\) means no non-trivial linear combination equals zero, i.e., \\(x_1u + x_2v + x_3w = 0\\) only if \\(x_1 = x_2 = x_3 = 0\\). Linear independence is crucial for unique representation of vectors and solutions to linear systems. Linear independence in vector spaces means that no vector in a set can be expressed as a linear combination of the others. A family of vectors \\((u_i)_{i \\in I}\\) is linearly independent if the equation \\(\\sum_{i \\in I} \\lambda_i u_i = 0\\) implies that \\(\\lambda_i = 0\\) for all \\(i \\in I\\). A family of vectors \\((u_i)_{i \\in I}\\) is linearly dependent if there exists a family of scalars \\((\\lambda_i)_{i \\in I}\\) such that \\(\\sum_{i \\in I} \\lambda_i u_i = 0\\) and \\(\\lambda_j \\neq 0\\) for some \\(j \\in I\\).",
        "A family \\((u_i)_{i \\in I}\\) of vectors \\(v_i \\in V\\) spans \\(V\\) or generates \\(V\\) if for every \\(v \\in V\\), there is some family of scalars \\((\\lambda_i)_{i \\in I}\\) such that \\(v = \\sum_{i \\in I} \\lambda_i v_i\\). A family \\((u_i)_{i \\in I}\\) that spans \\(V\\) and is linearly independent is called a basis of \\(V\\). If \\(v \\in E\\) is not a linear combination of \\((u_i)_{i \\in I}\\), then the family \\((u_i)_{i \\in I} \\cup \\{v\\}\\) obtained by adding \\(v\\) to the family \\((u_i)_{i \\in I}\\) is linearly independent. Given any finite family \\(S = (u_i)_{i \\in I}\\) generating a vector space \\(E\\) and any linearly independent subfamily \\(L = (u_j)_{j \\in J}\\) of \\(S\\) (where \\(J \\subseteq I\\)), there is a basis \\(B\\) of \\(E\\) such that \\(L \\subseteq B \\subseteq S\\). A family \\((v_i)_{i \\in I}\\) is a maximal linearly independent family of \\(E\\) if it is linearly independent, and if for any vector \\(w \\in E\\), the family \\((v_i)_{i \\in I} \\cup \\{w\\}\\) obtained by adding \\(w\\) to the family \\((v_i)_{i \\in I}\\) is linearly dependent. A family \\((v_i)_{i \\in I}\\) is a minimal generating family of \\(E\\) if it spans \\(E\\), and if for any index \\(p \\in I\\), the family \\((v_i)_{i \\in I \\setminus \\{p\\}}\\) obtained by removing \\(v_p\\) from the family \\((v_i)_{i \\in I}\\) does not span \\(E\\). A basis in a vector space is an efficient generating family, where every vector in the space can be uniquely expressed as a linear combination of the basis vectors. Given a linearly independent family (u\\u1d62) of elements in a vector space E, if a vector v is not a linear combination of (u\\u1d62), then adding v to the family preserves linear independence. A maximal linearly independent family of E is a linearly independent set to which no other vector can be added without losing linear independence. A minimal generating family is a set that spans E, but removing any vector from the set results in a set that no longer spans E. A set of vectors {e\u2081, ..., e\u2099} forms a basis for a vector space E if every vector v \u2208 E can be uniquely written as a linear combination v = \u03bb\u2081e\u2081 + ... + \u03bb\u2099e\u2099; the scalars \u03bb\u2081, ..., \u03bb\u2099 are the coordinates of v with respect to the basis. For any two bases of a vector space E, the index sets have the same cardinality. If E has a finite basis of n elements, every basis of E has n elements, and this number is the dimension of E.",
        "The determinant \\(\\text{det}(u, v, w)\\) is a numerical quantity used to determine linear independence; non-zero determinant implies linear independence. Other methods like LU decomposition, QR decomposition, and Singular Value Decomposition (SVD) are used for larger systems. The determinant, det(u, v, w), is a numerical quantity that indicates whether the vectors u, v, w are linearly independent; if det(u, v, w) \u2260 0, the vectors are linearly independent.",
        "A linear transformation is a map \\(x \\rightarrow Ax\\) that preserves vector addition and scalar multiplication, satisfying \\(A(\\lambda x) = \\lambda(Ax)\\) and \\(A(u + v) = Au + Av\\). Expressing a linear system as \\(Ax = b\\) highlights this transformation. A linear map (or linear transformation) between vector spaces E and F is a function f: E \\u2192 F that preserves vector addition and scalar multiplication, i.e., f(x + y) = f(x) + f(y) and f(\\u03bbx) = \\u03bbf(x) for all x, y \\u2208 E and \\u03bb \\u2208 K.",
        "The image of a linear map f: E \\u2192 F, denoted Im f, is the set of all vectors in F that are the image of some vector in E. The kernel of a linear map \\(f: E \\rightarrow F\\) is defined as \\(\\\\text{Ker } f = \\\\{x \\\\in E \\\\mid f(x) = 0\\\\}\\\\). The kernel (or nullspace) of f, denoted Ker f, is the set of all vectors in E that are mapped to the zero vector in F. The image of a linear map is a subspace of the codomain, and the kernel is a subspace of the domain. A linear map \\(f: E \\\\rightarrow F\\\\) is injective if and only if \\(\\\\text{Ker } f = \\\\{0\\\\}\\\\). A linear map f: E \\u2192 F is injective if and only if its kernel is the trivial subspace {0}.",
        "The rank \\(\\text{rk}(f)\\) of a linear map \\(f: E \\rightarrow F\\) is defined as the dimension of the image \\(\\text{Im } f\\).  It provides a measure of the linear map's ability to span the codomain.",
        "Given any two vector spaces \\(E\\) and \\(F\\), given any basis \\((u_i)_{i \\in I}\\) of \\(E\\), and given any other family of vectors \\((v_i)_{i \\in I}\\) in \\(F\\), there is a unique linear map \\(f: E \\rightarrow F\\) such that \\(f(u_i) = v_i\\) for all \\(i \\in I\\). This property is crucial for defining linear maps and understanding their behavior.",
        "The set of all linear maps from E to F, denoted Hom(E, F) or L(E; F), forms a vector space under the operations of pointwise addition and scalar multiplication. The dimension of Hom(E, F) is related to the dimensions of E and F.",
        "For any set I, there exists a vector space K^(I) and an injection \u03b9: I \\u2192 K^(I) such that, for any vector space F and any function f: I \\u2192 F, there is a unique linear map f: K^(I) \\u2192 F such that f = f \\u2218 \u03b9. This indicates that K^(I) is the vector space freely generated by I.",
        "If a linear map f has a left inverse g (i.e., g \\u2218 f = id), then f is an isomorphism, and g is the inverse of f. If a linear map f has a right inverse h (i.e., f \\u2218 h = id), then f is an isomorphism, and h is the inverse of f.",
        "If (u\\u1d62) is a basis of E, and f: E \\u2192 F is a linear map that is an isomorphism, then (f(u\\u1d62)) is a basis of F. This property is essential for understanding how linear maps transform bases.",
        "The vector space \\(\\text{Hom}(E, K)\\) of linear maps from \\(E\\) to the field \\(K\\) is called the dual space (or dual) of \\(E\\), also denoted by \\(E^*\\). The linear maps in \\(E^*\\) are called the linear forms, or covectors. Given a vector space E of finite dimension n, and (u\u2081, ..., u\u2099) is a basis of E, then for any linear form f* \\u2208 E*, f*(x) = \u03bb\u2081x\u2081 + ... + \u03bb\u2099x\u2099, where \u03bb\u1d62 = f*(u\u1d62) \\u2208 K for every i.",
        "Given a vector space \\(E\\) and any basis \\((u_i)_{i \\in I}\\) for \\(E\\), for every \\(i \\in I\\), there is a unique linear form \\(u^i\\) such that \\(u^i(u_j) = 1\\) if \\(i = j\\) and \\(u^i(u_j) = 0\\) if \\(i \\neq j\\). For every basis (u\u2081, ..., u\u2099) of \\(E\\), the family of coordinate forms (u\u00b9, ..., u\u207f) is a basis of \\(E^*\\) (called the dual basis of (u\u2081, ..., u\u2099)).",
        "Given a vector space E and a subspace M, the quotient space E/M is the set of equivalence classes under the relation where u \\u2261 v if and only if u - v \\u2208 M.  It is a way to construct a new vector space by identifying elements that differ by elements of a subspace.",
        "Addition and scalar multiplication are defined on the quotient space E/M as [u] + [v] = [u + v] and \\u03bb[u] = [\\u03bbu].  These operations are well-defined, meaning that they do not depend on the specific choice of representatives in the equivalence classes.",
        "The natural projection \u03c0: E \\u2192 E/M, defined by \u03c0(u) = [u], is a surjective linear map.  This map provides a way to relate the original vector space E to the quotient space E/M.",
        "Given any linear map f: E \\u2192 F, the image Im f is isomorphic to the quotient space E/Ker f.  This result, known as the first isomorphism theorem, connects the concepts of image, kernel, and quotient space.",
        "The inner product of two vectors \\(x = (x_1, ..., x_n)\\) and \\(y = (y_1, ..., y_n)\\) is defined as \\(x \\cdot y = \\sum_{i=1}^{n} x_i y_i\\), which quantifies the relationship between vectors, leading to the Euclidean norm \\((||x|| = \\sqrt{x \\cdot x})\\) and the concept of orthogonality when \\(x \\cdot y = 0\\). Orthogonal vectors have an inner product of zero, indicating they are perpendicular. The inner product of two vectors x = (x\u2081, ..., x\u2099) and y = (y\u2081, ..., y\u2099) in R\u207f is defined as x \\u22c5 y = \u03a3\u1d62 x\u1d62y\u1d62; it quantifies the projection of one vector onto another. Orthogonal matrices preserve the inner product, representing generalized rotations. The Euclidean norm (or l\u00b2-norm) of a vector \\(x\\) is the square root of the sum of the squares of its components, denoted as \\((||x||_2)\\), representing the length or magnitude of the vector.",
        "A matrix \\(Q\\) is orthogonal if \\(QQ^T = Q^TQ = I_n\\), corresponding to length-preserving linear transformations. Orthogonal matrices Q satisfy QQ\u1d40 = Q\u1d40Q = I\u2099, where Q\u1d40 is the transpose of Q and I\u2099 is the identity matrix. Orthogonal matrices preserve length and correspond to linear transformations that preserve length.",
        "The transpose of a matrix \\(A\\) is formed by interchanging its rows and columns, denoted as \\(A^T\\), playing a key role in adjoints, inner products, and symmetric matrices. Given an \\(m \\times n\\) matrix \\(A = (a_{ij})\\), its transpose \\(A^T = (a'_{ij})\\) is the \\(n \\times m\\)-matrix such that \\(a'_{ji} = a_{ij}\\) for all \\(i, 1 \\leq i \\leq m\\), and all \\(j, 1 \\leq j \\leq n\\).",
        "If a square matrix \\(A\\) has an inverse \\(A^{-1}\\) such that \\(AA^{-1} = A^{-1}A = I_n\\), it is invertible or nonsingular. A square matrix is invertible if and only if its columns are linearly independent, which is also equivalent to its determinant being nonzero. The solution to \\(Ax = b\\) is \\(x = A^{-1}b\\) if \\(A\\) is invertible. A matrix is invertible if there exists another matrix that, when multiplied, results in the identity matrix. Also called nonsingular, possessing linearly independent columns and a non-zero determinant. A square matrix A is invertible if and only if for any vector x, the equation Ax = 0 implies that x = 0. If a square matrix A is invertible, its columns are linearly independent. Conversely, if the columns of A are linearly independent, then A is invertible.",
        "The identity matrix, denoted as \\(I\\) or \\(I_n\\), is a square matrix with ones on the main diagonal and zeros elsewhere, acting as the multiplicative identity for matrix multiplication.",
        "Matrix multiplication is defined such that (AB)\u1d62\u2c7c = \u03a3\u2096 a\u1d62\u2096b\u2096\u2c7c. Unlike real or complex number multiplication, matrix multiplication is generally noncommutative (AB \u2260 BA). Given an \\(m \\times n\\) matrix \\(A = (a_{ik})\\) and an \\(n \\times p\\) matrix \\(B = (b_{kj})\\), their product \\(AB\\) is the \\(m \\times p\\) matrix \\(C = (c_{ij})\\) such that \\(c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\\) for \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq p\\).",
        "Singular Value Decomposition (SVD) is expressed as A = V\u03a3U\u1d40, where V and U are orthogonal matrices, and \u03a3 is a matrix with singular values on the diagonal. The pseudo-inverse A\u207a, computed from SVD as A\u207a = U\u03a3\u207aV\u1d40, provides an approximate solution to linear systems, minimizing the error ||Ax - b||\u2082. Principal Component Analysis (PCA) is an application of SVD used for data analysis. Low-rank decomposition factors a matrix A into B and C, where A \u2248 BC, reducing storage and exposing underlying data structures. Singular Value Decomposition (SVD) decomposes a matrix A into A = V\u03a3U\u1d40, where V and U are orthogonal matrices, and \u03a3 is a diagonal matrix containing the singular values of A; SVD is used for solving linear systems and dimensionality reduction. The pseudo-inverse A\u207a of a matrix A, computed using SVD, provides a least-squares solution to the linear system Ax = b, especially when the system is overdetermined (more equations than variables). Principal Component Analysis (PCA) is a technique that uses SVD to identify the most significant components in a dataset, allowing for dimensionality reduction and feature extraction."
      ]
    }
  ]
}