## Singular Value Decomposition para Compressão de Dados

### Introdução
Este capítulo explora a aplicação da **Singular Value Decomposition (SVD)** na compressão de dados. Como mencionado anteriormente [^1], o SVD é uma ferramenta poderosa para decompor matrizes e revelar sua estrutura subjacente. Uma das aplicações mais notáveis é a compressão de dados, onde o SVD permite aproximar uma matriz por outra de menor rank, reduzindo assim o espaço de armazenamento necessário. O objetivo é apresentar os conceitos e resultados teóricos que justificam o uso do SVD para compressão, com foco no **Teorema de Eckart-Young**.

### Conceitos Fundamentais
A compressão de dados usando SVD baseia-se na ideia de que uma matriz $A$ de dimensões $m \times n$ e rank $r$ pode ser aproximada por uma matriz $B$ de rank $k \leq r$ de tal forma que a diferença entre $A$ e $B$ seja minimizada em alguma norma [^1]. Especificamente, o objetivo é encontrar $B$ que minimize $||A - B||_2$ (norma espectral) ou $||A - B||_F$ (norma de Frobenius).

O **Teorema de Eckart-Young** [^1] fornece uma solução para este problema. Ele afirma que a melhor aproximação de rank $k$ de uma matriz $A$ é obtida somando os primeiros $k$ termos da decomposição SVD de $A$. Formalmente, se $A = VDU^T$ é a decomposição SVD de $A$, onde $V$ e $U$ são matrizes ortogonais e $D$ é uma matriz diagonal contendo os valores singulares $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r > 0$, então a matriz $A_k$ definida como:

$$A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^T$$

minimiza $||A - B||_2$ e $||A - B||_F$ para qualquer matriz $B$ de rank $k$ [^1]. Aqui, $u_i$ e $v_i$ são as colunas de $U$ e $V$, respectivamente.

**Demonstração do Teorema de Eckart-Young (para a norma espectral)**

Seja $A$ uma matriz $m \times n$ com SVD $A = VDU^T$, onde $D = diag(\sigma_1, \sigma_2, ..., \sigma_r, 0, ..., 0)$ e $\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_r > 0$ são os valores singulares de $A$. Definimos $A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^T$. Então, $A_k$ tem rank $k$ e podemos escrever $A - A_k = \sum_{i=k+1}^{r} \sigma_i u_i v_i^T$.

Pela propriedade da norma espectral, $||A - A_k||_2 = \sigma_{k+1}$. Agora, precisamos mostrar que para qualquer matriz $B$ de rank $k$, $||A - B||_2 \geq \sigma_{k+1}$.

Suponha que $B$ é uma matriz de rank $k$. Então, o núcleo de $B$, $Ker(B)$, tem dimensão $n - k$. Seja $U_{k+1}$ o subespaço gerado pelos primeiros $k+1$ vetores singulares à direita de $A$, ou seja, $U_{k+1} = span\\{u_1, u_2, ..., u_{k+1}\\}$. A dimensão de $U_{k+1}$ é $k+1$.

Como $dim(Ker(B)) + dim(U_{k+1}) = n - k + k + 1 = n + 1 > n$, existe um vetor unitário $z$ tal que $z \in Ker(B) \cap U_{k+1}$. Como $z \in U_{k+1}$, podemos escrever $z = \sum_{i=1}^{k+1} c_i u_i$, onde $\sum_{i=1}^{k+1} |c_i|^2 = 1$.

Agora, considere $||(A - B)z||_2 = ||Az - Bz||_2 = ||Az||_2$, pois $Bz = 0$. Como $Az = A\sum_{i=1}^{k+1} c_i u_i = \sum_{i=1}^{k+1} c_i A u_i = \sum_{i=1}^{k+1} c_i \sigma_i v_i$, temos:

$$||Az||_2^2 = ||\sum_{i=1}^{k+1} c_i \sigma_i v_i||_2^2 = \sum_{i=1}^{k+1} |c_i|^2 \sigma_i^2 \geq \sigma_{k+1}^2 \sum_{i=1}^{k+1} |c_i|^2 = \sigma_{k+1}^2$$

Portanto, $||(A - B)z||_2 \geq \sigma_{k+1}$. Como $||A - B||_2 = \sup_{||x||_2 = 1} ||(A - B)x||_2$, segue que $||A - B||_2 \geq \sigma_{k+1}$. $\blacksquare$

**Armazenamento e Compressão**
A matriz $A_k$ pode ser armazenada usando $(m + n)k$ entradas [^1]. Isso ocorre porque $A_k$ é formada por $k$ vetores $u_i$ de tamanho $m$ e $k$ vetores $v_i$ de tamanho $n$, juntamente com $k$ valores singulares $\sigma_i$. Portanto, o número total de entradas é $mk + nk + k$, que pode ser aproximado por $(m+n)k$, se $k$ for muito menor que $m$ e $n$.
Quando $k$ é significativamente menor que $m$ e $n$, essa representação oferece um ganho substancial em termos de espaço de armazenamento [^1].

**Exemplo Ilustrativo**
Considere uma imagem representada por uma matriz $A$ de $1000 \times 1000$ pixels. Armazenar esta imagem requer $10^6$ entradas. Suponha que a decomposição SVD de $A$ revele que apenas os primeiros 100 valores singulares são significativos. Então, podemos aproximar $A$ por $A_{100}$, que requer apenas $(1000 + 1000) \times 100 = 200,000$ entradas. Isso representa uma compressão de um fator de 5, mantendo uma boa aproximação da imagem original.

### Conclusão
A compressão de dados via SVD é uma técnica poderosa que explora a estrutura inerente das matrizes de dados. O Teorema de Eckart-Young garante que a aproximação obtida é ótima no sentido de minimizar a norma espectral ou a norma de Frobenius da diferença entre a matriz original e a aproximação. A capacidade de reduzir significativamente o espaço de armazenamento, mantendo uma boa aproximação dos dados, torna o SVD uma ferramenta valiosa em diversas aplicações, incluindo processamento de imagens, análise de dados e aprendizado de máquina.

### Referências
[^1]: Capítulo 23, Applications of SVD and Pseudo-Inverses.
<!-- END -->