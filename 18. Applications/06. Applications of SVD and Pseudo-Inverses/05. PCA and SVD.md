## Capítulo 23.4: Análise de Componentes Principais (PCA)

### Introdução
Este capítulo explora a Análise de Componentes Principais (PCA), uma técnica fundamental para identificar padrões em dados e entender sua estrutura de variância-covariância [^1]. A PCA é uma das aplicações da Decomposição em Valores Singulares (SVD), conforme mencionado no início deste capítulo [^1]. O objetivo da PCA é transformar dados em um novo sistema de coordenadas onde a maior variância pela projeção dos dados esteja na primeira coordenada (chamada de primeiro componente principal), a segunda maior variância na segunda coordenada e assim por diante.

### Conceitos Fundamentais

Dado um conjunto de dados $X = \{X_1, ..., X_n\}$, onde cada $X_i \in \mathbb{R}^d$, a PCA envolve várias etapas [^1]:

1.  **Centralização dos Dados:** Transladar a origem para o centroide $\mu$ dos $X_i$'s. O centroide $\mu$ é definido como:
    $$\
    \mu = \frac{1}{n} \sum_{i=1}^{n} X_i
    $$
    Este passo garante que os componentes principais capturem a variabilidade em torno da média.

2.  **Projeção e Maximização da Variância:** Os componentes principais são projeções não correlacionadas $Y$ dos pontos de dados em direções $v$ tais que a variância de $Y$ é maximizada. Matematicamente, buscamos encontrar vetores $v$ que maximizem a variância da projeção $Y = (X - \mu)v$.

3.  **Teorema Central:** O teorema principal da PCA afirma que a SVD de $X$ fornece os componentes principais. O $k$-ésimo componente principal é dado por $Y_k = (X - \mu)u_k$, onde $u_k$ é a $k$-ésima coluna de $U$ (a matriz de vetores singulares à esquerda na SVD de $X - \mu$).

**Detalhes Matemáticos e Derivações**

A variância da projeção $Y$ ao longo da direção $v$ é dada por:
$$\
\text{var}(Y) = \frac{1}{n-1} \sum_{i=1}^{n} ((X_i - \mu)v)^2
$$
Esta variância pode ser expressa em termos da matriz de covariância $\Sigma$ de $X$:
$$\
\Sigma = \frac{1}{n-1} (X - \mu)^T (X - \mu)
$$
Assim, a variância da projeção se torna:
$$\
\text{var}(Y) = v^T \Sigma v
$$
Maximizar $\text{var}(Y)$ sujeito a $\|v\| = 1$ (para evitar soluções triviais) leva a encontrar os autovetores de $\Sigma$. Os autovetores de $\Sigma$ são os componentes principais, e os autovalores correspondentes representam a variância explicada por cada componente principal.

**Conexão com a SVD**

A SVD de $X - \mu$ é dada por:
$$\
X - \mu = U D V^T
$$
onde:

*   $U$ é uma matriz ortogonal cujas colunas são os vetores singulares à esquerda.
*   $V$ é uma matriz ortogonal cujas colunas são os vetores singulares à direita.
*   $D$ é uma matriz diagonal com os valores singulares $\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_d$ na diagonal.

A matriz de covariância $\Sigma$ pode ser expressa em termos da SVD de $X - \mu$:
$$\
\Sigma = \frac{1}{n-1} (X - \mu)^T (X - \mu) = \frac{1}{n-1} V D^2 V^T
$$
Isto mostra que os vetores singulares à direita $V$ são os autovetores de $\Sigma$, e os valores singulares ao quadrado, divididos por $n-1$, são os autovalores de $\Sigma$. Portanto, as colunas de $V$ fornecem as direções dos componentes principais, e os valores singulares correspondentes indicam a quantidade de variância que cada componente explica.

**Algoritmo PCA**

1.  Calcular o centroide $\mu$ do conjunto de dados $X$.
2.  Centralizar os dados subtraindo o centroide de cada ponto de dados: $X_{centered} = X - \mu$.
3.  Calcular a SVD da matriz centralizada: $X_{centered} = U D V^T$.
4.  Os componentes principais são as colunas de $V$.
5.  A variância explicada por cada componente principal é dada pelos valores singulares ao quadrado correspondentes, divididos por $n-1$.

### Conclusão

A PCA é uma ferramenta poderosa para redução de dimensionalidade e análise de dados, conforme mencionado na seção 23.4 do texto [^1]. Ao identificar os componentes principais, podemos representar os dados em um espaço de dimensão inferior, preservando a maior parte da variância original. Isso simplifica a análise e modelagem dos dados, além de reduzir o custo computacional. Conforme discutido na seção 23.4, a PCA é útil para tarefas como redução de dados e interpretação [^1].

### Referências
[^1]: Chapter 23 Applications of SVD and Pseudo-Inverses
<!-- END -->