## Projeções Ortogonais e o Pseudo-Inverso

### Introdução
Este capítulo aprofunda o conceito de **pseudo-inverso** e suas aplicações em problemas de mínimos quadrados, com ênfase nas projeções ortogonais associadas às matrizes $AA^+$ e $A^+A$ [^753]. O objetivo é apresentar uma análise detalhada e rigorosa, adequada para um leitor com conhecimento avançado em álgebra linear, análise matricial e otimização. Em continuidade ao conceito de pseudo-inverso, que desempenha um papel crucial na solução de sistemas lineares pelo método dos mínimos quadrados [^753], exploraremos como as propriedades do pseudo-inverso se manifestam em projeções ortogonais e como essas projeções se relacionam com o kernel e a imagem da matriz original e sua transposta.

### Conceitos Fundamentais

**Projeções Ortogonais e Pseudo-Inversos**
Seja $A$ uma matriz $m \\times n$. De acordo com o contexto fornecido [^761], a matriz $AA^+$ representa a **projeção ortogonal** sobre a imagem de $A$ (Im($A$)), enquanto $A^+A$ representa a projeção ortogonal sobre o complemento ortogonal do kernel de $A$, denotado por Ker($A$)<sup>⊥</sup>. Este complemento ortogonal é igual à imagem da transposta de $A$, ou seja, Ker($A$)<sup>⊥</sup> = Im($A^T$).

*As projeções $AA^+$ e $A^+A$ são simétricas e idempotentes*, ou seja, $(AA^+)^T = AA^+$, $(A^+A)^T = A^+A$, $(AA^+)^2 = AA^+$ e $(A^+A)^2 = A^+A$ [^761]. A propriedade de simetria decorre do fato de que o pseudo-inverso, quando aplicado a matrizes normais, preserva certas relações de ortogonalidade [^763]. A idempotência, por sua vez, reflete a natureza de projeção: projetar um vetor já projetado sobre o mesmo subespaço não altera o vetor.

**Demonstração da Projeção Ortogonal**

A demonstração de que $AA^+$ é a projeção ortogonal sobre Im($A$) pode ser estruturada da seguinte forma:

1.  **Im($AA^+$) ⊆ Im($A$)**: Para qualquer vetor $y$, $AA^+y$ é uma combinação linear das colunas de $A$, e, portanto, pertence a Im($A$).
2.  **AA+y = y para y ∈ Im(A)**: Se $y ∈ Im(A)$, então $y = Ax$ para algum vetor $x$. Assim, $AA^+y = AA^+Ax = Ax = y$, pois $AA^+A = A$ [^761].
3.  **Simetria**: $(AA^+)^T = (A^+)^TA^T = AA^+$, pois $AA^+$ é uma matriz simétrica [^761].

Essas três propriedades garantem que $AA^+$ é a projeção ortogonal sobre Im($A$). Uma demonstração análoga pode ser construída para $A^+A$ e Ker($A$)<sup>⊥</sup>.

**Teorema da Projeção Ortogonal**

O teorema da projeção ortogonal é fundamental para entender a unicidade da solução de mínimos quadrados de norma mínima [^755]. Ele afirma que, para qualquer vetor $b \\in \\mathbb{R}^m$ e subespaço $U \\subseteq \\mathbb{R}^m$, existe um único vetor $p \\in U$ que minimiza a distância $||y - b||_2$ para todo $y \\in U$. Este vetor $p$ é a projeção ortogonal de $b$ sobre $U$.

**Prova**
Do contexto extraímos que, dado $b \\in \\mathbb{R}^m$, podemos interpretá-lo como um ponto no espaço Euclidiano. A imagem de $A$, Im($A$), representa um subespaço $U$ de $\\mathbb{R}^m$ passando pela origem. O objetivo é encontrar $x$ que minimize $||Ax - b||^2$. Isso é equivalente a encontrar $y \\in U$ que minimize $||y - b||^2$ [^755].

Seja $p$ a projeção ortogonal de $b$ sobre $U$. Então, para qualquer $y \\in U$, os vetores $p-y$ e $b-p$ são ortogonais. Assim,
$$||b-y||^2 = ||b-p + p-y||^2 = ||b-p||^2 + ||p-y||^2$$
[^756].

Como $||p-y||^2 \\geq 0$, $||b-y||^2$ é minimizado quando $y=p$. Portanto, $Ax = p$, onde $p$ é a projeção ortogonal de $b$ sobre $U$.

A unicidade de $x^+$ é garantida pela restrição de norma mínima e pela injetividade da restrição de $A$ a (Ker $A$)$^\\perp$ [^756].

**Método dos Mínimos Quadrados**

O método dos mínimos quadrados busca encontrar uma solução $x$ para o sistema linear $Ax = b$ que minimize a norma do resíduo $||Ax - b||_2$. Quando o sistema é *superdeterminado* (mais equações do que incógnitas), uma solução exata pode não existir [^753]. Neste caso, o método dos mínimos quadrados fornece a "melhor" solução aproximada.

**Observação:**
É importante notar que, em alguns contextos, o problema de mínimos quadrados pode ser ponderado (weighted least squares), onde pesos diferentes são atribuídos a diferentes equações no sistema linear. Além disso, o método pode ser implementado de forma recursiva (recursive least squares) para lidar com dados que chegam em fluxo [^759].

**Decomposição QR e Mínimos Quadrados**
O contexto menciona a **decomposição QR** como uma ferramenta eficiente para resolver problemas de mínimos quadrados [^759]. A decomposição QR expressa a matriz $A$ como o produto de uma matriz ortogonal $Q$ e uma matriz triangular superior $R$, ou seja, $A = QR$. A decomposição QR utilizando transformações de Householder é particularmente útil para resolver problemas de mínimos quadrados [^759].

Dado o sistema $Ax = b$, podemos reescrevê-lo como $QRx = b$. Multiplicando ambos os lados por $Q^T$, obtemos $Rx = Q^Tb$. Como $R$ é triangular superior, o sistema pode ser resolvido eficientemente por substituição retroativa.

### Conclusão

O pseudo-inverso fornece uma solução elegante e unificada para problemas de mínimos quadrados, mesmo quando a matriz $A$ não tem posto completo. As projeções ortogonais $AA^+$ e $A^+A$ revelam a estrutura geométrica subjacente ao problema, conectando a solução de mínimos quadrados com a imagem e o kernel da matriz $A$. A decomposição QR, por sua vez, oferece um método computacionalmente eficiente para encontrar a solução de mínimos quadrados.

### Referências

[^753]: Aplicações de SVD e Pseudo-Inversos.
[^755]: Solução de Mínimos Quadrados.
[^756]: Prova Geométrica da Solução de Mínimos Quadrados.
[^759]: Decomposição QR e Mínimos Quadrados.
[^761]: Propriedades da Pseudo-Inversa.
[^763]: Pseudo-Inversa e Matrizes Normais.

<!-- END -->