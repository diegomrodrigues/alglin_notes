## Solução de Mínimos Quadrados de Norma Mínima e a Pseudo-Inversa

### Introdução
Este capítulo explora a solução de mínimos quadrados de norma mínima para sistemas lineares, mesmo quando as colunas da matriz do sistema são linearmente dependentes. Este tópico é fundamental na teoria de mínimos quadrados e está intimamente ligado ao conceito de pseudo-inversa, que será detalhado. A solução de norma mínima possui interpretações geométricas e propriedades algébricas importantes, que serão exploradas em profundidade. Este capítulo complementa a discussão prévia sobre aplicações de SVD e pseudo-inversas, focando em um aspecto específico e crucial: a garantia de uma solução única e de norma mínima para problemas de mínimos quadrados [^753].

### Conceitos Fundamentais

**Definição do Problema de Mínimos Quadrados**

Dado um sistema linear *Ax = b*, onde *A* é uma matriz *m × n*, *x* é um vetor de *n* incógnitas, e *b* é um vetor em *R<sup>m</sup>*, o problema de mínimos quadrados busca encontrar um vetor *x* que minimize a norma Euclidiana do resíduo *||Ax - b||<sup>2</sup>* [^755]. Historicamente, este método foi utilizado por Gauss e Legendre para resolver problemas em astronomia e geodésia [^753].

**Existência e Unicidade da Solução de Norma Mínima**

Mesmo quando o sistema *Ax = b* é *overdetermined* (mais equações do que incógnitas, *m > n*) ou quando as colunas de *A* são linearmente dependentes, existe uma única solução *x<sup>+</sup>* de norma mínima que minimiza *||Ax - b||<sup>2</sup>* [^755].

**Teorema 23.1**: Para todo sistema linear *Ax = b*, onde *A* é uma matriz *m × n*, existe uma única solução de mínimos quadrados *x<sup>+</sup>* de norma mínima [^755].

*Prova*:
A prova geométrica oferece uma visão clara da existência e unicidade de *x<sup>+</sup>*. O vetor *b* pode ser interpretado como um ponto no espaço Euclidiano *R<sup>m</sup>*, e a imagem de *A* (o espaço coluna de *A*, denotado por *Im A*) como um subespaço *U* de *R<sup>m</sup>* [^755].  A distância mínima entre *b* e qualquer vetor em *U* é alcançada pela projeção ortogonal de *b* em *U*. Seja *p* a projeção ortogonal de *b* em *U*. Então, *||Ax - b||<sup>2</sup>* é minimizado se e somente se *Ax = p* [^755].

$$\
\inf_{x \in R^n} ||Ax - b||^2 = \inf_{y \in U} ||y - b||^2
$$

onde *U = Im A*.

Resta provar que existe um único *x<sup>+</sup>* de norma mínima tal que *Ax<sup>+</sup> = p*.  Sabemos que *R<sup>n</sup> = Ker A ⊕ (Ker A)<sup>⊥</sup>* [^756], onde *Ker A* é o núcleo de *A* e *(Ker A)<sup>⊥</sup>* é o complemento ortogonal do núcleo de *A*. Assim, qualquer vetor *x ∈ R<sup>n</sup>* pode ser escrito de forma única como *x = u + v*, onde *u ∈ Ker A* e *v ∈ (Ker A)<sup>⊥</sup>* [^756].  Portanto, *||x||<sup>2</sup> = ||u||<sup>2</sup> + ||v||<sup>2</sup>* [^756], já que *u* e *v* são ortogonais.

Se *u ∈ Ker A*, então *Au = 0*, e portanto *Ax = A(u + v) = Av* [^756].  Isso mostra que as soluções de *Ax = p* com norma mínima devem pertencer a *(Ker A)<sup>⊥</sup>* [^756]. A restrição de *A* a *(Ker A)<sup>⊥</sup>* é injetiva. Para verificar isso, suponha que *Av<sub>1</sub> = Av<sub>2</sub>*, onde *v<sub>1</sub>, v<sub>2</sub> ∈ (Ker A)<sup>⊥</sup>*. Então, *A(v<sub>2</sub> - v<sub>1</sub>) = 0*, o que implica que *v<sub>2</sub> - v<sub>1</sub> ∈ Ker A*. Como *v<sub>1</sub>, v<sub>2</sub> ∈ (Ker A)<sup>⊥</sup>*, também temos *v<sub>2</sub> - v<sub>1</sub> ∈ (Ker A)<sup>⊥</sup>*. Portanto, *v<sub>2</sub> - v<sub>1</sub> = 0*, e *v<sub>1</sub> = v<sub>2</sub>* [^756].

A injetividade de *A* restrita a *(Ker A)<sup>⊥</sup>* garante que existe um único *x<sup>+</sup> ∈ (Ker A)<sup>⊥</sup>* tal que *Ax<sup>+</sup> = p*.  Esse *x<sup>+</sup>* é a solução de norma mínima que minimiza *||Ax - b||<sup>2</sup>*. $\blacksquare$

**Interpretação Geométrica**

A solução *x<sup>+</sup>* pode ser interpretada geometricamente como o vetor em *(Ker A)<sup>⊥</sup>* que, quando transformado por *A*, resulta na projeção ortogonal de *b* no espaço coluna de *A*. Em outras palavras, *x<sup>+</sup>* "aponta" na direção que minimiza a distância entre *b* e o subespaço *Im A* [^753].

**As Equações Normais**

As soluções *x* que minimizam *||Ax - b||<sup>2</sup>* são dadas pelo sistema *n × n* chamado de equações normais:

$$\
A^T A x = A^T b
$$

Quando as colunas de *A* são linearmente independentes, a matriz *A<sup>T</sup>A* é invertível, e a solução é única:

$$\
x = (A^T A)^{-1} A^T b
$$

No entanto, quando as colunas de *A* são linearmente dependentes, *A<sup>T</sup>A* não é invertível, e a solução não é única. É aqui que a solução de norma mínima *x<sup>+</sup>* se torna crucial.

**Pseudo-Inversa de Moore-Penrose**

A solução de mínimos quadrados de norma mínima *x<sup>+</sup>* pode ser expressa em termos da pseudo-inversa de *A*, denotada por *A<sup>+</sup>* [^757]. Se *A = VDU<sup>T</sup>* é a decomposição em valores singulares (SVD) de *A*, onde *V* e *U* são matrizes ortogonais e *D* é uma matriz diagonal retangular com os valores singulares de *A*, então a pseudo-inversa *A<sup>+</sup>* é definida como:

$$\
A^+ = U D^+ V^T
$$

onde *D<sup>+</sup>* é obtida invertendo os elementos diagonais não nulos de *D* e transpondo a matriz resultante [^757].

Com a pseudo-inversa definida, a solução de mínimos quadrados de norma mínima é dada por:

**Teorema 23.2**: A solução de mínimos quadrados de norma mínima do sistema *Ax = b* é dada por *x<sup>+</sup> = A<sup>+</sup>b = UD<sup>+</sup>V<sup>T</sup>b* [^758].

*Prova*:
Primeiro, assuma que *A* é uma matriz diagonal retangular *D*. Então, *x* minimiza *||Dx - b||<sup>2</sup>* se e somente se *Dx* é a projeção de *b* no subespaço imagem de *D*. Nesse caso, é óbvio que *x<sup>+</sup> = D<sup>+</sup>b*.

Agora, considere o caso geral *A = VDU<sup>T</sup>*, onde *U* e *V* são ortogonais. Então,

$$\
||Ax - b||^2 = ||VDU^T x - b||^2 = ||DU^T x - V^T b||^2
$$

Como *V* é uma isometria (preserva normas), definindo *y = U<sup>T</sup>x*, temos *||x||<sup>2</sup> = ||y||<sup>2</sup>*. Portanto, *||Ax - b||<sup>2</sup>* é minimizado se e somente se *||Dy - V<sup>T</sup>b||<sup>2</sup>* é minimizado, e já mostramos que a solução para isso é *y<sup>+</sup> = D<sup>+</sup>V<sup>T</sup>b*.

Como *y = U<sup>T</sup>x*, temos *x = Uy*. Portanto, a solução de norma mínima é obtida por:

$$\
x^+ = U y^+ = U D^+ V^T b = A^+ b
$$

$\blacksquare$

**Propriedades da Pseudo-Inversa**

A pseudo-inversa *A<sup>+</sup>* possui propriedades importantes:

*   *AA<sup>+</sup>A = A*
*   *A<sup>+</sup>AA<sup>+</sup> = A<sup>+</sup>*
*   *(AA<sup>+</sup>)<sup>T</sup> = AA<sup>+</sup>* (AA<sup>+</sup> é simétrica)
*   *(A<sup>+</sup>A)<sup>T</sup> = A<sup>+</sup>A* (A<sup>+</sup>A é simétrica) [^765]

A matriz *AA<sup>+</sup>* representa a projeção ortogonal no espaço imagem de *A*, enquanto *A<sup>+</sup>A* representa a projeção ortogonal no complemento ortogonal do núcleo de *A* [^761].

### Conclusão

A solução de mínimos quadrados de norma mínima, obtida através da pseudo-inversa, fornece uma abordagem robusta e bem definida para resolver sistemas lineares, mesmo na presença de dependência linear e sistemas *overdetermined*. A interpretação geométrica e as propriedades algébricas da pseudo-inversa oferecem *insights* valiosos sobre a estrutura e as soluções desses sistemas. A discussão sobre a pseudo-inversa complementa a introdução inicial, fornecendo uma ferramenta prática e teórica para lidar com problemas de mínimos quadrados em uma variedade de aplicações [^753].

### Referências
[^753]: Legendre, 1805, *Nouvelles Méthodes pour la détermination des Orbites des Comètes*
[^755]: Capítulo 23, página 755
[^756]: Capítulo 23, página 756
[^757]: Capítulo 23, página 757
[^758]: Capítulo 23, página 758
[^761]: Capítulo 23, página 761
[^765]: Capítulo 23, página 765
<!-- END -->