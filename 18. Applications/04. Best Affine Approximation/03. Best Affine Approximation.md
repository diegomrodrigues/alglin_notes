## Melhor Aproximação Afim via SVD

### Introdução
Este capítulo explora a aplicação da Singular Value Decomposition (SVD) para encontrar a melhor aproximação afim de um conjunto de dados. Este problema está intimamente relacionado à Análise de Componentes Principais (PCA) e se baseia nos conceitos de mínimos quadrados [^753]. Dado um conjunto de dados, o objetivo é encontrar um subespaço afim de uma determinada dimensão que minimize a soma dos quadrados das distâncias dos pontos de dados a esse subespaço.

### Conceitos Fundamentais
O problema da melhor aproximação afim pode ser formulado da seguinte forma: Dado um conjunto de *n* pontos $X_1, ..., X_n \\in \\mathbb{R}^d$, encontrar um subespaço afim *A* de dimensão *p*, onde $1 \\leq p \\leq d-1$, que melhor se ajusta aos dados no sentido de mínimos quadrados [^778].

Para o caso especial onde $p = d - 1$, *A* é um hiperplano afim em $\\mathbb{R}^d$, definido por uma equação da forma [^778]:
$$a_1x_1 + ... + a_dx_d + c = 0$$
onde $x_i$ são as coordenadas e $a_i$ são os coeficientes, e $c$ é uma constante.

A condição de melhor aproximação implica que a soma dos quadrados das distâncias dos pontos $X_i$ ao hiperplano deve ser minimizada. Isso leva a um sistema linear homogêneo que pode ser resolvido no sentido de mínimos quadrados [^778]:
$$\
\begin{bmatrix}
x_{11} & ... & x_{1d} & 1 \\\\
\vdots & \vdots & \vdots & \vdots \\\\
x_{n1} & ... & x_{nd} & 1
\end{bmatrix}
\begin{bmatrix}
a_1 \\\\
\vdots \\\\
a_d \\\\
c
\end{bmatrix}
=
\begin{bmatrix}
0 \\\\
\vdots \\\\
0
\end{bmatrix}
$$
sujeito à restrição de que o vetor $a = (a_1, ..., a_d)$ seja um vetor unitário, ou seja, $a^Ta = 1$ [^778].

Observa-se que o hiperplano $A_1$ deve passar pelo centroide $\\mu$ dos pontos de dados $X_1, ..., X_n$, onde $\\mu = \\frac{1}{n}\\sum_{i=1}^n X_i$ [^778].  Portanto, podemos reescrever o sistema original em relação aos dados centrados $X_i - \\mu$, resultando no sistema [^778]:
$$(X - \\mu)a = 0$$
onde *X* é a matriz cujas linhas são os pontos de dados $X_i$.

Para resolver este sistema, podemos calcular a SVD de $X - \\mu = VDU^T$, onde *D* é uma matriz diagonal contendo os valores singulares $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_d$ [^779].  A solução para *a* é o último vetor coluna de *U*, correspondendo ao menor autovalor $\\sigma_d$ de $(X - \\mu)^T(X - \\mu)$ [^779].

Assim, se $U_{d-1}$ é o hiperplano linear definido por *a*, então $A_1 = \\mu + U_{d-1}$ é a melhor aproximação afim dos dados no sentido de mínimos quadrados [^779]. Este hiperplano minimiza a soma das distâncias quadradas de cada $X_i$ à sua projeção ortogonal em $A_1$ [^779].  Além disso, $U_{d-1}$ é gerado pelas primeiras *d-1* colunas de *U*, correspondendo às primeiras *d-1* direções principais de $X - \\mu$ [^779].

Este resultado pode ser generalizado para encontrar o melhor subespaço afim de dimensão $(d-k)$, denotado por $A_k$, onde $1 \\leq k \\leq d-1$. Tal subespaço é definido por *k* hiperplanos independentes $H_i$, cada um dado por uma equação da forma [^779]:
$$a_{i1}x_1 + ... + a_{id}x_d + c_i = 0$$

Os vetores normais $a_1, ..., a_k$ formam um sistema ortonormal.  Encontrar o melhor subespaço $A_k$ equivale a resolver o sistema linear homogêneo [^779]:
$$\
\begin{bmatrix}
X - \\mu & 0 & ... & 0 \\\\
0 & X - \\mu & ... & 0 \\\\
\vdots & \vdots & \vdots & \vdots \\\\
0 & 0 & ... & X - \\mu
\end{bmatrix}
\begin{bmatrix}
a_1 \\\\
a_2 \\\\
\vdots \\\\
a_k
\end{bmatrix}
=
\begin{bmatrix}
0 \\\\
0 \\\\
\vdots \\\\
0
\end{bmatrix}
$$
onde $a_i$ são vetores coluna.

A solução de mínimos quadrados para este sistema é dada pelas últimas *k* colunas de *U*, assumindo que os valores singulares de $X - \\mu$ estão dispostos em ordem decrescente [^780].  O subespaço $(d-k)$-dimensional $U_{d-k}$ é simplesmente o complemento ortogonal de $(a_1, ..., a_k)$, que é o subespaço gerado pelas primeiras $d-k$ colunas de *U* [^780].

**Teorema 23.12** Seja *X* uma matriz *n x d* de pontos de dados $X_1,...,X_n$, e seja $\\mu$ o centroide dos $X_i$'s. Se $X - \\mu = VDU^T$ é uma decomposição SVD de $X - \\mu$ e se a diagonal principal de *D* consiste nos valores singulares $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_d$, então a melhor aproximação afim $(d-k)$ dimensional $A_k$ de $X_1,...,X_n$ no sentido dos mínimos quadrados é dada por [^780]:

$$A_k = \\mu + U_{d-k}$$
onde $U_{d-k}$ é o subespaço linear gerado pelas primeiras $d-k$ colunas de *U*, as primeiras $d-k$ direções principais de $X - \\mu$ ($1 \\leq k \\leq d-1$) [^780]. $\\blacksquare$

### Conclusão
Em resumo, a SVD fornece uma ferramenta poderosa para encontrar a melhor aproximação afim de um conjunto de dados. Ao centrar os dados e calcular a SVD, podemos identificar as direções principais e construir o subespaço afim que minimiza a soma dos quadrados das distâncias dos pontos de dados a esse subespaço. As primeiras *d-k* direções principais de $X-\\mu$ geram o subespaço linear $U_{d-k}$. O subespaço afim $A_k = \\mu + U_{d-k}$ é, portanto, a melhor aproximação $(d-k)$-dimensional de $X_1, ..., X_n$ no sentido de mínimos quadrados.

### Referências
[^753]: Chapter 23. Applications of SVD and Pseudo-Inverses.
[^778]: 23.5 Best Affine Approximation
[^779]: 23.5 Best Affine Approximation
[^780]: 23.5 Best Affine Approximation
<!-- END -->