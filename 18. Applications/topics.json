{
  "topics": [
    {
      "topic": "Least Squares Problems and the Pseudo-Inverse",
      "sub_topics": [
        "The least squares method addresses overdetermined systems of linear equations, represented as Ax = b, where A is an m \u00d7 n matrix with m > n, indicating more equations than unknowns. The goal is to find solutions x that minimize ||Ax - b||\u00b2, the square of the Euclidean norm. Solutions to the least squares problem Ax = b are given by the normal equations ATAx = ATb, derived by minimizing the Euclidean norm ||Ax - b||\u00b2, where ||u||\u00b2 = u\u2081\u00b2 + ... + un\u00b2 for a vector u = (u\u2081, ..., un). When the columns of A are linearly independent, the matrix ATA is invertible, leading to a unique least squares solution x = (ATA)\u207b\u00b9ATb. The matrix ATA is symmetric, which simplifies computations.",
        "For any real m \u00d7 n matrix A, there exists a unique vector x\u207a of minimum norm that minimizes ||Ax - b||\u00b2, even when the columns of A are linearly dependent. This solution can be interpreted geometrically as the orthogonal projection of b onto the column space of A. This vector x\u207a is referred to as the least squares solution of smallest norm and satisfies Ax\u207a = p, where p is the orthogonal projection of b onto the subspace U = Im A. The vector x\u207a belongs to (Ker A)\u22a5, the orthogonal complement of the kernel of A, and minimizes ||Ax - b||\u00b2.",
        "The minimum norm least squares solution x\u207a can be expressed using the pseudo-inverse A\u207a of A, derived from the singular value decomposition (SVD) of A. If A = VDU\u1d40 is the SVD of A, then A\u207a = UD\u207aV\u1d40, where D\u207a is obtained by inverting the nonzero diagonal entries of D and transposing the matrix. The least squares solution of smallest norm for the linear system Ax = b is given by x\u207a = A\u207ab = UD\u207aV\u1d40b. The pseudo-inverse A\u207a provides the optimal solution to the least squares problem and depends only on A.",
        "The pseudo-inverse A\u207a can be computed without explicitly determining the SVD factorization. If A has full rank, A\u207a = (ATA)\u207b\u00b9AT when m \u2265 n, and A\u207a = AT(AAT)\u207b\u00b9 when n \u2265 m. A\u207aA = I in the first case, and AA\u207a = I in the second case, indicating that A\u207a is a left or right inverse of A, respectively.",
        "The matrix AA\u207a is the orthogonal projection onto the range of A, while A\u207aA is the orthogonal projection onto Ker(A)\u22a5 = Im(AT), where Ker(A) is the kernel of A and Im(AT) is the range of AT. These projections are symmetric and satisfy (AA\u207a)\u00b2 = AA\u207a and (A\u207aA)\u00b2 = A\u207aA. The method of least squares is a fundamental tool in mathematical sciences, with extensions such as weighted least squares and recursive least squares. The QR-decomposition, using Householder transformations, can also be applied to solve least squares problems efficiently."
      ]
    },
    {
      "topic": "Properties of the Pseudo-Inverse",
      "sub_topics": [
        "Given any m \u00d7 n matrix A (real or complex), the pseudo-inverse A\u207a of A is the unique n \u00d7 m matrix satisfying the four Penrose conditions: AA\u207aA = A, A\u207aAA\u207a = A\u207a, (AA\u207a)* = AA\u207a, and (A\u207aA)* = A\u207aA.",
        "When A has full rank, the pseudo-inverse A\u207a can be expressed as A\u207a = (ATA)\u207b\u00b9AT when m \u2265 n, and as A\u207a = AT(AAT)\u207b\u00b9 when n \u2265 m. In the first case (m \u2265 n), A\u207aA = I, so A\u207a is a left inverse of A; in the second case (n \u2265 m), we have AA\u207a = I, so A\u207a is a right inverse of A.",
        "For any m \u00d7 n matrix A, both AA\u207a and A\u207aA are symmetric matrices. If A = VSigmau\u1d40 is the SVD of A, then AA\u207a = V[[I\u1d63, 0], [0, 0]]V\u1d40 and A\u207aA = U[[I\u1d63, 0], [0, 0]]U\u1d40, where r is the rank of A.",
        "The matrix AA\u207a is the orthogonal projection onto the range of A, and A\u207aA is the orthogonal projection onto Ker(A)\u22a5 = Im(AT), the range of AT. The matrix AA\u207a consists of all vectors y \u2208 \u211d\u1d50 such that V\u1d40y = [[z], [0]], with z \u2208 \u211d\u02b3. The matrix A\u207aA consists of all vectors y \u2208 \u211d\u207f such that U\u1d40y = [[z], [0]], with z \u2208 \u211d\u02b3.",
        "If A is a normal matrix (i.e., AAT = ATA), there is a close relationship between the SVD of A and its block diagonalization. The pseudo-inverse of a normal matrix can be obtained directly from a block diagonalization of A. For any (real) normal matrix A and any block diagonalization A = UAU\u1d40 of A, the pseudo-inverse of A is given by A\u207a = UA\u207aU\u1d40, where A\u207a is the pseudo-inverse of A. Furthermore, if A = [[Ar, 0], [0, 0]], then A\u207a = [[Ar\u207b\u00b9, 0], [0, 0]]."
      ]
    },
    {
      "topic": "Data Compression and SVD",
      "sub_topics": [
        "Given an m \u00d7 n matrix A of rank r, the best approximation of A by a matrix B of rank k \u2264 r, in terms of minimizing ||A - B||\u2082, is given by Ak = \u03a3\u1d62\u208c\u2081\u1d4f \u03c3\u1d62u\u1d62v\u1d62\u1d40, where u\u1d62 and v\u1d62 are the columns of U and V, respectively, and \u03c3\u1d62 are the singular values of A. The error of this approximation is ||A - Ak||\u2082 = \u03c3\u2096\u208a\u2081. The matrix Ak can be stored using (m + n)k entries, which is a substantial gain when k < m.",
        "Data compression is one of the many applications of SVD. In order to make precise the notion of closeness of matrices, we use the notion of matrix norm.",
        "Principal Component Analysis (PCA) is used to identify patterns in data and understand the variance-covariance structure of the data. It involves finding uncorrelated projections Y of the data points X\u2081, ..., X\u2099 onto some directions v such that var(Y) is maximal. Given an n \u00d7 d matrix X of data points X\u2081, ..., X\u2099, a first principal component of X is a centered point Y\u2081 = (X - \u03bc)v\u2081, where \u03bc is the centroid of the X\u1d62's, and v\u2081 is a unit vector such that var(Y\u2081) is maximized. More generally, a (k+1)th principal component is a centered point Y\u2096\u208a\u2081 = (X - \u03bc)v\u2096\u208a\u2081, where v\u2096\u208a\u2081 is a unit vector such that var(Y\u2096\u208a\u2081) is maximized, subject to cov(Y\u2095, Y\u2096\u208a\u2081) = 0 for all h with 1 \u2264 h \u2264 k.",
        "The principal directions u\u2081, ..., ud of X - \u03bc (and X) are the columns of U in the SVD decomposition X - \u03bc = VDU\u1d40. These directions are mutually orthogonal, and the singular values \u03c3\u2081, ..., \u03c3d are the eigenvalues of the symmetric positive semidefinite matrix (X \u2013 \u03bc)\u1d40(X \u2013 \u03bc). The SVD of a matrix X yields its principal components; if X - \u03bc = VDU\u1d40 is an SVD decomposition, where \u03bc is the centroid of the data points, then the columns of VD, denoted as Yk = (X - \u03bc)uk, represent the principal components of X. The variance of the kth principal component Yk is given by \u03c3k\u00b2 / (n - 1), where \u03c3k is the kth singular value on the main diagonal of D, and the covariance between different principal components Yh and Yk is zero (cov(Yh, Yk) = 0) for h \u2260 k."
      ]
    },
    {
      "topic": "Best Affine Approximation",
      "sub_topics": [
        "Finding the best affine approximation involves approximating a data set of n points X\u2081, ..., X\u2099 in \u211d\u1d48 by a p-dimensional affine subspace A of \u211d\u1d48, where 1 \u2264 p \u2264 d - 1. A problem very close to PCA (and based on least squares) is to best approximate a data set of n points (X\u2081, ..., X\u2099), with Xi \u2208 \u211d\u1d48, by a p-dimensional affine subspace A of (\u211d\u1d48), with (1 \u2264 p \u2264 d-1) (the terminology rank d p is also used).",
        "For the case p = d - 1, the affine subspace is a hyperplane A\u2081 given by an equation a\u2081x\u2081 + \u00b7\u00b7\u00b7 + a\u2093d + c = 0, where a = (a\u2081, ..., a\u2093) is a unit vector. The best approximation is found by solving a homogeneous linear system in the least squares sense, subject to the condition that a is a unit vector. This leads to the conclusion that the hyperplane A\u2081 must pass through the centroid \u03bc of the data points X\u2081, ..., X\u2099.",
        "The best (d - k)-dimensional affine subspace Ak approximating X\u2081, ..., X\u2099 in the least squares sense is given by Ak = \u03bc + Ud-k, where Ud-k is the linear subspace spanned by the first d \u2013 k principal directions of X - \u03bc, i.e., the first d \u2013 k columns of U in the SVD decomposition X - \u03bc = VDU\u1d40. A best (d-k)-dimensional affine subspace (A_k) approximating (X_1,..., X_n) in the least squares sense is given by (A_k = \u03bc + U_{d-k}), where (U_{d-k}) is the linear subspace spanned by the first d \u2013 k columns of U, the first d \u2013 k principal directions of (X - \u03bc) ((1 \u2264 k \u2264 d- 1)).",
        "In general, since an SVD of X is not unique, the principal directions u\u2081, ..., ud are not unique. This can happen when a data set has some rotational symmetries, and in such a case, PCA is not a very good method for analyzing the data set."
      ]
    },
    {
      "topic": "Principal Components Analysis (PCA)",
      "sub_topics": [
        "Principal Component Analysis (PCA) identifies patterns in data and understands the variance-covariance structure. PCA is useful for data reduction and interpretation. The main idea of PCA is to find uncorrelated projections Y of the data points (X\u2081, ..., X\u2099) onto some directions v such that var(Y) is maximal.",
        "Given an (n \u00d7 d) matrix X of data points (X\u2081, ..., X\u2099), a first principal component of X is a centered point (Y\u2081 = (X - \u03bc)v\u2081), the projection of (X\u2081, ..., X\u2099) onto a direction (v\u2081) such that var((Y\u2081)) is maximized, where (v\u2081) is a unit vector (recall that (Y\u2081 = (X - \u03bc)v\u2081) is a linear combination of the (C\u2c7c)'s, the columns of (X - \u03bc)).",
        "More generally, if (Y\u2081, ..., Y\u2096) are k principal components of X along some unit vectors (u\u2081, ..., u\u2096), where (1 \u2264 k < d), a ((k+1))th principal component of X (((k+1))th PC) is a centered point (Y\u2096\u208a\u2081 = (X - \u03bc)v\u2096\u208a\u2081), the projection of (X\u2081, ..., X\u2099) onto some direction (v\u2096\u208a\u2081) such that var((Y\u2096\u208a\u2081)) is maximized, subject to cov((Y\u2095, Y\u2096\u208a\u2081)) = 0 for all h with (1 \u2264 h \u2264 k), and where (v\u2096\u208a\u2081) is a unit vector (recall that (Y\u2095 = (X - \u03bc)v\u2095) is a linear combination of the (C\u2c7c)'s).",
        "Let X be an (n \u00d7 d) matrix of data points (X\u2081, ..., X\u2099), and let (\u03bc) be the centroid of the (X\u1d62)'s. If (X - \u03bc = VDU\u1d40) is an SVD decomposition of (X - \u03bc) and if the main diagonal of D consists of the singular values (\u03bb\u2081 \u2265 \u03bb\u2082 \u2265 ... \u2265 \u03bbd), then the centered points (Y\u2081, ..., Yd), where (Y\u2096 = (X - \u03bc)u\u2096 = kth column of VD) and (u\u2096) is the kth column of U, are d principal components of X. Furthermore, (var(Y\u2096) = \u03c3\u2096\u00b2/(n-1)) and (cov(Y\u2095, Y\u2096) = 0), whenever (h \u2260 k) and (1 \u2264 k, h \u2264 d). The d columns (u\u2081, ..., ud) of U are usually called the principal directions of (X - \u03bc) (and X)."
      ]
    },
    {
      "topic": "Applications of SVD and Pseudo-Inverses",
      "sub_topics": [
        "The pseudo-inverse plays a crucial role in solving linear systems by the method of least squares, especially in overdetermined systems where the matrix A is a rectangular m \u00d7 n matrix with more equations than unknowns (m > n). The least squares solution minimizes ||Ax - b||\u00b2, and when the columns of A are linearly independent, the unique solution is x = (ATA)\u207b\u00b9ATb. For any real m \u00d7 n matrix A, there exists a unique x\u207a of minimum norm that minimizes ||Ax - b||\u00b2, even when the columns of A are linearly dependent. This solution x\u207a can be interpreted geometrically as the orthogonal projection of b onto the column space of A. The minimum norm least squares solution x\u207a can be found using the pseudo-inverse A\u207a of A, which is obtained from the singular value decomposition (SVD) of A.",
        "Given a nonzero m \u00d7 n matrix A of rank r, its SVD is A = VDU\u1d40, where D is a matrix with \u039b (an r \u00d7 r diagonal matrix consisting of the nonzero singular values of A) and zeros elsewhere. The pseudo-inverse A\u207a is defined as A\u207a = UD\u207aV\u1d40, where D\u207a is obtained by inverting the nonzero diagonal entries of D and transposing. The pseudo-inverse satisfies properties such as AA\u207aA = A, A\u207aAA\u207a = A\u207a, (AA\u207a)\u1d40 = AA\u207a, and (A\u207aA)\u1d40 = A\u207aA.",
        "When A has full rank, the pseudo-inverse A\u207a can be expressed as (ATA)\u207b\u00b9AT when m \u2265 n (left inverse) and as A\u1d40(AAT)\u207b\u00b9 when n \u2265 m (right inverse). AA\u207a is the orthogonal projection onto the range of A, and A\u207aA is the orthogonal projection onto Ker(A)\u22a5, the range of A\u1d40. If A is a normal matrix (AAT = ATA), its pseudo-inverse can be obtained directly from a block diagonalization.",
        "Singular value decomposition (SVD) is useful in data compression. Given an m \u00d7 n matrix A of rank r, a best approximation B of rank k \u2264 r minimizes ||A - B||\u2082 (or ||A - B||F), where || ||\u2082 is the spectral norm and || ||F is the Frobenius norm. The Eckart-Young theorem states that the matrix Ak formed by summing the first k singular values of A provides this best approximation. A\u2096 can be stored using (m + n)k entries, offering a substantial gain when k is smaller than m and n.",
        "Principal Component Analysis (PCA) identifies patterns in data and understands its variance-covariance structure. Given a data set X = {X\u2081, ..., X\u2099}, where each X\u1d62 \u2208 \u211d\u1d48, PCA translates the origin to the centroid \u03bc of the X\u1d62's. The principal components are uncorrelated projections Y of the data points onto directions v such that var(Y) is maximized. The main theorem states that the SVD of X yields the principal components, where the kth principal component is given by Y\u2096 = (X - \u03bc)u\u2096 (kth column of VD), and u\u2096 is the kth column of U.",
        "QR decomposition using Householder transformations can solve least squares problems for an m \u00d7 n matrix A of rank n (m \u2265 n). The decomposition A = H\u2081...H\u1d63R, where H\u1d62 are Householder matrices and R is an upper triangular matrix, transforms the least squares problem into an equivalent system Rx = H\u1d63...H\u2081b. The least squares solution of smallest norm is x\u207a = R\u2081\u207b\u00b9c, where R\u2081 is an invertible n \u00d7 n matrix and c \u2208 \u211d\u207f."
      ]
    }
  ]
}