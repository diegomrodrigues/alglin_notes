## Extração de Componentes Principais Subsequentes

### Introdução
Este capítulo expande o conceito de **Principal Component Analysis (PCA)**, introduzido anteriormente [^768], detalhando a extração de componentes principais subsequentes. O PCA é uma técnica essencial para identificar padrões em dados, compreendendo a estrutura de variância-covariância dos dados [^768]. Veremos como, após a identificação dos primeiros *k* componentes principais, podemos encontrar componentes adicionais que capturem a variância restante nos dados, sujeitos a restrições de ortogonalidade. Este processo permite uma representação mais completa e eficiente dos dados em um espaço de dimensionalidade reduzida.

### Conceitos Fundamentais
Como vimos anteriormente, o PCA envolve a transformação de um conjunto de dados em um novo sistema de coordenadas onde a maior variância por qualquer projeção dos dados está no primeiro eixo (a primeira componente principal), a segunda maior variância no segundo eixo, e assim por diante [^768]. Formalmente, se $(Y_1, ..., Y_k)$ são *k* componentes principais de *X* ao longo de vetores unitários $(u_1, ..., u_k)$, onde $1 \leq k < d$ [^772], definimos o próximo componente principal da seguinte forma:

Um $((k+1))$ésimo componente principal de *X* (o $((k+1))$ésimo PC) é um ponto centrado $(Y_{k+1} = (X - \mu)v_{k+1})$, a projeção de $(X_1, ..., X_n)$ em alguma direção $(v_{k+1})$ tal que $var(Y_{k+1})$ é maximizada [^772], sujeita a $cov(Y_h, Y_{k+1}) = 0$ para todo *h* com $1 \leq h \leq k$, e onde $(v_{k+1})$ é um vetor unitário [^772]. Aqui, $(Y_h = (X - \mu)v_h)$ é uma combinação linear das colunas de *X* centrada em $\mu$ [^772].

A condição $cov(Y_h, Y_{k+1}) = 0$ impõe que o $((k+1))$ésimo componente principal seja *não correlacionado* com os *k* componentes principais anteriores [^774]. Geometricamente, isso significa que a direção $v_{k+1}$ é ortogonal às direções $v_1, ..., v_k$ [^775]. Essa ortogonalidade garante que cada componente principal subsequente capture uma parte da variância dos dados que ainda não foi explicada pelos componentes anteriores [^775].

Para encontrar $v_{k+1}$, resolvemos um problema de otimização com restrições. Queremos maximizar a variância de $Y_{k+1}$, que é dada por:
$$\
var(Y_{k+1}) = \frac{1}{n-1}v_{k+1}^T(X - \mu)^T(X - \mu)v_{k+1} = v_{k+1}^T\Sigma v_{k+1}
$$
onde $\Sigma$ é a matriz de covariância amostral de *X* [^769]. As restrições são:
1.  $v_{k+1}^Tv_{k+1} = 1$ (vetor unitário)
2.  $cov(Y_h, Y_{k+1}) = v_h^T\Sigma v_{k+1} = 0$, para $h = 1, ..., k$

Este problema pode ser resolvido usando multiplicadores de Lagrange. A solução $v_{k+1}$ é o autovetor de $\Sigma$ correspondente ao $((k+1))$ésimo maior autovalor, sujeito às restrições de ortogonalidade [^774].

**Lema 1:** *A variância de $Y_{k+1}$ é igual ao $((k+1))$ésimo maior autovalor da matriz de covariância $\Sigma$.*

*Prova:* Seja $\lambda_{k+1}$ o $((k+1))$ésimo maior autovalor de $\Sigma$ e $v_{k+1}$ o autovetor correspondente. Então, $\Sigma v_{k+1} = \lambda_{k+1} v_{k+1}$. Substituindo na expressão da variância:
$$\
var(Y_{k+1}) = v_{k+1}^T\Sigma v_{k+1} = v_{k+1}^T(\lambda_{k+1} v_{k+1}) = \lambda_{k+1} (v_{k+1}^Tv_{k+1}) = \lambda_{k+1}
$$
já que $v_{k+1}$ é um vetor unitário. $\blacksquare$

**Corolário 1:** *Os autovalores da matriz de covariância $\Sigma$ representam a variância dos dados ao longo das direções dos autovetores correspondentes (componentes principais).*

### Conclusão
A extração de componentes principais subsequentes é um processo iterativo que permite decompor a variância total dos dados em componentes ortogonais, ordenados por sua importância [^774]. Cada componente principal subsequente maximiza a variância não explicada pelos componentes anteriores, sujeito à restrição de ortogonalidade [^775]. Este processo é fundamental para a redução de dimensionalidade e para a identificação de padrões significativos em conjuntos de dados complexos [^768].

### Referências
[^768]: Aplicações de SVD e Pseudo-Inversas: Análise de Componentes Principais (PCA).
[^769]: Cálculo da matriz de covariância amostral de X.
[^772]: Definição formal de componentes principais subsequentes.
[^774]: Relação entre SVD e PCA.
[^775]: Ortogonalidade das direções principais.
<!-- END -->