## Primeiro Componente Principal: Maximização da Variância Projetada

### Introdução
Este capítulo aprofunda o conceito de **Principal Component Analysis (PCA)**, com foco na determinação do primeiro componente principal de um conjunto de dados. PCA é uma técnica fundamental para redução de dimensionalidade e identificação de padrões em dados [^753]. Conforme mencionado anteriormente [^753], PCA busca identificar padrões nos dados e entender a estrutura de variância-covariância. Vamos explorar como o primeiro componente principal é derivado, utilizando a matriz de dados centrada e a maximização da variância da projeção.

### Conceitos Fundamentais
Considere uma matriz $(n \times d)$, $X$, representando $n$ pontos de dados $(X_1, ..., X_n)$ em um espaço $d$-dimensional [^772]. O objetivo é encontrar uma direção $v_1$ (um vetor unitário) tal que a projeção dos pontos de dados centrados $(X_1, ..., X_n)$ nessa direção maximize a variância [^772, 774].

1.  **Centrando os Dados:** Para começar, é necessário centrar os dados subtraindo a média $\mu$ de cada ponto de dados, onde $\mu = \frac{1}{n} \sum_{i=1}^{n} X_i$ [^769, 772]. Isso resulta em uma nova matriz, $X - \mu$, onde cada linha representa um ponto de dados centrado [^769].

2.  **Projeção:** A projeção de um ponto de dados centrado $X_i - \mu$ na direção $v_1$ é dada por $(X_i - \mu)v_1$ [^770]. Assim, o primeiro componente principal $Y_1$ é um ponto centrado, a projeção de $(X_1, ..., X_n)$ em uma direção $v_1$ [^772].

3.  **Variância da Projeção:** O objetivo é maximizar a variância da projeção, dada por $var(Y_1)$ [^772]. A variância de $Y_1$ pode ser expressa como:

    $$var(Y_1) = \frac{1}{n-1} \sum_{i=1}^{n} ((X_i - \mu)v_1 - \overline{(X - \mu)v_1})^2$$

    Como os dados já estão centrados, $\overline{(X - \mu)v_1} = 0$. Portanto,

    $$var(Y_1) = \frac{1}{n-1} \sum_{i=1}^{n} ((X_i - \mu)v_1)^2 = \frac{1}{n-1} v_1^T \left( \sum_{i=1}^{n} (X_i - \mu)^T (X_i - \mu) \right) v_1$$

4.  **Matriz de Covariância:** A matriz de covariância $\Sigma$ de $X$ é definida como [^769]:

    $$\Sigma = \frac{1}{n-1} (X - \mu)^T (X - \mu) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \mu)^T (X_i - \mu)$$

    Assim, a variância da projeção pode ser reescrita como:

    $$var(Y_1) = v_1^T \Sigma v_1$$

5.  **Maximização:** O problema de encontrar o primeiro componente principal se resume a maximizar $v_1^T \Sigma v_1$ sujeito à restrição de que $v_1$ seja um vetor unitário, ou seja, $v_1^T v_1 = 1$ [^772].

6.  **Rayleigh Quotient:** A expressão $v_1^T \Sigma v_1$ é conhecida como o **quociente de Rayleigh** [^774]. O valor máximo do quociente de Rayleigh é o maior autovalor da matriz de covariância $\Sigma$, e o vetor $v_1$ correspondente é o autovetor associado a esse autovalor [^773, 774].

    **Proposição 23.10 [^772]:** *Se $A$ é uma matriz simétrica $d \times d$ com autovalores $\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_d$ e $(u_1, ..., u_d)$ é uma base ortonormal de autovetores de $A$, onde $u_i$ é um autovetor unitário associado a $\lambda_i$, então*\

    $$max_{x \neq 0} \frac{x^T A x}{x^T x} = \lambda_1$$

    *(com o máximo atingido para $x = u_1$)*

    Isso significa que a direção $v_1$ que maximiza a variância da projeção é o autovetor correspondente ao maior autovalor da matriz de covariância $\Sigma$ [^775].

### Conclusão
A determinação do primeiro componente principal envolve centrar os dados, calcular a matriz de covariância e encontrar o autovetor correspondente ao maior autovalor dessa matriz [^774]. Este autovetor define a direção na qual a variância dos dados projetados é maximizada, capturando a maior parte da variabilidade dos dados originais [^775]. Este processo é fundamental para a redução de dimensionalidade e a identificação de padrões significativos em conjuntos de dados complexos [^768].

### Referências
[^753]: Chapter 23. Applications of SVD and Pseudo-Inverses
[^768]: We usually form the n× d matrix X whose ith row is X₁, with 1 ≤ i ≤ n. Then the jth column is denoted by C'; (1 ≤ j ≤ d). It is sometimes called a feature vector, but this terminology is far from being universally accepted. In fact, many people in computer vision call the data points X₁ feature vectors! The purpose of principal components analysis, for short PCA, is to identify patterns in data and understand the variance-covariance structure of the data. This is useful for the following tasks: 1. Data reduction: Often much of the variability of the data can be accounted for by a smaller number of principal components. 2. Interpretation: PCA can show relationships that were not previously suspected.
[^769]: Given a vector (a sample of measurements) x = (x1,...,xn) ∈ R”, recall that the mean (or average) x of x is given by  i=1 Xi X = n We let x denote the centered data point X X = (x1 - X, . . ., Xn - X). In order to measure the spread of the xi's around the mean, we define the sample variance (for short, variance) var(x) (or s²) of the sample x by var(x) = Σ=1(xi - x)2 n-1
[^770]: We can think of the vector Cj as representing the features of X in the direction ej (the jth canonical basis vector in Rd, namely ej = (0, . . ., 1, . . . 0), with a 1 in the jth position). If v ∈ Rd is a unit vector, we wish to consider the projection of the data points X1, ..., Χη onto the line spanned by v. Recall from Euclidean geometry that if x ∈ Rd is any vector and v ∈ Rd is a unit vector, the projection of x onto the line spanned by v is (x,υ)υ. Thus, with respect to the basis v, the projection of x has coordinate (x, v). If x is represented by a row vector and v by a column vector, then (x,υ) = χυ.
[^772]: Definition 23.2. Given an n × d matrix X of data points X1, . . ., Xn, if u is the centroid of the Xi's, then a first principal component of X (first PC) is a centered point Y₁ = (X − μ)v1, the projection of X1, ..., Xn onto a direction v₁ such that var(Y₁) is maximized, where v₁ is a unit vector (recall that Y₁ = (X − μ)v₁ is a linear combination of the Cj's, the columns of Χ – μ).
[^773]: Since A is a symmetric matrix, its eigenvalues are real and it can be diagonalized with respect to an orthonormal basis of eigenvectors, so let (u1, ..., ua) be such a basis. If we write d x= Σ XiUi, i=1 a simple computation shows that d X Ax = Σλία. i=1
[^774]: The quantity X Ax T xx is known as the Rayleigh ratio or Rayleigh-Ritz ratio (see Section 17.6 ) and Proposition 23.10 is often known as part of the Rayleigh-Ritz theorem.
[^775]: Obviously, U(n-1)D2UT is a symmetric matrix whose eigenvalues are σ/ (n-1) >...>, and the columns of U form an orthonormal basis of unit eigenvectors.

<!-- END -->