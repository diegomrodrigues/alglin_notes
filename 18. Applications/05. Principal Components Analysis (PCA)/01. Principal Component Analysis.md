## Análise de Componentes Principais (PCA)

### Introdução
Este capítulo aprofunda o estudo da Análise de Componentes Principais (PCA), uma técnica fundamental para identificar padrões em dados e compreender a estrutura de variância-covariância. PCA é amplamente utilizada para redução de dimensionalidade e interpretação de dados [^1]. O objetivo principal é encontrar projeções não correlacionadas dos dados que maximizem a variância, permitindo uma representação mais eficiente e informativa dos dados [^1]. Este capítulo se baseia nos conceitos de decomposição em valores singulares (SVD) e pseudo-inversas, explorados anteriormente, e demonstra como o SVD é uma ferramenta essencial para a implementação do PCA [^1, 23.1].

### Conceitos Fundamentais

A Análise de Componentes Principais (PCA) é uma técnica poderosa para reduzir a dimensionalidade de um conjunto de dados, mantendo a maior parte da variabilidade original [^1]. A ideia central é transformar um conjunto de dados com variáveis correlacionadas em um novo conjunto de variáveis não correlacionadas, chamadas **componentes principais**. Esses componentes são ordenados de forma que os primeiros retenham a maior parte da variação presente em todos os dados originais [^1].

**Objetivo do PCA**

O PCA tem dois objetivos principais [^23.4]:

1.  **Redução de Dados:** Reduzir o número de variáveis sem perder informações importantes.
2.  **Interpretação:** Revelar relações entre variáveis que não eram evidentes inicialmente.

**Definições e Notações**

*   Seja $X$ uma matriz $n \\times d$, onde $n$ é o número de pontos de dados e $d$ é o número de dimensões (features) [^23.4]. Cada linha $X_i$ representa um ponto de dados em $R^d$.
*   O *vetor de média* (ou *centróide*) $\\mu$ é definido como [^23.4]:

$$\
\\mu = \\frac{1}{n} \\sum_{i=1}^{n} X_i
$$

*   A *matriz de covariância* $\\Sigma$ é uma matriz $d \\times d$ simétrica que descreve a variabilidade conjunta das variáveis [^1, 23.4]:

$$\
\\Sigma = \\frac{1}{n-1} (X - \\mu)^T (X - \\mu)
$$

**Passos do PCA**

1.  **Centrar os Dados:** Subtrair a média de cada variável para centralizar os dados em torno da origem [^23.4]:

$$\
X_{centered} = X - \\mu
$$

2.  **Calcular a Matriz de Covariância:** Calcular a matriz de covariância $\\Sigma$ dos dados centrados [^1, 23.4].

3.  **Decomposição em Autovalores e Autovetores:** Calcular os autovalores ($\\lambda_i$) e autovetores ($v_i$) da matriz de covariância $\\Sigma$ [^23.4]. Os autovetores representam as direções dos componentes principais, e os autovalores representam a variância explicada por cada componente.

4.  **Ordenar Autovalores e Autovetores:** Ordenar os autovalores em ordem decrescente e reorganizar os autovetores de acordo [^23.4].

5.  **Selecionar Componentes Principais:** Escolher os $k$ autovetores correspondentes aos $k$ maiores autovalores. Esses autovetores formam a matriz de projeção $W$, onde $W$ é uma matriz $d \\times k$ [^23.4].

6.  **Projetar os Dados:** Projetar os dados originais no novo espaço de menor dimensão:

$$\
Y = X_{centered} \\cdot W
$$

onde $Y$ é a matriz de dados projetados, com dimensão $n \\times k$.

**Maximização da Variância**

O PCA busca encontrar as projeções $Y$ que maximizem a variância [^1]. A variância de $Y$ é dada por [^23.4]:

$$\
var(Y) = v^T \\left[ \\frac{1}{n-1} (X - \\mu)^T (X - \\mu) \\right] v = v^T \\Sigma v
$$

onde $v$ é um vetor unitário que representa a direção da projeção. Maximizar $var(Y)$ é equivalente a encontrar o autovetor correspondente ao maior autovalor da matriz de covariância $\\Sigma$ [^23.4].

**Uso do SVD no PCA**

O SVD é uma ferramenta fundamental para calcular os autovalores e autovetores da matriz de covariância [^23.1, 23.4]. Dado que $X - \\mu = VDU^T$ é a decomposição SVD da matriz de dados centrados, onde $V$ é uma matriz ortogonal, $D$ é uma matriz diagonal com os valores singulares, e $U$ é outra matriz ortogonal, temos:

$$\
\\Sigma = \\frac{1}{n-1} (X - \\mu)^T (X - \\mu) = \\frac{1}{n-1} U D^2 U^T
$$

Os autovetores de $\\Sigma$ são as colunas de $U$, e os autovalores são dados por $\\frac{\\sigma_i^2}{n-1}$, onde $\\sigma_i$ são os valores singulares na diagonal de $D$ [^23.4].

**Teorema Fundamental do PCA via SVD**

Seja $X$ uma matriz $n \\times d$ de pontos de dados, e seja $\\mu$ o centróide dos $X_i$. Se $X - \\mu = VDU^T$ é uma decomposição SVD de $X - \\mu$, e se a diagonal principal de $D$ consiste dos valores singulares $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_d$, então os componentes principais centrados $Y_1, \\dots, Y_d$ são dados por [^23.4, 23.11]:

$$\
Y_k = (X - \\mu)u_k
$$

onde $u_k$ é a $k$-ésima coluna de $U$. Além disso [^23.11]:

$$\
var(Y_k) = \\frac{\\sigma_k^2}{n-1}
$$

e $cov(Y_h, Y_k) = 0$ sempre que $h \\neq k$ [^23.11].

**Exemplo**

Considere o exemplo dos matemáticos barbudos apresentado no texto [^23.4, 23.10]. Após centrar os dados, podemos aplicar o SVD para obter os autovetores (direções principais) e os autovalores (variâncias explicadas). As projeções dos dados centrados nessas direções fornecem os componentes principais.

### Conclusão
A Análise de Componentes Principais (PCA) é uma técnica poderosa para redução de dimensionalidade e interpretação de dados. Ao encontrar projeções não correlacionadas que maximizam a variância, o PCA permite uma representação mais eficiente e informativa dos dados. O SVD é uma ferramenta essencial para a implementação do PCA, fornecendo um meio eficiente de calcular os autovalores e autovetores da matriz de covariância. A compreensão dos princípios matemáticos subjacentes ao PCA, juntamente com o uso do SVD, permite uma aplicação eficaz desta técnica em uma ampla gama de problemas.

### Referências
[^1]: Chapter 23
[^23.1]: 23.1 Least Squares Problems and the Pseudo-Inverse
[^23.4]: 23.4 Principal Components Analysis (PCA)
[^23.10]: Example 23.10
[^23.11]: Theorem 23.11
<!-- END -->