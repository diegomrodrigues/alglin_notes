## Análise de Componentes Principais via Decomposição em Valores Singulares

### Introdução
Este capítulo detalha a aplicação da Decomposição em Valores Singulares (SVD) na Análise de Componentes Principais (PCA). PCA é uma técnica fundamental para redução de dimensionalidade e identificação de padrões em dados. Conforme mencionado no Capítulo 23 [^1], a PCA visa identificar padrões em dados e entender a estrutura de variância-covariância dos dados. Aqui, exploraremos como a SVD facilita a implementação e compreensão da PCA, fornecendo uma base matemática rigorosa para a técnica.

### Conceitos Fundamentais

Dado um conjunto de dados representado por uma matriz $X$ de dimensão $n \times d$, onde cada linha $X_i$ representa um ponto de dados em $d$ dimensões, o objetivo da PCA é encontrar um conjunto de *componentes principais* que capturem a maior parte da variância dos dados.

1.  **Centróide:** O primeiro passo na PCA é calcular o centróide $\mu$ dos pontos de dados $X_i$, definido como:

    $$\mu = \frac{1}{n} \sum_{i=1}^{n} X_i$$

    Este centróide representa o "centro de gravidade" dos dados [^769].

2.  **Matriz Centrada:** Subtraímos o centróide $\mu$ de cada ponto de dado $X_i$ para obter a matriz centrada $(X - \mu)$. Este passo é crucial porque a PCA é sensível à escala e à origem dos dados [^772].

3.  **Decomposição em Valores Singulares (SVD):** Aplicamos a SVD à matriz centrada $(X - \mu)$, obtendo:

    $$X - \mu = VDU^T$$

    onde:

    *   $U$ é uma matriz ortogonal $d \times d$ cujas colunas $u_k$ são os *vetores singulares à esquerda*.
    *   $V$ é uma matriz cujas colunas são os *vetores singulares à direita*.
    *   $D$ é uma matriz diagonal $d \times d$ com valores singulares $\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_d$ na diagonal principal [^1].

4.  **Componentes Principais:** Os $d$ *componentes principais* $Y_k$ de $X$ são definidos como:

    $$Y_k = (X - \mu)u_k$$

    onde $u_k$ é a k-ésima coluna de $U$. Equivalentemente, $Y_k$ é a k-ésima coluna de $VD$ [^774].

5.  **Variância e Covariância:** Uma propriedade importante dos componentes principais é que eles são não correlacionados. Especificamente:

    *   A variância do k-ésimo componente principal é dada por:

        $$var(Y_k) = \frac{\lambda_k^2}{n-1}$$

    *   A covariância entre os componentes principais $Y_h$ e $Y_k$ é zero quando $h \neq k$:

        $$cov(Y_h, Y_k) = 0$$

        Essas propriedades decorrem do fato de que os vetores singulares à esquerda $u_k$ são ortogonais [^774].

**Lema 1:** Os componentes principais $Y_k$ são combinações lineares das colunas de $X-\mu$.

*Prova:* Seja $C_j$ a j-ésima coluna da matriz $X - \mu$. Então, $Y_k = (X - \mu)u_k = \sum_{j=1}^{d} u_{kj}C_j$, onde $u_{kj}$ é o j-ésimo elemento do vetor $u_k$. Portanto, $Y_k$ é uma combinação linear das colunas $C_j$. $\blacksquare$

**Corolário 1:** A variância total dos dados é preservada pelos componentes principais.

*Prova:* A variância total dos dados originais é dada por $\sum_{k=1}^{d} var(C_k)$, onde $C_k$ é a k-ésima coluna de $X$. A variância total dos componentes principais é $\sum_{k=1}^{d} var(Y_k) = \sum_{k=1}^{d} \frac{\lambda_k^2}{n-1}$. Como a SVD preserva a norma de Frobenius, a variância total é preservada. $\blacksquare$

### Direções Principais

Os vetores $u_1, ..., u_d$ (as colunas de $U$) são chamados de *direções principais* de $(X - \mu)$ (e $X$) [^1]. Eles indicam as direções no espaço de dados original ao longo das quais os dados têm a maior variância. A primeira direção principal ($u_1$) corresponde à direção da maior variância, a segunda direção principal ($u_2$) corresponde à direção da segunda maior variância, e assim por diante [^774].

### Interpretação Geométrica

Geometricamente, a PCA pode ser vista como uma rotação dos eixos de coordenadas para um novo sistema de coordenadas alinhado com as direções de máxima variância. Os componentes principais são as projeções dos dados nesses novos eixos [^770].

### Redução de Dimensionalidade

Uma das principais aplicações da PCA é a redução de dimensionalidade. Como os valores singulares $\lambda_k$ estão ordenados em ordem decrescente, podemos selecionar os primeiros $k$ componentes principais (onde $k < d$) que capturam a maior parte da variância dos dados. Isso nos permite representar os dados em um espaço de menor dimensão sem perder muita informação [^768].

### SVD e PCA: Teorema Fundamental

O Teorema 23.11 [^774] do Capítulo 23 estabelece a conexão fundamental entre SVD e PCA:

**Teorema 23.11 (SVD Implica PCA):** Seja $X$ uma matriz $n \times d$ de pontos de dados $X_1, ..., X_n$, e seja $\mu$ o centróide dos $X_i$. Se $X - \mu = VDU^T$ é uma decomposição SVD de $X - \mu$, e se a diagonal principal de $D$ consiste dos valores singulares $\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_d$, então os pontos centrados $Y_1, ..., Y_d$, onde $Y_k = (X - \mu)u_k$ (a k-ésima coluna de $VD$) e $u_k$ é a k-ésima coluna de $U$, são os $d$ componentes principais de $X$. Além disso, $var(Y_k) = \frac{\lambda_k^2}{n-1}$ e $cov(Y_h, Y_k) = 0$ sempre que $h \neq k$ e $1 \leq k, h \leq d$. As $d$ colunas $u_1, ..., u_d$ de $U$ são usualmente chamadas de direções principais de $X - \mu$ (e $X$).

Este teorema formaliza a relação entre a SVD e a PCA, demonstrando que a SVD fornece uma maneira eficiente de calcular os componentes principais e suas variâncias.

### Conclusão
Este capítulo explorou a aplicação da SVD na PCA, detalhando os passos envolvidos no cálculo dos componentes principais, suas propriedades estatísticas e sua interpretação geométrica. A SVD fornece uma base matemática sólida para a PCA, permitindo a redução de dimensionalidade e a identificação de padrões em dados de forma eficiente e precisa. Através da SVD, podemos transformar dados complexos em representações mais simples e interpretáveis, mantendo as informações mais relevantes.

### Referências
[^1]: Capítulo 23 do texto original.
[^769]: Texto original, pág 769.
[^772]: Texto original, pág 772.
[^774]: Texto original, pág 774.
[^770]: Texto original, pág 770.

<!-- END -->