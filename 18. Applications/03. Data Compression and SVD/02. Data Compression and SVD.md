## Data Compression via Singular Value Decomposition (SVD)

### Introdução
Este capítulo explora a aplicação da Decomposição em Valores Singulares (SVD) na compressão de dados, um campo onde a redução da dimensionalidade e a preservação da informação crucial são de suma importância. Como mencionado anteriormente [^1], a compressão de dados é uma das muitas aplicações da SVD. Para quantificar a "proximidade" entre matrizes, introduzimos o conceito de **norma matricial**, uma ferramenta fundamental para avaliar a qualidade da aproximação obtida após a compressão [^1].

### Conceitos Fundamentais

**Norma Matricial**

A noção de **norma matricial** é essencial para definir a proximidade entre matrizes. Uma norma matricial, denotada por $||A||$, é uma função que atribui um valor não negativo a uma matriz $A$, satisfazendo as seguintes propriedades:

1.  $||A|| \geq 0$, e $||A|| = 0$ se e somente se $A = 0$ (não negatividade e definição).
2.  $||\alpha A|| = |\alpha| \cdot ||A||$ para qualquer escalar $\alpha$ (homogeneidade).
3.  $||A + B|| \leq ||A|| + ||B||$ (desigualdade triangular).

Existem diversas normas matriciais, sendo duas particularmente relevantes para a compressão de dados via SVD: a **norma espectral** (ou 2-norma) e a **norma de Frobenius**.

*   **Norma Espectral (2-norma):** Definida como o maior valor singular de $A$, ou seja, $||A||_2 = \sigma_{max}(A)$ [^766]. Equivalentemente, $||A||_2 = \sup_{x \neq 0} \frac{||Ax||_2}{||x||_2}$, onde $||x||_2$ é a norma Euclidiana do vetor $x$.

*   **Norma de Frobenius:** Definida como a raiz quadrada da soma dos quadrados de todos os elementos da matriz, ou seja, $||A||_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij}|^2}$ [^766]. Equivalentemente, $||A||_F = \sqrt{tr(A^T A)}$, onde $tr$ denota o traço da matriz.

**Compressão de Dados via SVD**

A SVD decompõe uma matriz $A_{m \times n}$ de rank $r$ em três matrizes: $A = VDU^T$ [^577, 757], onde:

*   $V$ é uma matriz ortogonal $m \times m$ contendo os vetores singulares esquerdos.
*   $U$ é uma matriz ortogonal $n \times n$ contendo os vetores singulares direitos.
*   $D$ é uma matriz diagonal $m \times n$ contendo os valores singulares não negativos $\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_r > 0$ na diagonal principal [^577, 757].

A ideia central da compressão de dados via SVD é aproximar a matriz original $A$ por uma matriz $A_k$ de rank $k < r$, onde $A_k$ é obtida mantendo apenas os $k$ maiores valores singulares e seus correspondentes vetores singulares [^766]. Matematicamente:

$$A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^T$$

onde $u_i$ e $v_i$ são as colunas de $U$ e $V$, respectivamente [^766].

**Teorema de Eckart-Young**

O **Teorema de Eckart-Young** formaliza a otimalidade da aproximação de rank reduzido obtida via SVD [^766]. Ele afirma que a matriz $A_k$ de rank $k$ obtida truncando a SVD de $A$ é a melhor aproximação de $A$ em termos da norma espectral e da norma de Frobenius. Ou seja, para qualquer matriz $B$ de rank $k$:

$$||A - A_k||_2 \leq ||A - B||_2$$
$$||A - A_k||_F \leq ||A - B||_F$$

Além disso,  $||A - A_k||_2 = \sigma_{k+1}$ [^766] e $A_k$ é dada por:

$$A_k = \sum_{i=1}^{k} \sigma_i v_i u_i^T = V diag(\sigma_1, \dots, \sigma_k, 0, \dots, 0) U^T$$

e $||A - A_k||_2 = \sigma_{k+1}$ [^766].

O erro da aproximação, medido pela norma espectral, é igual ao ($k+1$)-ésimo valor singular, $\sigma_{k+1}$.  Isso significa que ao escolher $k$, estamos controlando diretamente o erro máximo da aproximação.

**Justificativa e Vantagens**

A compressão via SVD funciona bem porque, em muitos conjuntos de dados reais, os valores singulares decaem rapidamente. Isso significa que podemos obter uma boa aproximação de $A$ com um valor de $k$ muito menor que o rank original $r$, resultando em uma significativa redução no armazenamento necessário [^766].  Em vez de armazenar a matriz original $A$ com $mn$ elementos, armazenamos $U$, $V$ e os $k$ maiores valores singulares, totalizando $(m+n)k$ elementos [^766].

### Conclusão
A compressão de dados via SVD é uma técnica poderosa que explora a estrutura intrínseca dos dados para reduzir a dimensionalidade e o armazenamento necessário, mantendo a informação essencial. O Teorema de Eckart-Young garante que a aproximação obtida é ótima em termos de normas matriciais relevantes, proporcionando um controle preciso sobre o erro introduzido pela compressão. O sucesso desta técnica reside na rápida decadência dos valores singulares, permitindo uma representação compacta com um erro aceitável.

### Referências
[^1]: Capítulo 23, Applications of SVD and Pseudo-Inverses.
[^757]: Definition 23.1.
[^766]: Proposition 23.9.

<!-- END -->