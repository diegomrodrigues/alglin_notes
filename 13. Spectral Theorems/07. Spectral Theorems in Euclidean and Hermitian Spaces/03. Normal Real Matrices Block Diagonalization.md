## Block Diagonalização de Matrizes Normais Reais

### Introdução
Este capítulo expande os teoremas espectrais discutidos anteriormente, focando na forma normal para matrizes normais reais. Especificamente, exploraremos como uma matriz normal real pode ser block diagonalizada sobre uma base ortonormal, com blocos de tamanho no máximo dois [^1]. Este resultado fornece uma estrutura mais refinada para a análise de matrizes normais em espaços Euclidianos, complementando o teorema espectral para matrizes simétricas e Hermitianas [^1].

### Conceitos Fundamentais

**Definição de Matriz Normal:** Uma matriz real $A$ é considerada **normal** se $A A^T = A^T A$, onde $A^T$ denota a transposta de $A$ [^626]. Essa condição de comutatividade é crucial para a block diagonalização.

**Teorema da Block Diagonalização:** Seja $A$ uma matriz normal real $n \times n$. Então, existe uma base ortonormal de $\mathbb{R}^n$ tal que a matriz de $A$ com relação a essa base é uma matriz diagonal em blocos, onde cada bloco é de tamanho no máximo $2$ [^1, 618].

**Forma dos Blocos:** Os blocos na forma diagonal podem ser de dois tipos [^618]:
1.  Matrizes $1 \times 1$, que correspondem a autovalores reais de $A$.
2.  Matrizes $2 \times 2$ da forma:
    $$\
    \begin{pmatrix}\
    \lambda & -\mu \\\\\
    \mu & \lambda\
    \end{pmatrix}\
    $$
    onde $\lambda, \mu \in \mathbb{R}$ e $\mu > 0$. Esses blocos correspondem a pares de autovalores complexos conjugados $\lambda \pm i\mu$ de $A$.

**Demonstração:** A demonstração segue por indução na dimensão $n$ do espaço Euclidiano $E$ [^618]. O caso base $n=1$ é trivial. Para $n \geq 2$, utilizamos o fato de que a complexificação $A_c$ de $A$ possui pelo menos um autovalor complexo $z = \lambda + i\mu$, onde $\lambda, \mu \in \mathbb{R}$. Seja $w = u + iv$ um autovetor de $A_c$ correspondente a $z$, onde $u, v \in E$.

Pela Proposição 17.11 [^617], se $\mu \neq 0$, então $(u, v) = 0$ e $||u|| = ||v||$, o que implica que $u$ e $v$ são linearmente independentes. Além disso, se $W$ é o subespaço gerado por $u$ e $v$, então $A(W) = W$ e $A^T(W) = W$. Com relação à base ortogonal $\\{u/||u||, v/||v||\\}$, a restrição de $A$ a $W$ tem a matriz:
$$\
\begin{pmatrix}\
\lambda & \mu \\\\\
-\mu & \lambda\
\end{pmatrix}\
$$

Se $\mu < 0$, trocamos os sinais para que $\mu > 0$. O complemento ortogonal $W^\perp$ tem dimensão $n-2$ e é invariante sob $A$, ou seja, $A(W^\perp) \subseteq W^\perp$. Como a restrição de $A$ a $W^\perp$ também é normal, podemos aplicar a hipótese de indução a $W^\perp$ [^618].

Se $\mu = 0$, então $\lambda$ é um autovalor real de $A$, e $u$ ou $v$ é um autovetor de $A$. Seja $W$ o subespaço de dimensão 1 gerado por um autovetor unitário $e_1 = u/||u||$ (ou $v/||v||$). Então, $A(W) \subseteq W$ e $A^T(W) \subseteq W$. O complemento ortogonal $W^\perp$ tem dimensão $n-1$ e é invariante sob $A$. Novamente, a restrição de $A$ a $W^\perp$ é normal, e aplicamos a hipótese de indução a $W^\perp$ [^618].

**Corolário:** Toda matriz ortogonal real pode ser transformada, por uma similaridade ortogonal, em uma matriz diagonal em blocos, onde cada bloco é ou $\pm 1$ ou uma matriz de rotação da forma
$$\
\begin{pmatrix}\
\cos \theta & -\sin \theta \\\\\
\sin \theta & \cos \theta\
\end{pmatrix}\
$$
onde $0 < \theta < \pi$ [^623].

**Lema:** Se $f: E \rightarrow E$ é uma transformação linear normal, então $ker(f) = ker(f^*)$ [^610].

*Prova:* Seja $u, v \in E$. Como $f^*$ é o adjunto de $f$ e $f \circ f^* = f^* \circ f$, temos:
$$(f(u), f(v)) = (u, (f^* \circ f)(v)) = (u, (f \circ f^*)(v)) = (f^*(u), f^*(v))$$
Em particular, $(f(u), f(u)) = (f^*(u), f^*(u))$. Como o produto interno é positivo definido, $(f(u), f(u)) = 0$ se e somente se $f(u) = 0$, e $(f^*(u), f^*(u)) = 0$ se e somente se $f^*(u) = 0$. Portanto, $f(u) = 0$ se e somente se $f^*(u) = 0$, o que implica que $ker(f) = ker(f^*)$. $\blacksquare$

### Conclusão
A block diagonalização de matrizes normais reais fornece uma ferramenta poderosa para simplificar a análise e a compreensão de suas propriedades espectrais. Ao decompor a matriz em blocos de tamanho no máximo dois, podemos reduzir a complexidade dos cálculos e obter informações sobre os autovalores e autovetores da matriz original [^1]. Este resultado complementa os teoremas espectrais para matrizes simétricas e Hermitianas, oferecendo uma perspectiva unificada sobre a estrutura de operadores normais em diferentes espaços vetoriais [^1]. Além disso, a forma normal obtida facilita a aplicação de técnicas de otimização e análise de dados, como discutido em outros capítulos [^1].

### Referências
[^1]: Capítulo 17, Spectral Theorems in Euclidean and Hermitian Spaces.
[^610]: Definition 17.1 e Proposition 17.1.
[^617]: Proposition 17.11.
[^618]: Theorem 17.12.
[^623]: Theorem 17.16.
[^626]: Definition 17.3.

<!-- END -->