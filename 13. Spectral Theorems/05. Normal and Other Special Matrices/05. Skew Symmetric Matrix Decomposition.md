## Decomposição de Matrizes Skew-Symmetric Reais

### Introdução
Este capítulo explora as formas normais para matrizes skew-symmetric reais. O objetivo é demonstrar que, para cada matriz skew-symmetric $A$, existe uma matriz ortogonal $P$ tal que $A$ pode ser decomposta como $A = PDP^T$, onde $D$ é uma matriz diagonal em blocos com uma estrutura específica [^1]. Essa decomposição revela propriedades importantes dos autovalores de matrizes skew-symmetric. Este capítulo se baseia nos conceitos de matrizes normais, matrizes skew-symmetric e teoremas espectrais, conforme discutido anteriormente [^1].

### Conceitos Fundamentais
Uma matriz $A$ é dita **skew-symmetric** se $A^T = -A$ [^18]. O Teorema 17.20 [^19] estabelece que para cada matriz skew-symmetric real $A$, existe uma matriz ortogonal $P$ e uma matriz diagonal em blocos $D$ tal que $A = PDP^T$, onde $D$ é da forma:

$$
D = \begin{pmatrix}
D_1 & & \\
& D_2 & \\
& & \ddots \\
& & & D_p
\end{pmatrix}
$$

Cada bloco $D_j$ é uma matriz $0$ ou uma matriz bidimensional da forma:

$$
D_j = \begin{pmatrix}
0 & -\mu_j \\
\mu_j & 0
\end{pmatrix}
$$

onde $\mu_j \in \mathbb{R}$, com $\mu_j > 0$ [^19]. Em particular, os autovalores de $A$ são puramente imaginários da forma $\pm i\mu_j$, ou 0.

**Prova:**
A prova começa observando que, para uma matriz skew-symmetric $A$, $f_c$ possui algum autovalor $z = \lambda + i\mu$, onde $\lambda, \mu \in \mathbb{R}$. Afirmamos que $\lambda = 0$ [^14]. Primeiro, mostramos que $(f(w), w) = 0$ para todo $w \in E$. De fato, como $f = -f^*$, temos
$$(f(w), w) = (w, f^*(w)) = (w, -f(w)) = -(w, f(w)) = -(f(w), w),$$
já que $( \cdot, \cdot )$ é simétrico. Isso implica que $(f(w), w) = 0$ [^14]. Aplicando isso a $u$ e $v$ e usando o fato de que $f(u) = \lambda v - \mu u$ e $f(v) = \mu u + \lambda v$, obtemos
$$0 = (f(u), u) = (\lambda v - \mu u, u) = \lambda (v, u) - \mu (u, u)$$
e
$$0 = (f(v), v) = (\mu u + \lambda v, v) = \mu (u, v) + \lambda (v, v),$$
do qual, por adição, obtemos
$$\lambda ((u, u) + (v, v)) = 0.$$
Como $u \neq 0$ ou $v \neq 0$, temos $\lambda = 0$ [^14].

Portanto, voltando à prova do Teorema 17.12 [^10], a menos que $\mu = 0$, aplica-se o caso onde $u$ e $v$ são ortogonais e abrangem um subespaço de dimensão 2, e a indução mostra que todos os blocos são bidimensionais ou reduzidos a 0 $\blacksquare$ [^14].

**Exemplo:**
Considere a matriz skew-symmetric:
$$
A = \begin{pmatrix}
0 & -2 \\
2 & 0
\end{pmatrix}
$$

Os autovalores de $A$ são $\pm 2i$. A matriz $A$ já está na forma de bloco diagonal descrita no teorema, com $\mu_1 = 2$.

### Conclusão
A decomposição de matrizes skew-symmetric reais na forma $A = PDP^T$ com $D$ sendo uma matriz diagonal em blocos, onde os blocos são da forma especificada, oferece uma visão clara da estrutura e das propriedades espectrais dessas matrizes. Em particular, essa decomposição demonstra que os autovalores de matrizes skew-symmetric reais são sempre puramente imaginários ou zero. Este resultado é uma especialização do Teorema Espectral para matrizes normais [^19], e fornece uma ferramenta poderosa para analisar e compreender sistemas lineares com simetrias específicas.

### Referências
[^1]: Capítulo 17, Introdução.
[^18]: Definição de matriz skew-symmetric, Seção 17.5.
[^19]: Teorema 17.20, Seção 17.5.
[^14]: Prova do Teorema 17.15
[^10]: Teorema 17.12

<!-- END -->