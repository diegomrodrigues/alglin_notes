## Capítulo 7.5.1: Critérios de Determinantes para Existência e Unicidade de Soluções em Sistemas Lineares

### Introdução

Em continuidade à nossa exploração dos determinantes e suas propriedades fundamentais, como detalhado nas seções anteriores deste capítulo [^1]-[^20], este segmento aprofunda a aplicação dos determinantes na análise de sistemas de equações lineares. Especificamente, focaremos em estabelecer um critério rigoroso, baseado no determinante da matriz de coeficientes, para determinar a existência e unicidade de soluções para sistemas da forma $Ax = b$. Construindo sobre a definição de determinantes [^9], suas propriedades de multilinearidade e alternância [^6, ^12], a relação entre o determinante e a invertibilidade de matrizes [^19, ^20], e a conexão entre o determinante e a independência linear das colunas de uma matriz [^21], demonstraremos uma proposição central que interliga essas ideias. A Proposição 7.12 [^22] encapsula essa relação, fornecendo uma ferramenta teórica poderosa para a análise de sistemas lineares.

### Conceitos Fundamentais: Independência Linear e Determinantes

Antes de abordarmos o teorema principal, é crucial revisitar a conexão entre o determinante de uma matriz quadrada e a independência linear de seus vetores coluna, como estabelecido na Proposição 7.11 [^21]. Seja $A$ uma matriz $n \times n$ sobre um corpo $K$, e $A^1, ..., A^n$ suas colunas. O sistema $Ax = b$ pode ser interpretado como a busca por coeficientes $x_1, ..., x_n$ tais que a combinação linear das colunas de $A$ iguale o vetor $b$:

$$ x_1A^1 + \dots + x_jA^j + \dots + x_nA^n = b $$ [^21]

A Proposição 7.11 [^21] afirma que:

> Os vetores coluna $A^1, ..., A^n$ de $A$ são **linearmente dependentes** se e somente se $\det(A) = \det(A^1, ..., A^n) = 0$. Equivalentemente, a matriz $A$ tem posto $n$ (ou seja, suas colunas formam uma base para $K^n$) se e somente se $\det(A) \neq 0$. [^22]

A prova dessa proposição [^22] utiliza a propriedade de multilinearidade e alternância do determinante. Se as colunas são linearmente dependentes, existe uma combinação linear não trivial $x_1A^1 + \dots + x_nA^n = 0$ com algum $x_j \neq 0$. Substituindo a $j$-ésima coluna $A^j$ por essa combinação linear (que é o vetor nulo), e usando a multilinearidade, demonstra-se que $x_j \det(A) = 0$. Como $x_j \neq 0$ e $K$ é um corpo, conclui-se que $\det(A) = 0$. Reciprocamente, se as colunas são linearmente independentes, elas formam uma base para $K^n$. A base canônica $(e_1, ..., e_n)$ pode ser expressa em termos dessa base, $e_k = \sum_{i=1}^n b_{ik} A^i$, definindo uma matriz $B=(b_{ij})$. Usando a Proposição 7.8 [^22] (uma reformulação do Lema 7.4 [^7, ^8]), temos $\det(e_1, ..., e_n) = \det(B) \det(A^1, ..., A^n)$. Como $\det(e_1, ..., e_n) = \det(I_n) = 1$ [^12, ^13], segue que $\det(A) \neq 0$ [^22].

Esta proposição é fundamental, pois conecta a propriedade algébrica da independência linear com o valor numérico do determinante.

### O Critério do Determinante para Soluções Únicas

Agora, estamos prontos para enunciar e provar a proposição central que fornece o critério para a existência e unicidade de soluções de sistemas lineares.

**Proposição 7.12 (Critério de Existência e Unicidade)** [^22]
Dada uma matriz $n \times n$ $A$ sobre um corpo $K$, as seguintes propriedades são equivalentes:
(1) Para cada vetor coluna $b$, existe um único vetor coluna $x$ tal que $Ax = b$.
(2) A única solução para o sistema homogêneo $Ax = 0$ é o vetor trivial $x = 0$.
(3) O determinante de $A$ é não nulo, $\det(A) \neq 0$.

*Prova.* Vamos demonstrar a equivalência dessas três afirmações seguindo a lógica apresentada na prova contextual [^23].

**(3) $\implies$ (1):** Assumimos $\det(A) \neq 0$. Como $K$ é um corpo, $\det(A)$ é um elemento invertível em $K$. Pela Proposição 7.10 [^19, ^20], uma matriz $A$ é invertível se e somente se $\det(A)$ é invertível. Portanto, $A$ possui uma inversa $A^{-1}$. Para qualquer vetor $b$, podemos multiplicar a equação $Ax=b$ à esquerda por $A^{-1}$, obtendo $A^{-1}(Ax) = A^{-1}b$, o que simplifica para $I_n x = A^{-1}b$, ou seja, $x = A^{-1}b$. Esta solução existe e é única, pois é dada explicitamente pela multiplicação de $A^{-1}$ por $b$ [^23].

**(1) $\implies$ (2):** Assumimos que para cada $b$, $Ax=b$ tem uma solução única $x_0$. Consideremos o sistema homogêneo $Ax=0$. Claramente, $x=0$ é uma solução (o **vetor trivial**). Suponhamos, por contradição, que exista uma solução não trivial $y \neq 0$ tal que $Ay=0$. Seja $x_0$ a única solução para $Ax=b$. Então, considere $x_0 + y$. Temos $A(x_0 + y) = Ax_0 + Ay = b + 0 = b$. Como $y \neq 0$, temos $x_0 + y \neq x_0$. Isso significa que $x_0$ e $x_0 + y$ são duas soluções distintas para $Ax=b$, o que contradiz a hipótese de unicidade da solução $x_0$. Portanto, a única solução para $Ax=0$ deve ser a trivial, $x=0$ [^23].

**(2) $\implies$ (3):** Assumimos que a única solução para $Ax=0$ é $x=0$. A equação $Ax=0$ é equivalente a $x_1A^1 + \dots + x_nA^n = 0$ [^21]. A hipótese de que apenas $x=0$ (ou seja, $x_1 = x_2 = \dots = x_n = 0$) satisfaz esta equação é a definição de **independência linear** dos vetores coluna $A^1, ..., A^n$ [^21, ^23]. De acordo com a Proposição 7.11 [^21, ^22], a independência linear das colunas de $A$ é equivalente a $\det(A) \neq 0$. Portanto, $\det(A) \neq 0$.

Com isso, demonstramos as implicações (3) $\implies$ (1) $\implies$ (2) $\implies$ (3), estabelecendo a equivalência das três afirmações. $\blacksquare$

> **Caixa de Destaque: Equivalências Fundamentais**
> Para uma matriz quadrada $A$ sobre um corpo $K$:
> *   Existe solução única para $Ax=b$ para todo $b$.
> *   O sistema homogêneo $Ax=0$ admite apenas a solução trivial $x=0$.
> *   Os vetores coluna de $A$ são linearmente independentes.
> *   A matriz $A$ tem posto $n$.
> *   A matriz $A$ é invertível.
> *   $\det(A) \neq 0$.

Estas equivalências são pilares da álgebra linear e interconectam diversas propriedades importantes de matrizes e sistemas lineares.

Adicionalmente, a Proposição 7.12 (3) [^22] estabelece que o sistema homogêneo $Ax=0$ possui uma solução não nula se e somente se $\det(A)=0$. Isso segue diretamente da equivalência entre a existência de solução não trivial para $Ax=0$ e a dependência linear das colunas de $A$, que por sua vez é equivalente a $\det(A)=0$ pela Proposição 7.11 [^21, ^23].

Quando $\det(A) \neq 0$, a Proposição 7.12 (2) [^22] também fornece a fórmula explícita para a única solução $x = (x_1, ..., x_n)$ do sistema $Ax=b$, conhecida como **Regra de Cramer**:\n\n$$ x_j = \frac{\det(A^1, \dots, A^{j-1}, b, A^{j+1}, \dots, A^n)}{\det(A^1, \dots, A^{j-1}, A^j, A^{j+1}, \dots, A^n)} $$ [^22]

Esta fórmula é derivada [^23] ao calcular o determinante $\det(A^1, \dots, A^{j-1}, b, A^{j+1}, \dots, A^n)$. Substituindo $b$ por $x_1A^1 + \dots + x_nA^n$ e usando a multilinearidade do determinante, todos os termos onde uma coluna $A^k$ aparece duas vezes se anulam (pois o determinante é alternante [^6, ^12]), restando apenas o termo $x_j \det(A^1, \dots, A^n) = x_j \det(A)$. Embora elegante, a Regra de Cramer é geralmente considerada impraticável para cálculos numéricos em sistemas grandes, comparada a métodos como a eliminação Gaussiana [^21, ^23]. No entanto, ela tem importância teórica, por exemplo, mostrando que a solução $x$ varia continuamente (ou diferencialmente) com as entradas de $A$ e $b$, desde que $\det(A)$ permaneça não nulo [^23].

### Conclusão

Este capítulo estabeleceu um elo fundamental entre o determinante de uma matriz quadrada $A$ e as propriedades das soluções dos sistemas de equações lineares $Ax=b$. Demonstramos, com base nas proposições e definições apresentadas no contexto [^1]-[^23], que a condição $\det(A) \neq 0$ é um critério necessário e suficiente para garantir a existência e unicidade de solução para $Ax=b$ para qualquer vetor $b$. Esta condição é equivalente à afirmação de que o sistema homogêneo associado $Ax=0$ admite apenas a solução trivial, o que por sua vez é equivalente à independência linear das colunas de $A$ e à invertibilidade da matriz $A$. O determinante, portanto, emerge não apenas como uma ferramenta computacional ou uma medida de volume [^13], mas como um indicador crucial da natureza das soluções de sistemas lineares e das propriedades estruturais da transformação linear representada pela matriz $A$.

### Referências

[^1]: Página 205 do contexto (Introdução ao Capítulo 7).
[^2]: Página 206 do contexto (Permutações, notação).
[^3]: Página 207 do contexto (Proposição 7.1, decomposição em ciclos e transposições).
[^4]: Página 208 do contexto (Definição 7.2, assinatura $\epsilon(\pi)$; Proposição 7.2, paridade).
[^5]: Página 209 do contexto (Definição 7.3, mapas multilineares).
[^6]: Página 210 do contexto (Mapas alternantes; Proposição 7.3, propriedades de mapas alternantes).
[^7]: Página 211 do contexto (Notação L(A), Lema 7.4).
[^8]: Página 212 do contexto (Prova do Lema 7.4, fórmula do determinante com permutações).
[^9]: Página 213 do contexto (Definição 7.4, determinante como mapa D: Mn(K) -> K, n-linear alternante com D(In)=1).
[^10]: Página 214 do contexto (Definição 7.5, menor; Definição 7.6, definição indutiva de D via expansão de Laplace).
[^11]: Página 215 do contexto (Cofator, expansão de Laplace por linha).
[^12]: Página 216 do contexto (Lema 7.5, prova que D definido indutivamente é multilinear alternante e D(In)=1).
[^13]: Página 217 do contexto (Teorema 7.6, unicidade do determinante e fórmula explícita; interpretação geométrica).
[^14]: Página 218 do contexto (Corolário 7.7, det(A) = det(A^T); determinante como mapa multilinear alternante das linhas).
[^15]: Página 219 do contexto (Exemplo 7.2, Determinante de Vandermonde).
[^16]: Página 220 do contexto (Exemplo 7.3, Determinante de matrizes triangulares e bloco-triangulares).
[^17]: Página 221 do contexto (Proposição 7.8, reformulação do Lema 7.4; Proposição 7.9, det(AB) = det(A)det(B)).
[^18]: Página 222 do contexto (Definição 7.7, matriz adjugada $\tilde{A}$).
[^19]: Página 223 do contexto (Proposição 7.10, $A\tilde{A} = \tilde{A}A = \det(A)I_n$; consequência sobre invertibilidade e fórmula da inversa).
[^20]: Página 224 do contexto (Prova da Proposição 7.10; exemplo).
[^21]: Página 225 do contexto (Seção 7.5, introdução a sistemas lineares; equivalência $Ax=b$ e combinação linear; Proposição 7.11 sobre dependência linear e determinante).
[^22]: Página 226 do contexto (Prova da Proposição 7.11; Proposição 7.12 enunciando as equivalências e a Regra de Cramer).
[^23]: Página 227 do contexto (Prova da Proposição 7.12; discussão sobre a Regra de Cramer).
[^24]: Página 228 do contexto (Determinante de um operador linear; Definição 7.8; Proposição 7.13).
[^25]: Página 229 do contexto (Teorema de Cayley-Hamilton, Teorema 7.14; Definição 7.9, polinômio característico).
[^26]: Página 230 do contexto (Prova do Teorema de Cayley-Hamilton).
[^27]: Página 231 do contexto (Segunda prova do Teorema de Cayley-Hamilton, abordagem via K[X]-módulo).
[^28]: Página 232 do contexto (Continuação da segunda prova).
[^29]: Página 233 do contexto (Permanentes, Definição).
[^30]: Página 234 do contexto (Permanentes, fórmula de Laplace, grafos bipartidos).
[^31]: Página 235 do contexto (Permanentes, matchings perfeitos, #P-completude, SDRs).
[^32]: Página 236 do contexto (Resumo do Capítulo 7).
[^33]: Página 237 do contexto (Leituras Adicionais, Problemas).

<!-- END -->