## Capítulo 7.5: Determinantes Não-Nulos e a Unicidade de Soluções em Sistemas Lineares

### Introdução

Em capítulos anteriores e seções precedentes, exploramos a definição e as propriedades fundamentais dos **determinantes** [^1]. Vimos como os determinantes podem ser calculados, por exemplo, através da fórmula de expansão de Laplace [^10], [^11] ou pela definição direta envolvendo permutações [^13]. Estabelecemos propriedades cruciais como $det(A) = det(A^T)$ [^14] e a multiplicatividade $det(AB) = det(A)det(B)$ [^17]. Introduzimos também a matriz **adjugata** $\\tilde{A}$ [^18] e sua relação com a **matriz inversa** $A^{-1}$ [^19]. Este capítulo se aprofunda em uma das aplicações mais significativas dos determinantes no contexto da álgebra linear: a caracterização da existência e unicidade de soluções para sistemas de equações lineares da forma $Ax = b$. Focaremos em demonstrar rigorosamente o teorema fundamental que conecta um determinante não-nulo da matriz $A$ à garantia de uma solução única para qualquer vetor $b$, assumindo que $A$ é uma matriz quadrada $n \\times n$ sobre um corpo $K$ [^21].

### Conceitos Fundamentais

#### Determinantes e Invertibilidade de Matrizes

A conexão entre o determinante e a invertibilidade de uma matriz é a pedra angular para entender a unicidade de soluções em sistemas lineares. Como estabelecido na Proposição 7.10, para qualquer matriz quadrada $A \\in M_n(K)$ sobre um anel comutativo $K$, a relação entre $A$, sua adjugata $\\tilde{A}$, e seu determinante é dada por [^19]:

$$ A\\tilde{A} = \\tilde{A}A = det(A)I_n $$

> Uma consequência direta desta proposição é que a matriz $A$ é invertível se, e somente se, seu determinante $det(A)$ for invertível no anel $K$ [^19].

Quando trabalhamos especificamente sobre um corpo $K$ (como $\\mathbb{R}$ ou $\\mathbb{C}$), um elemento $a \\in K$ é invertível se e somente se $a \\neq 0$ [^21]. Portanto, para matrizes sobre um corpo:

> **Teorema (Invertibilidade via Determinante):** Uma matriz $A \\in M_n(K)$ sobre um corpo $K$ é invertível se e somente se $det(A) \\neq 0$.

Adicionalmente, quando $det(A) \\neq 0$, a Proposição 7.10 nos fornece a fórmula explícita para a inversa [^19]:

$$ A^{-1} = (det(A))^{-1}\\tilde{A} $$

Recorde que a matriz adjugata $\\tilde{A} = (b_{ij})$ é definida por $b_{ij} = (-1)^{i+j} det(A_{ji})$, onde $A_{ji}$ é o **minor** obtido pela remoção da linha $j$ e coluna $i$ de $A$ [^18], [^19]. Assim, $\\tilde{A}$ é a transposta da matriz de cofatores de $A$ [^19].

#### Determinantes e Independência Linear

A resolução de sistemas lineares $Ax = b$ está intrinsecamente ligada à independência linear das colunas da matriz $A$. Um sistema $Ax = b$ pode ser reescrito como uma combinação linear das colunas de $A$, denotadas por $A^1, \\dots, A^n$ [^21]:

$$ x_1A^1 + x_2A^2 + \\dots + x_nA^n = b $$

A Proposição 7.11 estabelece a relação fundamental entre a dependência linear das colunas e o determinante [^21]:

> **Proposição 7.11:** Dada uma matriz $n \\times n$ $A$ sobre um corpo $K$, as colunas $A^1, \\dots, A^n$ de $A$ são linearmente dependentes se e somente se $det(A) = det(A^1, \\dots, A^n) = 0$. Equivalentemente, $A$ tem posto $n$ (suas colunas são linearmente independentes) se e somente se $det(A) \\neq 0$.

A demonstração da parte "se $det(A) = 0$, então as colunas são dependentes" envolve mostrar que se $det(A) \\neq 0$, as colunas $A^1, \\dots, A^n$ formam uma base de $K^n$. Neste caso, a base canônica $(e_1, \\dots, e_n)$ pode ser expressa em termos das colunas de $A$, e utilizando a Proposição 7.8 [^17] (que relaciona $f(v_1, \\dots, v_n) = det(M)f(u_1, \\dots, u_n)$ quando $v_j = \\sum M_{ij}u_i$) com $f=det$ e a base canônica, conclui-se que $det(A) \\neq 0$ [^22]. Por outro lado, se as colunas são dependentes, existe uma combinação linear não-trivial $x_1A^1 + \\dots + x_nA^n = 0$ com algum $x_j \\neq 0$. Usando a multilinearidade e a propriedade alternante do determinante, ao calcular $det(A^1, \\dots, \\sum x_k A^k, \\dots, A^n)$ (onde a soma está na $j$-ésima posição), obtemos $x_j det(A^1, \\dots, A^n) = 0$. Como $x_j \\neq 0$ e $K$ é um corpo, devemos ter $det(A) = 0$ [^22].

#### O Teorema Fundamental: Unicidade da Solução e Determinantes

Agora estamos prontos para enunciar e provar o resultado central que conecta determinantes não-nulos à unicidade de soluções de sistemas lineares.

> **Proposição 7.12 (1):** Dado uma matriz $n \\times n$ $A$ sobre um corpo $K$, as seguintes propriedades são equivalentes:
> (a) Para cada vetor coluna $b$, existe um único vetor coluna $x$ tal que $Ax = b$.
> (b) A única solução para $Ax = 0$ é o vetor trivial $x = 0$.
> (c) $det(A) \\neq 0$.

**Prova (Seguindo [^23]):**

*   **(a) $\\Rightarrow$ (b):** Assuma que $Ax = b$ tem uma única solução $x_0$ para um dado $b$. Se existisse uma solução não-trivial $y \\neq 0$ para o sistema homogêneo $Ay = 0$, então $x_0 + y$ também seria uma solução para $Ax = b$, pois $A(x_0 + y) = Ax_0 + Ay = b + 0 = b$. Como $y \\neq 0$, teríamos $x_0 + y \\neq x_0$, contradizendo a unicidade da solução $x_0$. Portanto, $Ax = 0$ deve ter apenas a solução trivial $x = 0$.

*   **(b) $\\Rightarrow$ (c):** Se $Ax = 0$ tem apenas a solução trivial $x = 0$, isso significa que a única combinação linear das colunas de $A$ que resulta no vetor nulo é a trivial ($x_1A^1 + \\dots + x_nA^n = 0 \\implies x_1 = \\dots = x_n = 0$). Isso é a definição de independência linear das colunas $A^1, \\dots, A^n$. Pela Proposição 7.11 [^21], a independência linear das colunas é equivalente a $det(A) \\neq 0$.

*   **(c) $\\Rightarrow$ (a):** Assuma $det(A) \\neq 0$. Pelo Teorema da Invertibilidade via Determinante (baseado na Proposição 7.10 [^19]), a matriz $A$ é invertível. Dado qualquer vetor $b$, podemos definir $x = A^{-1}b$. Este $x$ é uma solução, pois $Ax = A(A^{-1}b) = (AA^{-1})b = I_n b = b$. Para mostrar a unicidade, suponha que $y$ seja outra solução, ou seja, $Ay = b$. Então, subtraindo as equações, temos $Ax - Ay = b - b$, o que implica $A(x-y) = 0$. Como $A$ é invertível, podemos multiplicar ambos os lados por $A^{-1}$ pela esquerda: $A^{-1}A(x-y) = A^{-1}0$, resultando em $I_n(x-y) = 0$, ou seja, $x-y = 0$. Portanto, $x = y$, demonstrando que a solução é única. $\\blacksquare$

Um corolário imediato desta prova é a forma explícita da solução única quando $det(A) \\neq 0$:

> **Corolário:** Se $det(A) \\neq 0$, a única solução para o sistema $Ax = b$ é dada por $x = A^{-1}b$.

Combinando com a fórmula da inversa $A^{-1} = (det(A))^{-1}\\tilde{A}$ [^19], a solução pode ser expressa em termos da matriz adjugata. A Proposição 7.12 (2) [^22] apresenta as **Regras de Cramer**, que fornecem uma fórmula para cada componente $x_j$ da solução:

$$ x_j = \\frac{det(A^1, \\dots, A^{j-1}, b, A^{j+1}, \\dots, A^n)}{det(A^1, \\dots, A^n)} $$

Esta fórmula é derivada calculando $det(A^1, \\dots, b, \\dots, A^n)$ e substituindo $b$ por $\\sum_{k=1}^n x_k A^k$. A multilinearidade e a propriedade alternante do determinante [^6], [^12] fazem com que todos os termos se anulem, exceto aquele onde $x_j A^j$ substitui $b$, resultando em $x_j det(A)$ no numerador após a expansão [^23].

Finalmente, a Proposição 7.12 (3) afirma que o sistema homogêneo $Ax = 0$ tem uma solução não-nula se e somente se $det(A) = 0$ [^22]. Isso é uma reafirmação direta da Proposição 7.11 [^21], pois a existência de uma solução não-nula para $Ax=0$ é precisamente a definição de dependência linear das colunas $A^1, \\dots, A^n$.

### Conclusão

Este capítulo estabeleceu a conexão vital entre o determinante de uma matriz quadrada $A$ e a natureza das soluções do sistema linear $Ax = b$ sobre um corpo $K$. Demonstramos rigorosamente, baseando-nos nas propriedades de invertibilidade [^19] e independência linear [^21] associadas ao determinante, que a condição $det(A) \\neq 0$ é necessária e suficiente para garantir a existência e unicidade de uma solução para $Ax = b$ para qualquer vetor $b$. Além disso, vimos que quando esta condição é satisfeita, a solução única é dada por $x = A^{-1}b$, que pode ser calculada explicitamente usando a matriz adjugata [^19] ou através das Regras de Cramer [^22]. O determinante, portanto, emerge não apenas como uma ferramenta de cálculo, mas como um indicador fundamental da estrutura e das propriedades de sistemas lineares e transformações lineares associadas.

### Referências

[^1]: Capítulo 7, p. 205.
[^5]: Definição 7.3, p. 209.
[^6]: Proposição 7.3 e definição de mapa alternante, p. 210.
[^7]: Lemma 7.4 setup, p. 211.
[^8]: Lemma 7.4 e prova, p. 212.
[^9]: Definição 7.4, p. 213.
[^10]: Definição 7.5 e 7.6, p. 214.
[^11]: Definição de cofator e expansão de Laplace, p. 215.
[^12]: Lemma 7.5, p. 216.
[^13]: Teorema 7.6, p. 217.
[^14]: Corolário 7.7, p. 218.
[^17]: Proposição 7.8 e 7.9, p. 221.
[^18]: Definição 7.7 (Matriz Adjugata), p. 222.
[^19]: Proposição 7.10 (Relação Adjugata-Inversa), p. 223.
[^20]: Prova da Proposição 7.10, pp. 223-224.
[^21]: Seção 7.5 Introdução e Proposição 7.11 (Dependência Linear e Determinante), p. 225.
[^22]: Proposição 7.12 (Unicidade de Solução, Regras de Cramer), p. 226.
[^23]: Prova da Proposição 7.12, p. 227.

<!-- END -->