## Invertibilidade de Matrizes via Determinantes em Anéis Comutativos

### Introdução

Este capítulo aprofunda a relação intrínseca entre a invertibilidade de uma matriz quadrada e o valor de seu determinante. Expandindo os conceitos de determinantes, suas propriedades e a matriz adjunta introduzidos anteriormente, estabeleceremos um critério fundamental para a invertibilidade que se aplica não apenas a corpos (*fields*), mas de forma mais geral a anéis comutativos (*commutative rings*) $K$ [^1]. Demonstraremos como o determinante encapsula informações cruciais sobre a existência de uma matriz inversa e forneceremos uma fórmula explícita para a inversa, quando existente, utilizando a matriz adjunta. Esta análise reforça a importância do determinante como uma ferramenta teórica e computacional na álgebra linear. Faremos uso de propriedades já discutidas, como a multiplicatividade do determinante, $\det(AB) = \det(A)\det(B)$ (Proposição 7.9 [^2]), para fundamentar nossos resultados.

### Conceitos Fundamentais

#### A Matriz Adjunta (Adjugate Matrix)

Um conceito central para conectar o determinante à inversa de uma matriz é a **matriz adjunta** (*adjugate matrix*). Seja $K$ um anel comutativo e $A \in M_n(K)$ uma matriz $n \times n$.

> **Definição 7.7 [^3]:** A **matriz adjunta** de $A$, denotada por $\tilde{A}$, é a matriz $\tilde{A} = (b_{ij})$ onde cada entrada $b_{ij}$ é definida como
> $$ b_{ij} = (-1)^{i+j} \det(A_{ji}) $$
> Aqui, $\det(A_{ji})$ é o determinante do **minor** (*minor*) $A_{ji}$, que é a matriz $(n-1) \times (n-1)$ obtida pela remoção da linha $j$ e da coluna $i$ de $A$. O termo $(-1)^{i+j} \det(A_{ji})$ é o **cofator** (*cofactor*) do elemento $a_{ji}$ da matriz $A$.

É crucial observar a inversão dos índices na definição de $b_{ij}$, que utiliza o determinante do minor $A_{ji}$ [^4]. Isso significa que a matriz $\tilde{A}$ é a transposta da matriz de cofatores dos elementos de $A$ [^5].

Consideremos o exemplo fornecido no contexto [^6]:
Se $ A = \begin{pmatrix} 1 & 1 & 1 \\\\ 2 & -2 & -2 \\\\ 3 & 3 & -3 \end{pmatrix} $, então os elementos de $\tilde{A}$ são calculados como:
$b_{11} = (-1)^{1+1} \det(A_{11}) = \det \begin{pmatrix} -2 & -2 \\\\ 3 & -3 \end{pmatrix} = (-2)(-3) - (-2)(3) = 6 - (-6) = 12$
$b_{12} = (-1)^{1+2} \det(A_{21}) = -\det \begin{pmatrix} 1 & 1 \\\\ 3 & -3 \end{pmatrix} = -((1)(-3) - (1)(3)) = -(-3 - 3) = 6$
$b_{13} = (-1)^{1+3} \det(A_{31}) = \det \begin{pmatrix} 1 & 1 \\\\ -2 & -2 \end{pmatrix} = (1)(-2) - (1)(-2) = -2 - (-2) = 0$
Continuando de forma similar para as outras entradas, obtemos:
$ \tilde{A} = \begin{pmatrix} 12 & 6 & 0 \\\\ 0 & -6 & 4 \\\\ 12 & 0 & -4 \end{pmatrix} $ [^6].

#### A Relação Fundamental: $A\tilde{A} = \tilde{A}A = \det(A)I_n$

A importância da matriz adjunta reside na sua relação com a matriz original e o seu determinante, encapsulada na seguinte proposição.

> **Proposição 7.10 [^7]:** Seja $K$ um anel comutativo. Para toda matriz $A \in M_n(K)$, temos
> $$ A\tilde{A} = \tilde{A}A = \det(A)I_n $$
> onde $I_n$ é a matriz identidade $n \times n$.

*Prova:* Seja $A = (a_{ij})$ e $\tilde{A} = (b_{ij})$. Consideremos o produto $C = A\tilde{A}$, onde $C = (c_{ij})$. A entrada $c_{ij}$ na linha $i$ e coluna $j$ de $C$ é dada por $c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}$ [^8]. Usando a definição de $b_{kj} = (-1)^{k+j} \det(A_{jk})$, temos:
$$ c_{ij} = \sum_{k=1}^{n} a_{ik} (-1)^{k+j} \det(A_{jk}) $$

Analisemos dois casos:

1.  **Caso Diagonal (j = i):**
    $$ c_{ii} = \sum_{k=1}^{n} a_{ik} (-1)^{k+i} \det(A_{ik}) $$
    Esta é precisamente a expansão de Laplace do determinante de $A$ ao longo da $i$-ésima linha [^9]. Portanto, $c_{ii} = \det(A)$.

2.  **Caso Fora da Diagonal (j ≠ i):**
    $$ c_{ij} = \sum_{k=1}^{n} a_{ik} (-1)^{k+j} \det(A_{jk}) $$
    Consideremos uma matriz $A\'$ formada substituindo a $j$-ésima linha de $A$ pela $i$-ésima linha de $A$ [^10]. A matriz $A\'$ possui duas linhas idênticas (a $i$-ésima e a $j$-ésima). Como o determinante é uma função alternada das linhas (uma consequência de $\det(A) = \det(A^T)$ [^12] e da propriedade alternada sobre colunas [^12]), temos $\det(A\') = 0$ [^10].
    Agora, calculemos $\det(A\')$ usando a expansão de Laplace ao longo da $j$-ésima linha. Os elementos da $j$-ésima linha de $A\'$ são $a\'_{jk} = a_{ik}$. Os cofatores dos elementos da $j$-ésima linha de $A\'$ são os mesmos que os da $j$-ésima linha de $A$, pois os minors $A\'_{jk}$ são obtidos removendo a linha $j$ (que é a linha $i$ original) e a coluna $k$. No entanto, a definição do minor $A_{jk}$ usado em $c_{ij}$ remove a linha $j$ e a coluna $k$ da *matriz original* $A$. Assim, a expressão para $c_{ij}$ corresponde exatamente à expansão de Laplace de $\det(A\')$ ao longo da sua $j$-ésima linha [^10]:
    $$ \det(A\') = \sum_{k=1}^{n} a\'_{jk} (-1)^{j+k} \det(A\'_{jk}) = \sum_{k=1}^{n} a_{ik} (-1)^{j+k} \det(A_{jk}) = c_{ij} $$
    Como $\det(A\') = 0$, concluímos que $c_{ij} = 0$ para $j \neq i$ [^10].

Combinando os dois casos, temos $C = A\tilde{A} = \det(A)I_n$.

Para provar que $\tilde{A}A = \det(A)I_n$, podemos aplicar o argumento acima à matriz transposta $A^T$. Sabemos que $\det(A^T) = \det(A)$ [^12] e $(\tilde{A})^T = \widetilde{A^T}$ [^11]. Aplicando a primeira parte do resultado a $A^T$:\n$$ A^T \widetilde{A^T} = \det(A^T) I_n = \det(A) I_n $$
Tomando a transposta de ambos os lados:\n$$ (\widetilde{A^T})^T (A^T)^T = (\det(A) I_n)^T $$
$$ (\tilde{A}) A = \det(A) I_n $$
Isso completa a prova [^11]. $\blacksquare$

#### Critério de Invertibilidade e Fórmula da Inversa

A Proposição 7.10 fornece imediatamente um critério para a invertibilidade de $A$ e uma fórmula para sua inversa.

> **Corolário:** Uma matriz $A \in M_n(K)$ (onde $K$ é um anel comutativo) é invertível se, e somente se, seu determinante $\det(A)$ é um elemento invertível no anel $K$ [^13]. Se $\det(A)$ é invertível, então a inversa de $A$ é dada por:
> $$ A^{-1} = (\det(A))^{-1} \tilde{A} $$
> onde $(\det(A))^{-1}$ denota o inverso multiplicativo de $\det(A)$ em $K$ [^14].

*Prova:*\n($\Rightarrow$) Se $A$ é invertível, existe $A^{-1}$ tal que $AA^{-1} = I_n$. Aplicando o determinante e usando a Proposição 7.9 [^2], temos $\det(A)\det(A^{-1}) = \det(I_n) = 1$ [^15]. Como $K$ é um anel comutativo, esta equação mostra que $\det(A)$ possui um inverso multiplicativo em $K$, a saber $\det(A^{-1})$. Logo, $\det(A)$ é invertível em $K$ [^15].

($\Leftarrow$) Se $\det(A)$ é invertível em $K$, seja $(\det(A))^{-1}$ seu inverso. A partir da Proposição 7.10, temos $A\tilde{A} = \det(A)I_n$. Multiplicando ambos os lados por $(\det(A))^{-1}$, obtemos:\n$$ (\det(A))^{-1} (A\tilde{A}) = (\det(A))^{-1} (\det(A)I_n) $$
$$ A ((\det(A))^{-1} \tilde{A}) = I_n $$
Da mesma forma, partindo de $\tilde{A}A = \det(A)I_n$, obtemos $((\det(A))^{-1} \tilde{A}) A = I_n$. Portanto, a matriz $B = (\det(A))^{-1} \tilde{A}$ satisfaz $AB = BA = I_n$, o que significa que $A$ é invertível e $A^{-1} = B = (\det(A))^{-1} \tilde{A}$ [^14]. $\blacksquare$

#### O Caso Especial de Corpos (Fields)

A condição de invertibilidade torna-se particularmente simples quando o anel comutativo $K$ é um corpo (*field*).

> Em um corpo $K$, um elemento $a \in K$ é invertível se, e somente se, $a \neq 0$ [^16].

Aplicando este fato ao corolário anterior, obtemos o critério de invertibilidade bem conhecido para matrizes sobre corpos:

> **Teorema:** Uma matriz $A \in M_n(K)$, onde $K$ é um corpo, é invertível se, e somente se, $\det(A) \neq 0$ [^17].

Este resultado também está conectado à Proposição 7.11 [^18], que afirma que as colunas de $A$ são linearmente dependentes se, e somente se, $\det(A) = 0$. Equivalentemente, $A$ tem posto (*rank*) $n$ (suas colunas são linearmente independentes) se, e somente se, $\det(A) \neq 0$ [^18].

#### Determinante da Matriz Inversa

Como vimos na prova do corolário, se $A$ é invertível, a relação $AA^{-1} = I_n$ implica $\det(A)\det(A^{-1}) = \det(I_n) = 1$ [^19]. Como $A$ é invertível, sabemos que $\det(A)$ é invertível em $K$ (ou $\det(A) \neq 0$ se $K$ for um corpo). Portanto, podemos escrever:\n$$ \det(A^{-1}) = \frac{1}{\det(A)} = (\det(A))^{-1} $$
O determinante da matriz inversa é o inverso multiplicativo do determinante da matriz original [^19].

### Conclusão

Este capítulo estabeleceu rigorosamente a ligação fundamental entre o determinante de uma matriz quadrada $A$ sobre um anel comutativo $K$ e sua invertibilidade. Provamos que $A$ é invertível se, e somente se, $\det(A)$ é um elemento invertível em $K$ [^13]. A matriz adjunta $\tilde{A}$ desempenhou um papel crucial nesta derivação, levando à relação $A\tilde{A} = \tilde{A}A = \det(A)I_n$ [^7] e fornecendo a fórmula explícita $A^{-1} = (\det(A))^{-1} \tilde{A}$ para a inversa [^14]. No caso importante em que $K$ é um corpo, este critério simplifica para a condição $\det(A) \neq 0$ [^17]. Também mostramos que $\det(A^{-1}) = (\det(A))^{-1}$ [^19]. Estes resultados não só aprofundam a compreensão teórica das matrizes e determinantes, mas também têm implicações práticas, embora o cálculo da inversa via adjunta seja geralmente computacionalmente intensivo para matrizes grandes [^17].

### Referências

[^1]: Página 18: "In the next two sections, K is a commutative ring..."
[^2]: Página 17: "Proposition 7.9. For any two n × n-matrices A and B, we have det(AB) = det(A) det(B)."
[^3]: Página 18: "Definition 7.7. Let K be a commutative ring. Given a matrix A ∈ Mn(K), let Ã = (bij) be the matrix defined such that bij = (−1)i+j det(Aji), the cofactor of aji. ... each matrix Aji is called a minor of the matrix A."
[^4]: Página 19: "Note the reversal of the indices in bij = (−1)i+j det(Aji)."
[^5]: Página 19: "Thus, Ã is the transpose of the matrix of cofactors of elements of A."
[^6]: Páginas 18-19: Exemplo de cálculo de Ã para a matriz 3x3 dada.
[^7]: Página 19: "Proposition 7.10. Let K be a commutative ring. For every matrix A ∈ Mn(K), we have AÃ = ÃA = det(A)In."
[^8]: Página 19: "Proof. If Ã = (bij) and AÃ = (cij), we know that the entry cij in row i and column j of AÃ is cij = ai1b1j + ··· + aikbkj + ··· + ainbnj,"
[^9]: Página 19: "If j = i, then we recognize the expression of the expansion of det(A) according to the i-th row: cii = det(A) = ai1(−1)i+1 det(Ai1) + ··· + ain(−1)i+n det(Ain)."
[^10]: Páginas 19-20: "If j ≠ i, we can form the matrix A\' by replacing the j-th row of A by the i-th row of A. ... since A\' has two identical rows i and j, because det is an alternating map of the rows ... we have det(A\') = 0. Thus, we have shown that cii = det(A), and cij = 0, when j ≠ i..."
[^11]: Página 20: "It is also obvious from the definition of Ã, that $\tilde{A}^T = \widetilde{A^T}$. Then applying the first part of the argument to $A^T$, we have $A^T \widetilde{A^T} = \det(A^T)I_n$. ... and since $\det(A^T) = \det(A)$, $\tilde{A}^T = \widetilde{A^T}$, and $(A\tilde{A})^T = \tilde{A}^T A^T$, we get $\det(A)I_n = \tilde{A}^T A^T = \widetilde{A^T}^T A^T = (\tilde{A}A)^T$. that is, $(\tilde{A}A)^T = \det(A)I_n$, which yields $\tilde{A}A = \det(A)I_n$."
[^12]: Página 14: "Corollary 7.7. For every matrix A ∈ Mn(K), we have det(A) = det(AT)." e Página 12: Discussão sobre D ser alternado e multilinear.
[^13]: Página 19: "As a consequence, A is invertible iff det(A) is invertible..."
[^14]: Página 19: "...and if so, A⁻¹ = (det(A))⁻¹Ã."
[^15]: Página 20: "Conversely, if A is invertible, from AA⁻¹ = In, by Proposition 7.9, we have det(A) det(A⁻¹) = 1, and det(A) is invertible."
[^16]: Página 21: "When K is a field, an element a ∈ K is invertible iff a ≠ 0."
[^17]: Página 21: "In this case, the second part of the proposition can be stated as A is invertible iff det(A) ≠ 0."
[^18]: Página 21: "Proposition 7.11. Given an n × n-matrix A over a field K, the columns A¹,..., Aⁿ of A are linearly dependent iff det(A) = det(A¹, ...,Aⁿ) = 0. Equivalently, A has rank n iff det(A) ≠ 0."
[^19]: Página 20: "...from AA⁻¹ = In, by Proposition 7.9, we have det(A) det(A⁻¹) = 1..."

<!-- END -->