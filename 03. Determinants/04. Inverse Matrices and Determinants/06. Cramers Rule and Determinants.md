## Capítulo 7.5.1: Resolução de Sistemas Lineares via Regra de Cramer

### Introdução

Neste capítulo, exploramos as propriedades e aplicações dos determinantes [^1]. Tendo estabelecido a definição, propriedades fundamentais (Seção 7.3) e a relação entre determinantes e a invertibilidade de matrizes através da matriz adjunta (Seção 7.4) [^2, ^4], abordamos agora uma aplicação direta e teoricamente importante: a resolução de sistemas de equações lineares da forma $Ax=b$ utilizando a **Regra de Cramer** [^1, ^8]. Esta regra, como veremos, fornece fórmulas explícitas para os componentes do vetor solução $x$ em termos de determinantes, sob a condição crucial de que o sistema possua uma solução única [^13]. Para esta análise, assumiremos que operamos sobre um corpo $K$ (como $\mathbb{R}$ ou $\mathbb{C}$) [^9], conforme estabelecido na Seção 7.5.

### Condição para Solução Única e a Regra de Cramer

Consideremos um sistema de $n$ equações lineares com $n$ incógnitas, representado na forma matricial $Ax = b$, onde $A \in M_n(K)$ é uma matriz quadrada de ordem $n$ com colunas $A^1, \dots, A^n$, $x$ é o vetor coluna das variáveis $(x_1, \dots, x_n)^T$, e $b$ é um vetor coluna de constantes $(b_1, \dots, b_n)^T$ [^10]. A relação fundamental entre o sistema e o determinante de $A$ é estabelecida na Proposição 7.11, que afirma que as colunas de $A$ são linearmente dependentes se e somente se $\det(A) = 0$ [^11].

Expandindo essa ideia, a Proposição 7.12(1) fornece a condição necessária e suficiente para a existência de uma solução única para o sistema $Ax=b$ para *qualquer* vetor $b$:\

> **Proposição 7.12(1) (Reformulada):** Para qualquer vetor coluna $b$, existe um único vetor coluna $x$ tal que $Ax = b$ se, e somente se, a única solução para $Ax = 0$ é o vetor trivial $x = 0$, o que é equivalente a $\det(A) \neq 0$ [^13, ^17].

Quando esta condição, $\det(A) \neq 0$, é satisfeita, a matriz $A$ é invertível [^5, ^7], garantindo uma única solução $x = A^{-1}b$. A Regra de Cramer oferece um método alternativo para encontrar cada componente $x_j$ dessa solução única diretamente, utilizando determinantes.

**A Regra de Cramer** é formalmente apresentada na Proposição 7.12(2). Definimos $A_j(b)$ como a matriz obtida ao substituir a $j$-ésima coluna de $A$, denotada por $A^j$, pelo vetor coluna $b$. A regra afirma que [^14]:

> **Proposição 7.12(2) (Regra de Cramer):** Se $\det(A) \neq 0$, a única solução do sistema $Ax = b$ é dada pelas expressões:
> $$ x_j = \frac{\det(A^1, \dots, A^{j-1}, b, A^{j+1}, \dots, A^n)}{\det(A^1, \dots, A^{j-1}, A^j, A^{j+1}, \dots, A^n)} = \frac{\det(A_j(b))}{\det(A)} $$
> para $j = 1, \dots, n$. Estas são conhecidas como as **regras de Cramer** (Cramer\'s rules) [^14].

**Demonstração da Regra de Cramer (Baseada na Prova da Proposição 7.12(2) [^18]):**

Assumimos que $Ax=b$ e $\det(A) \neq 0$. A equação matricial $Ax=b$ é equivalente à combinação linear das colunas de $A$ [^10]:
$$ x_1A^1 + x_2A^2 + \dots + x_jA^j + \dots + x_nA^n = b $$
Consideremos o determinante da matriz $A_j(b)$, onde a $j$-ésima coluna $A^j$ foi substituída por $b$:
$$ \det(A_j(b)) = \det(A^1, \dots, A^{j-1}, b, A^{j+1}, \dots, A^n) $$
Substituindo $b$ pela sua expressão como combinação linear das colunas de $A$:
$$ \det(A_j(b)) = \det(A^1, \dots, A^{j-1}, \sum_{k=1}^n x_k A^k, A^{j+1}, \dots, A^n) $$
Utilizamos agora a propriedade de **multilinearidade** do determinante (Lema 7.5 [^page 216]) em relação à $j$-ésima coluna:
$$ \det(A_j(b)) = \sum_{k=1}^n x_k \det(A^1, \dots, A^{j-1}, A^k, A^{j+1}, \dots, A^n) $$
O determinante $\det(A^1, \dots, A^{j-1}, A^k, A^{j+1}, \dots, A^n)$ possui uma propriedade crucial devido à natureza **alternada** do determinante (Proposição 7.3(2) [^page 210] e Lema 7.5 [^page 216]): se $k \neq j$, a matriz dentro do determinante terá duas colunas idênticas (a coluna $k$ original e a coluna $k$ na posição $j$), fazendo com que o determinante seja zero. Portanto, o único termo não nulo na soma ocorre quando $k=j$. Assim, a expressão simplifica-se para:
$$ \det(A_j(b)) = x_j \det(A^1, \dots, A^{j-1}, A^j, A^{j+1}, \dots, A^n) = x_j \det(A) $$
Como partimos da hipótese de que $\det(A) \neq 0$, podemos dividir ambos os lados por $\det(A)$, obtendo a fórmula explícita para $x_j$:
$$ x_j = \frac{\det(A_j(b))}{\det(A)} $$
Isso completa a derivação da Regra de Cramer [^18]. $\blacksquare$

É relevante notar que a Proposição 7.12(3) também decorre diretamente da Proposição 7.11: o sistema homogêneo $Ax=0$ tem uma solução não trivial (não nula) se, e somente se, as colunas de $A$ são linearmente dependentes, o que ocorre se, e somente se, $\det(A) = 0$ [^15].

**Relação com a Matriz Inversa e Adjunta:**

A Regra de Cramer está intrinsecamente ligada à fórmula da matriz inversa obtida através da matriz adjunta, $\tilde{A}$. Como visto na Proposição 7.10, se $\det(A)$ é invertível (no corpo $K$, isso significa $\det(A) \neq 0$), então $A^{-1} = (\det(A))^{-1}\tilde{A}$ [^5, ^7]. A solução única $x$ de $Ax=b$ é $x = A^{-1}b = (\det(A))^{-1}\tilde{A}b$.
Analisando o $j$-ésimo componente desta solução:
$$ x_j = \frac{1}{\det(A)} (\tilde{A}b)_j $$
O termo $(\tilde{A}b)_j$ é o produto da $j$-ésima linha de $\tilde{A}$ pelo vetor $b$. Recordemos que $\tilde{A}$ é a transposta da matriz de cofatores de $A$, ou seja, $(\tilde{A})_{jk} = C_{kj} = (-1)^{k+j}\det(A_{kj})$ [^3, ^page 223]. Portanto,
$$ (\tilde{A}b)_j = \sum_{k=1}^n (\tilde{A})_{jk} b_k = \sum_{k=1}^n (-1)^{k+j} \det(A_{kj}) b_k $$
Esta soma corresponde exatamente à **expansão de Laplace** do determinante da matriz $A_j(b)$ ao longo da sua $j$-ésima coluna (cujos elementos são $b_1, \dots, b_n$ e os respectivos menores são $A_{k j}$). Assim, $(\tilde{A}b)_j = \det(A_j(b))$, o que nos leva de volta à fórmula de Cramer $x_j = \det(A_j(b)) / \det(A)$. Isso demonstra a consistência entre a Regra de Cramer e a fórmula da inversa via adjunta.

**Considerações Práticas e Teóricas:**

Embora a Regra de Cramer forneça uma fórmula explícita e elegante para a solução, *ela é geralmente impraticável para cálculos numéricos*, especialmente para matrizes de grande dimensão ($n$ elevado) [^19]. O cálculo de $n+1$ determinantes de ordem $n$ é computacionalmente muito mais custoso do que métodos como a eliminação Gaussiana.
No entanto, a Regra de Cramer possui um valor teórico considerável. Por exemplo, como os determinantes são funções polinomiais das entradas da matriz, as fórmulas de Cramer mostram que cada componente $x_j$ da solução é uma função racional das entradas de $A$ e $b$. Isso implica que a solução $x$ varia continuamente com $A$ e $b$, desde que $\det(A)$ permaneça diferente de zero. Se as entradas de $A$ e $b$ forem funções diferenciáveis de um parâmetro $t$, então $x_j(t)$ também será diferenciável (onde $\det(A(t)) \neq 0$) [^19].

### Conclusão

A Regra de Cramer, apresentada e demonstrada nesta seção com base nas Proposições 7.11 e 7.12 [^11, ^12, ^18], constitui uma ferramenta teórica fundamental no estudo dos sistemas lineares $Ax=b$ sobre um corpo $K$ [^9]. Ela estabelece uma ligação direta entre a solução do sistema e os determinantes da matriz de coeficientes $A$ e das matrizes modificadas $A_j(b)$, sob a condição essencial de que $\det(A) \neq 0$ [^13, ^14]. Embora seu uso prático seja limitado pela complexidade computacional [^19], a Regra de Cramer oferece insights valiosos sobre a estrutura da solução e sua dependência contínua ou diferenciável das entradas do sistema, reforçando a importância dos determinantes na álgebra linear [^1, ^20].

### Referências
[^1]: Chapter 7 Introduction (p. 205): "It is shown how determinants can be used [...] to solve (at least in theory!) systems of linear equations (the Cramer formulae)."
[^2]: Section 7.4 Title (p. 222): "Inverse Matrices and Determinants"
[^3]: Definition 7.7 (p. 222): Definition of the adjugate matrix $\tilde{A} = (b_{ij})$ where $b_{ij} = (-1)^{i+j}\det(A_{ji})$.
[^4]: Proposition 7.10 (p. 223): "For every matrix $A \in M_n(K)$, we have $A\tilde{A} = \tilde{A}A = \det(A)I_n$."
[^5]: Proposition 7.10 Consequence (p. 223): "As a consequence, A is invertible iff det(A) is invertible, and if so, $A^{-1} = (\det(A))^{-1}\tilde{A}$."
[^6]: Proof Conclusion of Prop 7.10 (p. 224): "This proves that $A\tilde{A} = \tilde{A}A = \det(A)I_n$."
[^7]: Consequence after Prop 7.10 proof (p. 224): "As a consequence, if det(A) is invertible, we have $A^{-1} = (\det(A))^{-1}\tilde{A}$."
[^8]: Section 7.5 Title (p. 225): "Systems of Linear Equations and Determinants"
[^9]: Assumption in Section 7.5 (p. 225): "Therefore, we assume again that K is a field (usually, K = R or K = C)."
[^10]: System Representation (p. 225): Representation of $Ax=b$ and its equivalence to $x_1A^1 + \dots + x_nA^n = b$.
[^11]: Proposition 7.11 (p. 225): "Given an n x n-matrix A over a field K, the columns $A^1, ..., A^n$ of A are linearly dependent iff $\det(A) = \det(A^1, ..., A^n) = 0$. Equivalently, A has rank n iff $\det(A) \neq 0$."
[^12]: Proposition 7.12 (p. 226): Statement of properties including Cramer\'s rules.
[^13]: Proposition 7.12(1) (p. 226): "For every column vector b, there is a unique column vector x such that Ax = b iff the only solution to Ax = 0 is the trivial vector x = 0, iff $\det(A) \neq 0$."
[^14]: Proposition 7.12(2) (p. 226): "If $\det(A) \neq 0$, the unique solution of Ax = b is given by the expressions $x_j = \det(A^1, ..., A^{j-1}, b, A^{j+1}, ..., A^n) / \det(A^1, ..., A^j, ..., A^n)$, known as Cramer\'s rules."
[^15]: Proposition 7.12(3) (p. 226): "The system of linear equations Ax = 0 has a nonzero solution iff $\det(A) = 0$."
[^16]: Proof of Proposition 7.12 (p. 227).
[^17]: Proof of Prop 7.12(1) (p. 227): Links unique solution, trivial solution for Ax=0, linear independence of columns, and $\det(A) \neq 0$.
[^18]: Proof of Prop 7.12(2) (p. 227): Derivation using $b = \sum x_k A^k$ and multilinearity.
[^19]: Remark on Cramer\'s rules (p. 227): "As pleasing as Cramer\'s rules are, it is usually impractical to solve systems of linear equations using the above expressions. However, these formula imply an interesting fact, which is that the solution of the system Ax = b are continuous in A and b... are ratios of polynomials, and thus are also continuous as long as det(A(t)) is nonzero. Similarly, if the functions aij(t) and bj(t) are differentiable, so are the xj(t)."
[^20]: Chapter 7 Summary (p. 236): Bullet point "Solving linear equations using Cramer\'s rules."
[^page 210]: Proposition 7.3(2): $f(..., x_i, ..., x_j, ...) = 0$ if $x_i = x_j$.
[^page 216]: Lemma 7.5: $D \in D_n$ is an alternating multilinear map.
[^page 223]: Remark on Adjugate Indices (p. 223): "Note the reversal of the indices in $b_{ij} = (-1)^{i+j}\det(A_{ji})$."

<!-- END -->