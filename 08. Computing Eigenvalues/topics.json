{
  "topics": [
    {
      "topic": "Computing Eigenvalues and Eigenvectors",
      "sub_topics": [
        "Eigenvalues and eigenvectors are fundamental in numerical linear algebra for solving linear systems and understanding matrix properties. Various methods, including Jacobi, Givens-Householder, QR iteration, and Rayleigh-Ritz, are tailored for specific matrix types like symmetric matrices. A common approach involves constructing a sequence of matrices (Ak) such that Ak = PAPk converges to a matrix with easily determinable eigenvalues, often achieved by making Ak upper triangular through orthogonal matrices.",
        "The Schur form (A = UTU*) theoretically provides eigenvalues as diagonal entries of the upper triangular matrix T, where U is unitary. However, finding it practically requires unstable polynomial root-finding methods. Consequently, this approach is numerically unstable due to its reliance on finding polynomial roots.",
        "The QR iteration algorithm, developed by Rutishauser, Francis, and Kublanovskaya, is efficient for computing eigenvalues of general (non-symmetric) matrices. It generates a sequence of matrices (Ak) through QR decomposition (Ak = QkRk) and recombination (Ak+1 = RkQk), preserving eigenvalues across iterations (Ak+1 = Qk*AkQk). The basic QR algorithm iteratively computes Ak from QR factorization Ak = QkRk, setting Ak+1 = RkQk, aiming for Ak to converge to an upper triangular matrix with eigenvalues on the diagonal. Convergence is guaranteed only under restrictive conditions: the input matrix A must be diagonalizable with nonzero eigenvalues of distinct moduli; the part of Ak below the diagonal converges to zero, and diagonal entries converge to A's eigenvalues. However, the basic QR algorithm faces challenges with matrices having eigenvalues of the same modulus, leading to potential looping or non-convergence to an upper triangular matrix; modifications exist to address these issues. The convergence of the QR algorithm is guaranteed under specific conditions, such as the matrix being invertible, diagonalizable, and having distinct eigenvalue moduli, with the strictly lower-triangular part converging to zero.",
        "Hessenberg matrices, almost triangular, simplify eigenvalue computations; unreduced Hessenberg matrices (hi+1,i \u2260 0) have distinct eigenvalues. Symmetric/Hermitian positive definite tridiagonal matrices are a special case where the QR algorithm converges to a diagonal matrix. Converting the original matrix to a similar Hessenberg matrix, which is almost triangular, is advantageous because it enhances the convergence rate. These matrices can be constructed as U*AU, where U is a unitary matrix; applying the QR algorithm to a Hessenberg matrix H is more efficient, and the resulting matrices Hk remain upper Hessenberg.",
        "Shifting improves QR method efficiency by QR-factoring (Ak - \u03c3kI = QkRk) and updating (Ak+1 = RkQk + \u03c3kI), where \u03c3k approximates an eigenvalue, accelerating convergence. Double shifts handle complex conjugate eigenvalue pairs in real arithmetic. Shift techniques enhance the QR algorithm\u2019s efficiency by introducing a shift \u03c3 close to an eigenvalue of A, forming Ak - \u03c3kI = QkRk and Ak+1 = RkQk + \u03c3kI. The QR algorithm can be modified using shifts to accelerate convergence, especially for real matrices with complex conjugate eigenvalues, by performing implicit shifts that avoid explicit QR factorization.",
        "For large matrices, Arnoldi/Lanczos iterations reduce to Hessenberg form, computing Ritz values (eigenvalues of the Hessenberg submatrix) as eigenvalue approximations. Arnoldi iteration also provides an orthonormal basis for the Krylov subspace. Arnoldi iteration provides approximations of eigenvalues by reducing the matrix to a Hessenberg form, with Ritz values computed from the Hessenberg submatrix serving as eigenvalue estimates. The Rayleigh-Ritz method then computes eigenvalues of the Hessenberg submatrix using the QR algorithm with shifts, yielding approximations called Ritz values.",
        "The Implicit Q Theorem provides a theoretical foundation for the QR algorithm, stating that if A is reduced to an unreduced upper Hessenberg form A = UHU*, then the columns of U are uniquely determined up to sign by the first column of U.",
        "The QR algorithm can be modified to handle singular matrices by factoring A = QR, where Q is a product of Householder reflections and R is upper triangular, and if H is an unreduced upper Hessenberg matrix, the algorithm can identify and deflate zero eigenvalues."
      ]
    },
    {
      "topic": "Hessenberg Matrices",
      "sub_topics": [
        "A Hessenberg matrix is an n\u00d7n matrix that is almost triangular, except that it may have an extra nonzero diagonal below the main diagonal (hjk = 0 for all (j, k) such that j - k > 1). This structure simplifies eigenvalue computations.",
        "Every n \u00d7 n complex or real matrix A is similar to an upper Hessenberg matrix H (A = UHU* for some unitary matrix U) where U can be constructed as a product of Householder matrices. Any matrix can be transformed into a similar upper Hessenberg matrix using unitary transformations, often constructed as a product of Householder matrices.",
        "An upper Hessenberg n \u00d7 n matrix H is unreduced if hi+1i \u2260 0 for i = 1, ..., n - 1; a Hessenberg matrix which is not unreduced is said to be reduced. Unreduced Hessenberg matrices have geometrically simple eigenvalues (dim(Ex) = 1). If an upper Hessenberg matrix H has a subdiagonal entry Hp+1p = 0, it decomposes into block upper Hessenberg matrices, simplifying eigenvalue determination as the eigenvalues of H are the eigenvalues of the diagonal blocks.",
        "If H is an unreduced Hessenberg matrix, then every eigenvalue of H is geometrically simple, that is, dim(Ex) = 1 for every eigenvalue X, where Ex is the eigenspace associated with X; if H is diagonalizable, then every eigenvalue is simple, that is, H has n distinct eigenvalues.",
        "If a matrix A is Hermitian (or symmetric in the real case), then it is easy to show that the Hessenberg matrix similar to A is a Hermitian (or symmetric in real case) tridiagonal matrix and the conversion method is more efficient. If a Hessenberg matrix is Hermitian (or symmetric in the real case), the similar tridiagonal matrix is also Hermitian (or symmetric), and the QR algorithm converges to a diagonal matrix of eigenvalues when the matrix is positive definite.",
        "The upper Hessenberg form can handle singular matrices; if H is an unreduced upper Hessenberg matrix, it is singular if and only if rnn = 0 in its QR factorization, allowing deflation by considering a smaller Hessenberg matrix.",
        "The QR method's efficiency is improved by initially reducing the matrix to upper Hessenberg form, applying shifts to accelerate convergence, and performing implicit shifts to avoid explicit QR factorization."
      ]
    },
    {
      "topic": "Krylov Subspaces; Arnoldi Iteration",
      "sub_topics": [
        "Krylov subspaces, defined as Span(b, Ab, ..., An-1b), are fundamental for approximating eigenvalues and eigenvectors of large matrices, where A is a matrix and b is an arbitrary vector. The Krylov subspace Kn(A, b) is the span of vectors (b, Ab, ..., A^(n-1)b), and Arnoldi's algorithm constructs an orthonormal basis for it, acting as a Gram-Schmidt procedure.",
        "Arnoldi iteration constructs an orthonormal basis for the Krylov subspace Kn(A, b), producing a sequence of orthonormal vectors (u1, u2, ..., un) such that Span(u1, u2, ..., un) = Kn(A, b). The Arnoldi iteration algorithm constructs an orthonormal basis for the Krylov subspace, which is the span of vectors obtained by repeated application of the matrix to an initial vector. The space Kn(A, b) = Span(b, Ab, ..., An-1b) is a Krylov subspace. Arnoldi's algorithm constructs an orthonormal basis for Kn(A, b), akin to a Gram-Schmidt procedure. Arnoldi iteration constructs an orthonormal basis for the Krylov subspace Kn(A, b) = Span(b, Ab, ..., A^(n-1)b), which is a subspace spanned by a vector b and its successive matrix multiplications by A.",
        "The Arnoldi iteration algorithm iteratively computes Un+1 and Hn, where AUn = Un+1Hn, allowing for the approximation of eigenvalues of A by computing the eigenvalues of the smaller Hessenberg matrix Hn. The Arnoldi algorithm iteratively computes Un+1 and Hn, where Hn is the n \u00d7 n upper Hessenberg matrix, using the equations Aun = h1nU1 + ... + hnnUn + hn+1,nUn+1, derived from AUn = Un+1Hn. Arnoldi iteration produces upper Hessenberg matrices Hn that are projections of A onto the Krylov space Kn(A, b), relating successive iterates through the formula AUn = Un+1Hn.",
        "The Rayleigh-Ritz method approximates eigenvalues of A by running Arnoldi iteration and computing the eigenvalues of the resulting Hessenberg matrix Hn, known as Ritz values, which tend to approximate the extreme eigenvalues of A. The Rayleigh-Ritz method runs Arnoldi iteration and then the QR algorithm with shifts on Hn; eigenvalues of Hn (Ritz values) approximate eigenvalues of A, particularly the extreme eigenvalues, while eigenvectors of Hn approximate eigenvectors of A. The Rayleigh-Ritz method uses Arnoldi iteration and QR algorithm with shifts on Hn to approximate eigenvalues of A, considering eigenvalues of Hn as Arnoldi estimates or Ritz values. The Rayleigh-Ritz method involves running Arnoldi iteration and then the QR algorithm with shifts on Hn, where the eigenvalues of Hn, known as Ritz values, approximate the eigenvalues of A, particularly the extreme eigenvalues.",
        "GMRES (Generalized Minimal Residual method) uses Krylov subspaces to find approximate solutions to linear systems Ax = b, minimizing the residual rn = b - Axn over the Krylov subspace Kn(A, b).",
        "Lanczos iteration, a specialized version of Arnoldi for symmetric or Hermitian matrices, simplifies the computation by producing a tridiagonal matrix, making it more efficient and numerically stable.",
        "For an m \u00d7 m matrix A reduced to upper Hessenberg form H (A = UHU*), consider the (n + 1) \u00d7 n upper left block Hn for n < m; the first n columns of UH = AU can be expressed as AUn = Un+1Hn.",
        "The Arnoldi iteration algorithm iteratively computes Un+1 and Hn without using Householder matrices: given a nonzero vector b \u2208 Cm, the algorithm produces an orthonormal basis for the Krylov subspace.",
        "If Arnoldi iteration breaks down at stage n (hn+1n = 0), the eigenvalues of Hn are eigenvalues of A, allowing a new starting vector Un+1 orthogonal to previous vectors to continue the process. Arnoldi iteration can break down if hn+1,n = 0, which can be interpreted as finding an unreduced block of the Hessenberg matrix H, where the eigenvalues of Hn are also eigenvalues of A.",
        "If Arnoldi iteration runs without breakdown, then Kn = UnRn for some n \u00d7 n upper triangular matrix Rn (reduced QR factorization) and Hn = U*AUn (projection of A onto the Krylov space).",
        "Arnoldi iteration solves an optimization problem by finding a polynomial p(z) of degree n that minimizes ||p(A)b||, where p is the characteristic polynomial det(zI \u2013 Hn) of Hn. Arnoldi iteration solves an optimization problem: given a matrix A and a vector b, it finds a polynomial p(z) of degree n such that ||p(A)b||2 is minimized, where p(z) is the characteristic polynomial det(zI - Hn) of Hn.",
        "The Krylov matrix Kn is related to the orthonormal vectors produced by Arnoldi iteration through a reduced QR factorization Kn = UnRn, where Rn is an n \u00d7 n upper triangular matrix."
      ]
    },
    {
      "topic": "Power Methods",
      "sub_topics": [
        "Power iteration is an iterative method for finding the eigenvalue with the largest modulus (dominant eigenvalue) and its corresponding eigenvector of a matrix A. Power iteration is a method that yields one eigenvalue and one eigenvector associated with this vector. It only works if the matrix A has an eigenvalue \u03bb of largest modulus. Power iteration and inverse power iteration are methods for finding one eigenvalue and one eigenvector of a matrix, suitable for certain conditions. Power iteration requires the matrix A to have an eigenvalue \u03bb of largest modulus, yielding \u03bb and an associated eigenvector through iterative application of the matrix. Power iteration is a method to find one eigenvalue and its associated eigenvector for an m \u00d7 m complex or real matrix, which converges when the matrix A has an eigenvalue \u03bb of largest modulus, i.e., |\u03bb1| > |\u03bb2| \u2265 ... \u2265 |\u03bbn| \u2265 0.",
        "The basic power iteration algorithm involves repeatedly multiplying a vector by the matrix A and normalizing to converge to the dominant eigenvector, assuming the initial vector has a nonzero component in the dominant eigenvector's direction. The power iteration method involves iteratively computing xk+1 = Axk / ||Axk||, expecting (xk) to converge to an eigenvector associated with \u03bb1. The power iteration algorithm involves picking an initial unit vector x0 and computing the sequence xk+1 = Axk / ||Axk||, where the sequence (xk) converges to an eigenvector associated with \u03bb1.",
        "The eigenvalue \u03bb1 is found as limk\u2192\u221e (Axk)j / (xk)j if \u03bb1 is complex, or as limk\u2192\u221e (xk+1)*Axk+1 if \u03bb1 is real, under certain conditions. If \u03bb1 is complex and vj is a nonzero coordinate of the eigenvector v, then \u03bb1 = lim (Axk)j / xkj as k approaches infinity; if \u03bb1 is real, then \u03bb1 = lim (x(k)) as k approaches infinity, where x(k+1) = (xk+1)*Axk+1.",
        "Inverse iteration finds an eigenvector associated with a known approximate eigenvalue \u03bb by iteratively solving (A - \u03bcI)wk+1 = xk and normalizing, where \u03bc is a good approximation of \u03bb, allowing for convergence to the eigenvector associated with \u03bb. Inverse iteration finds an eigenvector associated with an eigenvalue \u03bb of A, given a good approximation \u03bc, by iteratively solving (A \u2013 \u03bcI)wk+1 = xk and normalizing. The inverse iteration method is designed to find an eigenvector associated with an eigenvalue A of A for which we know a good approximation \u03bc. It can be modified to make use of the newly computed \u03bb(k+1) instead of \u03bc, and an even faster convergence is achieved. Inverse iteration finds an eigenvector associated with an eigenvalue \u03bb for which a good approximation \u03bc is known; it involves computing sequences wk+1 = (A - \u03bcI)^(-1)xk and xk+1 = wk+1 / ||wk+1||.",
        "Rayleigh quotient iteration improves inverse iteration by using the Rayleigh quotient (xk+1*Axk+1) as an updated eigenvalue approximation, leading to cubic convergence under certain conditions, significantly accelerating the process. Rayleigh quotient iteration. When it converges (which is for almost all x\u2070), this method eventually achieves cubic convergence. Rayleigh quotient iteration is a modified inverse iteration for Hermitian matrices that uses the newly computed (k+1) instead of \u03bc, achieving cubic convergence.",
        "The inverse iteration method converges when \u03bc \u2260 \u03bb and |\u03bc - \u03bb| < |\u03bc - \u03bbj| for all j \u2260 l, where x0 does not belong to the subspace spanned by eigenvectors associated with eigenvalues \u03bbj with j \u2260 l; the sequence xk converges to an eigenvector associated with \u03bb."
      ]
    },
    {
      "topic": "Making the QR Method More Efficient Using Shifts",
      "sub_topics": [
        "Efficiency in QR algorithms is improved by initially reducing the matrix A to upper Hessenberg form (A = UHU*) and applying the QR algorithm to its unreduced Hessenberg blocks.",
        "Shifts are used to accelerate convergence, and double shifts handle pairs of complex conjugate eigenvalues in real matrices. Shifts are used to accelerate convergence and handle complex conjugate eigenvalues by performing implicit shifts that compute Ak+1 = Q*AkQ without explicit QR factorization.",
        "Instead of explicit QR-factorization, implicit shifts compute Ak+1 = QAkQk without QR-factorizing (Ak \u2013 \u03c3kI), a technique known as bulge chasing. The implicit shift technique computes Ak+1 = Q*AkQk without explicitly computing a QR factorization of Ak - \u03c3kI, referred to as bulge chasing, which is particularly useful for real matrices; Watkins discusses bulge chasing in complex matrices. Implicit shifting is based on the implicit Q theorem, generating Ak+1 = Q*AkQ without computing QR factorizations, using Givens rotations to perform a bulge chasing process.",
        "Deflation occurs when a subdiagonal entry (Hk)p+1p is zero or very small, allowing recursive application of the QR algorithm to smaller submatrices. If a subdiagonal entry (Hk)p+1p is zero or very small, the matrix Hk is of a block form, allowing recursive application of the QR algorithm to the smaller blocks. If a subdiagonal entry (Hk)p+1,p is zero or very small during the QR algorithm, the matrix Hk can be split into smaller Hessenberg matrices H11 and H22, allowing recursive application of the QR algorithm to each submatrix.",
        "Shifts are designed to create the above situations. Under the hypotheses of Theorem 18.1, it can be shown that the entry (Ak)ij with i > j converges to 0 as 1/|\u03bbi/\u03bbj|^k converges to 0.",
        "The Francis shift picks \u03c3\u03b5 and \u2126k as the complex conjugate eigenvalues of the corner block. In addition, there are matrices for which neither a shift by (Hk)nn nor the Francis shift works.",
        "A shift involves picking a value \u03c3k close to an eigenvalue \u03bbi and QR-factoring Ak \u2013 \u03c3kI, then forming Ak+1 = RkQk + \u03c3kI, which accelerates convergence. A shift is the process of picking some \u03c3k close to an eigenvalue of A (in general, \u03bbn), QR-factoring Ak - \u03c3kI = QkRk, and then forming Ak+1 = RkQk + \u03c3kI, such that Ak+1 = Q*AkQk.",
        "Wilkinson shifts are used for symmetric matrices, picking the eigenvalue of a lower corner block to accelerate convergence, while double shifts handle pairs of complex conjugate eigenvalues. Wilkinson shifts, specific to symmetric matrices, improve the QR algorithm's progress by choosing a shift based on the lower corner of Ak, which can lead to quadratic convergence.",
        "Double shifts are used for real matrices with complex eigenvalues, where for a complex number \u03c3k, Ak - \u03c3kI = QkRk, Ak+1 = RkQk + \u03c3kI, Ak+1 - \u03c3kI = Qk+1Rk+1, and Ak+2 = Rk+1Qk+1 + \u03c3kI; this keeps the products QkQk+1 real, avoiding complex arithmetic."
      ]
    },
    {
      "topic": "GMRES",
      "sub_topics": [
        "GMRES (Generalized Minimal Residual) method finds approximate solutions to linear systems Ax = b by minimizing the residual rn = b - Axn over Krylov subspaces. GMRES (Generalized Minimal Residual) is an iterative method for solving linear systems Ax = b, where A is an invertible m \u00d7 m matrix, by finding an approximate solution xn in the Krylov subspace Kn(A, b) that minimizes the residual ||b - Axn||2.",
        "The GMRES method searches for a solution xn in a Krylov space of dimension m \u2264 s, where s is the minimal degree of a polynomial p(z) such that p(A)b = 0.",
        "The minimization problem in GMRES is to minimize ||Axn - b||2 over xn \u2208 Kn(A, b), which can be restated as minimizing ||Hny - ||b||2e1||2, where Hn is the upper Hessenberg matrix and y \u2208 C^n. The minimization problem in GMRES is to minimize ||AUny - b||2, where xn = Uny and Un is the matrix whose columns form a basis for Kn(A, b); this is equivalent to minimizing ||Hny - ||b||2e1||2, where e1 is the first unit vector.",
        "The approximate solution of Ax = b in GMRES is given by xn = Uny, where Un is the matrix of orthonormal vectors produced by Arnoldi iteration and y is the solution to the least squares problem.",
        "The GMRES method involves running n \u2264 s Arnoldi iterations to find Un and Hn, followed by solving a least squares problem to determine the coefficients for the approximate solution. The GMRES method involves running n \u2264 s Arnoldi iterations to find Un and Hn, and then solving the least squares problem minimize ||Hny - ||b||2e1||2 to find y, leading to the approximate solution xn = Uny.",
        "GMRES searches for xn \u2208 Kn(A, b) such that ||rn||2 = ||Axn - b||2 is minimized, where the residual rn is b - Axn, effectively solving a least-squares problem."
      ]
    },
    {
      "topic": "The Hermitian Case; Lanczos Iteration",
      "sub_topics": [
        "Lanczos iteration simplifies Arnoldi's method for symmetric or Hermitian matrices, producing symmetric tridiagonal matrices and real eigenvalues, enhancing computational efficiency. For a symmetric or Hermitian matrix A, Arnoldi's method simplifies to Lanczos iteration, where the upper Hessenberg matrices Hn are symmetric or Hermitian tridiagonal, making the algorithm more efficient.",
        "In Lanczos iteration, the recurrence relation becomes a three-term recurrence: Aun = \u03b2n-1un-1 + \u03b1nun + \u03b2nun+1, simplifying computations. Lanczos iteration is characterized by a three-term recurrence: Aun = \u03b2n-1un-1 + \u03b1nun + \u03b2nun+1, where \u03b1n = u*n A un and \u03b2n = ||z||, with z being the vector orthogonalized against un-1 and un.",
        "The Lanczos algorithm involves computing \u03b1n = u*n A un and \u03b2n = ||z||, where z = Aun - \u03b2n-1un-1 - \u03b1nun, eliminating the inner loop from 1 to n. The Lanczos algorithm involves iterating z := Aun, \u03b1n := u*nz, z := z - \u03b2n-1un-1 - \u03b1nun, and \u03b2n := ||z||, where the process breaks down if \u03b2n = 0.",
        "The Rayleigh-Ritz method applied to Lanczos iteration uses eigenvalues of the symmetric tridiagonal matrix Hn to find some eigenvalues of A. Lanczos iteration provides the advantage that the Rayleigh-Ritz method, used to find some eigenvalues of A, applies to the symmetric or Hermitian tridiagonal matrix Hn, and there are more methods for finding eigenvalues of such matrices.",
        "Lanczos iteration may encounter issues with floating-point arithmetic, leading to loss of orthogonality among vectors, which may require reorthogonalization. A potential issue with Lanczos iteration in floating-point arithmetic is the loss of orthogonality among the vectors uj, which may require reorthogonalization.",
        "The version of GMRES using Lanczos iteration is called MINRES, which is more efficient for Hermitian matrices."
      ]
    }
  ]
}