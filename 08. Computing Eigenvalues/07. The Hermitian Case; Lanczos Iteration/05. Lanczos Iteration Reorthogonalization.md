## Efeitos da Aritmética de Ponto Flutuante e Reortogonalização na Iteração de Lanczos

### Introdução
A iteração de Lanczos, uma ferramenta poderosa para encontrar autovalores e autovetores de matrizes simétricas (ou Hermitianas), pode encontrar problemas quando implementada usando aritmética de ponto flutuante [^670]. Esses problemas podem levar à perda de ortogonalidade entre os vetores gerados, exigindo técnicas de reortogonalização para manter a precisão dos resultados [^670]. Este capítulo explora em profundidade esses desafios e as estratégias para mitigá-los.

### Conceitos Fundamentais
A iteração de Lanczos é uma forma especializada do algoritmo de Arnoldi, adaptada para matrizes simétricas ou Hermitianas [^647, 669]. Em vez de gerar uma matriz de Hessenberg superior geral, a iteração de Lanczos produz uma matriz tridiagonal, o que simplifica significativamente os cálculos [^669]. O algoritmo básico da iteração de Lanczos pode ser resumido da seguinte forma [^669]:

1.  Comece com um vetor arbitrário não nulo $b \in \mathbb{C}^m$ e normalize-o: $u_1 = \frac{b}{||b||}$ [^669].
2.  Para $n = 1, 2, 3, ...$ [^669]:
    *   Calcule $z = Au_n$ [^669].
    *   Calcule $\alpha_n = u_n^* z$ [^669].
    *   Atualize $z = z - \beta_{n-1} u_{n-1} - \alpha_n u_n$, onde $\beta_0 u_0 = 0$ [^669].
    *   Calcule $\beta_n = ||z||$ [^669].
    *   Se $\beta_n = 0$, interrompa a iteração [^669].
    *   Calcule $u_{n+1} = \frac{z}{\beta_n}$ [^669].

Idealmente, os vetores $u_1, u_2, u_3, ...$ formam uma base ortogonal para o subespaço de Krylov [^665, 666]. No entanto, em aritmética de ponto flutuante, essa ortogonalidade pode ser perdida devido a erros de arredondamento [^670].

**Perda de Ortogonalidade:**
Em teoria, os vetores $u_j$ gerados pela iteração de Lanczos devem ser ortogonais [^665, 666]. No entanto, devido a erros de arredondamento na aritmética de ponto flutuante, essa ortogonalidade pode ser perdida [^670]. Isso ocorre porque as operações de subtração e divisão podem amplificar pequenos erros, levando a desvios da ortogonalidade ideal [^670].

A perda de ortogonalidade tem um impacto significativo na precisão dos autovalores e autovetores calculados [^670]. Sem ortogonalidade precisa, a matriz tridiagonal $H_n$ não representa mais uma projeção precisa da matriz original $A$ no subespaço de Krylov [^666]. Isso pode levar a autovalores espúrios e autovetores imprecisos [^670].

**Reortogonalização:**
Para mitigar os efeitos da perda de ortogonalidade, técnicas de reortogonalização são empregadas [^670]. A reortogonalização envolve a ortogonalização explícita dos vetores $u_j$ periodicamente durante a iteração [^670]. Existem duas abordagens principais para reortogonalização [^670]:

1.  **Reortogonalização Completa:** Cada novo vetor $u_{n+1}$ é ortogonalizado contra todos os vetores anteriores $u_1, u_2, ..., u_n$ [^670]. Embora esta abordagem seja a mais precisa, ela também é a mais cara computacionalmente, pois requer um número crescente de produtos internos a cada iteração [^670].
2.  **Reortogonalização Seletiva:** O vetor $u_{n+1}$ é ortogonalizado apenas contra os vetores que perderam ortogonalidade significativa [^670]. Esta abordagem requer um método para estimar a perda de ortogonalidade, como monitorar a magnitude dos produtos internos $(u_i, u_j)$ [^670]. A reortogonalização seletiva oferece um bom compromisso entre precisão e custo computacional [^670].

**Reortogonalização Completa Detalhada:**
A reortogonalização completa garante que cada vetor $u_{n+1}$ seja ortogonal a todos os vetores anteriores [^670]. O processo envolve as seguintes etapas [^670]:

1.  Calcular o vetor $z$ como na iteração de Lanczos padrão: $z = Au_n - \beta_{n-1} u_{n-1} - \alpha_n u_n$ [^669].
2.  Para cada $j = 1, 2, ..., n$ [^670]:
    *   Calcular o produto interno $h_{jn} = u_j^* z$ [^665, 670].
    *   Subtrair a componente de $z$ na direção de $u_j$: $z = z - h_{jn} u_j$ [^670].
3.  Calcular a norma de $z$: $\beta_n = ||z||$ [^669].
4.  Se $\beta_n = 0$, interromper a iteração [^669].
5.  Calcular o novo vetor $u_{n+1} = \frac{z}{\beta_n}$ [^669].

Este processo garante que o novo vetor $u_{n+1}$ seja ortogonal a todos os vetores anteriores dentro da precisão da máquina [^670].

**Custo Computacional da Reortogonalização:**
A reortogonalização completa adiciona um custo computacional significativo à iteração de Lanczos. Sem reortogonalização, cada iteração requer um número fixo de operações [^669]. Com reortogonalização completa, o número de operações aumenta linearmente com o número de iterações [^670]. Portanto, para um grande número de iterações, o custo da reortogonalização pode dominar o custo da própria iteração de Lanczos [^670].

### Conclusão
A iteração de Lanczos é uma técnica poderosa para calcular autovalores e autovetores de matrizes simétricas (ou Hermitianas) [^669]. No entanto, a aritmética de ponto flutuante pode levar à perda de ortogonalidade entre os vetores gerados, o que pode afetar a precisão dos resultados [^670]. Técnicas de reortogonalização, como reortogonalização completa e reortogonalização seletiva, podem ser usadas para mitigar esses efeitos [^670]. A escolha da técnica de reortogonalização depende do equilíbrio desejado entre precisão e custo computacional [^670]. Em resumo, ao lidar com problemas de grande escala, é fundamental considerar os efeitos da aritmética de ponto flutuante e empregar estratégias de reortogonalização apropriadas para garantir resultados precisos e confiáveis [^670].
### Referências
[^647]: Capítulo 18, Computing Eigenvalues and Eigenvectors.
[^665]: 18.4. KRYLOV SUBSPACES; ARNOLDI ITERATION.
[^666]: The space Kn(A, b) = Span(b, Ab, . . ., An−1b) is called a Krylov subspace.
[^669]: 18.6. The Hermitian Case; Lanczos Iteration.
[^670]: Also theorems about error estimates exist. The version of Lanczos iteration given above may run into problems in floating point arithmetic. What happens is that the vectors uj may lose the property of being orthogonal, so it may be necessary to reorthogonalize them.
<!-- END -->