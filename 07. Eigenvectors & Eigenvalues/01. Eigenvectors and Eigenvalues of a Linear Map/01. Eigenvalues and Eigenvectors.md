## Eigenvalores: Escalares da Transformação Linear

### Introdução
Este capítulo aprofunda a discussão sobre **eigenvalores**, um conceito central na análise de transformações lineares [^1]. Como vimos anteriormente, transformações lineares podem ser representadas por matrizes, e a análise de eigenvalores nos permite entender como essas transformações atuam em vetores específicos, chamados **eigenvetores** [^1]. O estudo dos eigenvalores é crucial para diagonalização de matrizes e, consequentemente, para simplificar a análise de transformações lineares [^1].

### Conceitos Fundamentais

**Definição Formal de Eigenvalor**
Um **eigenvalor** (autovalor) $\\lambda$ de uma transformação linear $f: E \\rightarrow E$, onde $E$ é um espaço vetorial, é um escalar tal que existe um vetor não nulo $u \\in E$ (chamado **eigenvetor**) que satisfaz a equação [^2]:
$$ f(u) = \\lambda u $$
Esta equação fundamental indica que a transformação linear $f$ aplicada ao eigenvetor $u$ resulta em um vetor que é um múltiplo escalar de $u$. Em outras palavras, $f$ simplesmente estica ou contrai $u$ ao longo de sua direção, sem alterar sua direção [^1].

**Nulidade e Invertibilidade**
Uma formulação equivalente para determinar se $\\lambda$ é um eigenvalor de $f$ é verificar se o núcleo (kernel) da transformação $\\lambda id - f$ é não trivial, ou seja, $\\text{Ker}(\\lambda id - f) \\neq \\{0\\}$ [^2]. Aqui, $id$ denota a transformação identidade em $E$. A condição $\\text{Ker}(\\lambda id - f)$ ser não trivial é equivalente a dizer que $\\lambda id - f$ não é invertível [^2]. Essa condição é particularmente útil em espaços de dimensão finita [^2].

**Eigenspaço**
Para um dado eigenvalor $\\lambda$, o conjunto de todos os eigenvetores associados a $\\lambda$, juntamente com o vetor zero, forma um subespaço vetorial de $E$, chamado **eigenspaço** (autospaço) associado a $\\lambda$, denotado por $E_\\lambda(f)$ ou simplesmente $E_\\lambda$ [^2]. Formalmente:
$$ E_\\lambda(f) = \\{u \\in E \\mid f(u) = \\lambda u\\} = \\text{Ker}(\\lambda id - f) $$
O eigenspaço $E_\\lambda$ consiste de todos os vetores que são apenas escalados pela transformação $f$, juntamente com o vetor zero [^2].

**Polinômio Característico**
Os eigenvalores de uma transformação linear $f: E \\rightarrow E$ estão intimamente relacionados com o **polinômio característico** de $f$. Seja $E$ um espaço vetorial de dimensão finita $n$ e $f$ uma transformação linear [^3]. O polinômio característico de $f$, denotado por $P_f(X)$ ou $\\chi_f(X)$, é definido como [^3]:
$$ P_f(X) = \\chi_f(X) = \\text{det}(X \\cdot id - f) $$
onde $X$ é uma variável escalar e $id$ é a transformação identidade em $E$ [^3]. Os eigenvalores de $f$ são precisamente as raízes do polinômio característico [^3]. Ou seja, $\\lambda$ é um eigenvalor de $f$ se e somente se $P_f(\\lambda) = 0$ [^3].

**Multiplicidade Algébrica e Geométrica**
Seja $A$ uma matriz $n \\times n$ sobre um corpo $K$ e suponha que todas as raízes do polinômio característico $\\chi_A(X) = \\text{det}(XI - A)$ de $A$ pertençam a $K$ [^7]. Isso significa que podemos escrever [^7]:
$$ \\text{det}(XI - A) = (X - \\lambda_1)^{k_1} \\cdots (X - \\lambda_m)^{k_m} $$
onde $\\lambda_1, \\ldots, \\lambda_m \\in K$ são as raízes distintas de $\\text{det}(XI - A)$ e $k_1 + \\ldots + k_m = n$ [^7]. O inteiro $k_i$ é chamado de **multiplicidade algébrica** do eigenvalor $\\lambda_i$, denotada por $\\text{alg}(\\lambda_i)$, e a dimensão do eigenspaço $E_{\\lambda_i} = \\text{Ker}(\\lambda_i I - A)$ é chamada de **multiplicidade geométrica** de $\\lambda_i$, denotada por $\\text{geo}(\\lambda_i)$ [^7].
Por definição, a soma das multiplicidades algébricas é igual a $n$, mas a soma das multiplicidades geométricas pode ser estritamente menor [^7].

**Teorema:** Para cada eigenvalor $\\lambda_i$ de $A$, a multiplicidade geométrica de $\\lambda_i$ é sempre menor ou igual à sua multiplicidade algébrica [^7]:
$$ \\text{geo}(\\lambda_i) \\leq \\text{alg}(\\lambda_i) $$

**Independência Linear de Eigenvetores**
Eigenvetores associados a eigenvalores distintos são linearmente independentes [^7]. Mais formalmente, seja $E$ um espaço vetorial de dimensão finita $n$ e $f$ uma transformação linear [^7]. Se $u_1, \\ldots, u_m$ são eigenvetores associados a eigenvalores distintos $\\lambda_1, \\ldots, \\lambda_m$, então a família $(u_1, \\ldots, u_m)$ é linearmente independente [^7].

### Conclusão

O conceito de eigenvalor fornece uma ferramenta poderosa para analisar a estrutura das transformações lineares. A capacidade de identificar eigenvalores e eigenvetores permite decompor o espaço vetorial em subespaços invariantes, simplificando a análise de transformações complexas. Além disso, o polinômio característico oferece um método sistemático para encontrar eigenvalores, e as multiplicidades algébricas e geométricas fornecem informações adicionais sobre a estrutura dos eigenspaços.

### Referências
[^1]: Capítulo 15, Eigenvectors and Eigenvalues
[^2]: Definition 15.1
[^3]: Proposition 15.1
[^7]: Definition 15.3
<!-- END -->