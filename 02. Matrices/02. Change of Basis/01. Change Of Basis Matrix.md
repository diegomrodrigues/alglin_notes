## Matriz de Mudança de Base e Transformação de Coordenadas

### Introdução

Como estabelecido anteriormente (Seção 4.1), um vetor em um espaço vetorial de dimensão finita $E$ pode ser unicamente representado por um vetor coluna de coordenadas em relação a uma base escolhida $U = (u_1, ..., u_n)$ [^p2_1]. Da mesma forma, uma aplicação linear $f: E \to F$ entre espaços vetoriais de dimensão finita $E$ e $F$ com bases $U$ e $V$, respectivamente, pode ser representada por uma matriz $M(f)$ (ou mais explicitamente $M_{U,V}(f)$) [^p1_1, ^p4_1, ^p8_2]. Uma questão fundamental surge naturalmente: como as coordenadas de um vetor e a representação matricial de uma aplicação linear são afetadas quando mudamos as bases dos espaços vetoriais envolvidos? Este capítulo foca na primeira parte desta questão, introduzindo a **matriz de mudança de base**, uma ferramenta essencial para relacionar as coordenadas de um vetor em diferentes bases. Exploraremos sua definição, propriedades e interpretação.

### Conceitos Fundamentais

#### Definição Formal da Matriz de Mudança de Base

Considere um espaço vetorial $E$ de dimensão finita $n$. Sejam $U = (u_1, ..., u_n)$ e $V = (v_1, ..., v_n)$ duas bases de $E$. A **matriz de mudança de base** da base $U$ para a base $V$ é definida expressando cada vetor da *nova* base $V$ como uma combinação linear dos vetores da *antiga* base $U$.

> **Definição 4.3 (Adaptada):** Dado um espaço vetorial $E$ de dimensão $n$, e duas bases quaisquer $U = (u_1, ..., u_n)$ e $V = (v_1, ..., v_n)$ de $E$, a **matriz de mudança de base** de $U$ para $V$, denotada aqui temporariamente como $P = (a_{ij})$, é a matriz invertível definida tal que:
> $$ v_j = \sum_{i=1}^{n} a_{ij} u_i \quad \text{para cada } j = 1, ..., n $$
> [^p13_1]

É crucial observar a estrutura desta matriz: a $j$-ésima coluna de $P = (a_{ij})$ consiste nas coordenadas do novo vetor de base $v_j$ em relação à antiga base $U = (u_1, ..., u_n)$ [^p13_2].

#### Interpretação como Matriz da Aplicação Identidade

Uma interpretação alternativa e poderosa da matriz de mudança de base $P$ de $U$ para $V$ é que ela representa a aplicação identidade $id: E \to E$ com respeito às bases $V$ e $U$, *nesta ordem específica*. Formalmente, usando a notação introduzida na Definição 4.2 [^p8_2], a matriz $P$ que definimos acima, onde $v_j = \sum a_{ij} u_i$, é precisamente $M_{V,U}(id)$ [^p13_2]. Para evitar ambiguidade e alinhar com a notação posterior [^p15_2], adotaremos a notação $P_{V,U}$ para a matriz de mudança de base da base $U$ para a base $V$.

> **Definição 4.4 (Adaptada):** A matriz de mudança de base da base $U$ para a base $V$, cujas colunas são as coordenadas dos vetores de $V$ na base $U$, é denotada por $P_{V,U}$ [^p15_2]. Esta matriz corresponde a $M_{V,U}(id)$.

Portanto, $P_{V,U} = (a_{ij})$ onde $v_j = \sum_{i=1}^n a_{ij} u_i$.

#### Transformação de Coordenadas

A principal utilidade da matriz de mudança de base $P_{V,U}$ reside em sua capacidade de transformar as coordenadas de um vetor $x \in E$ da base $V$ para a base $U$. Seja $x \in E$. Se $x$ tem coordenadas $x_U = (x_1, ..., x_n)^T$ em relação à base $U$ e $x_V = (x\'_1, ..., x\'_n)^T$ em relação à base $V$, então:
$x = \sum_{i=1}^n x_i u_i = \sum_{j=1}^n x\'_j v_j$.
Substituindo a expressão para $v_j$ em termos de $u_i$:\n$x = \sum_{j=1}^n x\'_j \left( \sum_{i=1}^n a_{ij} u_i \right) = \sum_{i=1}^n \left( \sum_{j=1}^n a_{ij} x\'_j \right) u_i$.\nPela unicidade das coordenadas na base $U$, devemos ter:\n$x_i = \sum_{j=1}^n a_{ij} x\'_j$.\nEsta relação, em forma matricial, é exatamente:\n$$ x_U = P_{V,U} x_V $$\n[^p13_3], [^p14_1], [^p15_3]. Esta equação mostra como as *antigas* coordenadas ($x_U$) são obtidas a partir das *novas* coordenadas ($x_V$) usando a matriz $P_{V,U}$. O texto [^p14_1] fornece uma verificação explícita para $n=2$.\n\nNote que a transformação das coordenadas ocorre na direção "oposta" à mudança de base (de $V$ para $U$ nas coordenadas, enquanto a matriz expressa $V$ em termos de $U$). Por esta razão, as coordenadas de um vetor são por vezes ditas *contravariantes* [^p15_4]. Contudo, como o texto adverte, esta terminologia pode ser enganosa, pois o vetor $x$ em si é uma entidade intrínseca independente da base; são suas *coordenadas* que se transformam desta maneira [^p15_4].

#### Invertibilidade e Mudança de Base Inversa

A Proposição 4.3 estabelece que a matriz $P$ que expressa uma família de vetores $(v_1, ..., v_n)$ em termos de uma base $(u_1, ..., u_n)$ é invertível se, e somente se, a família $(v_1, ..., v_n)$ também for uma base de $E$ [^p12_1]. Como $U$ e $V$ são ambas bases, a matriz de mudança de base $P_{V,U}$ é sempre invertível [^p13_1].

A matriz de mudança de base da *nova* base $V$ para a *antiga* base $U$, denotada $P_{U,V}$, é simplesmente a inversa de $P_{V,U}$:\n$$ P_{U,V} = P_{V,U}^{-1} $$\n[^p13_4], [^p15_2]. Consequentemente, a transformação inversa de coordenadas é dada por:\n$$ x_V = P_{V,U}^{-1} x_U = P_{U,V} x_U $$\n[^p15_3].

#### Relação com Matrizes de Bases no Espaço $K^n$

Quando trabalhamos especificamente em $E = K^n$, e as bases $U=(u_1, ..., u_n)$ e $V=(v_1, ..., v_n)$ são dadas como vetores coluna, podemos formar as matrizes $n \times n$, $U = [u_1 \dots u_n]$ e $V = [v_1 \dots v_n]$, cujas colunas são os vetores das respectivas bases. A relação $v_j = \sum_{i=1}^n a_{ij} u_i$ pode ser escrita matricialmente. Seja $A^j$ a $j$-ésima coluna de $P_{V,U}$ (ou seja, $A^j = (a_{1j}, ..., a_{nj})^T$). Então $v_j$ é a combinação linear das colunas de $U$ com coeficientes $A^j$, o que corresponde a $v_j = U A^j$ [^p14_2]. Reunindo todas as colunas $j=1, ..., n$, obtemos:\n$$ V = [v_1 \dots v_n] = [U A^1 \dots U A^n] = U [A^1 \dots A^n] = U P_{V,U} $$\n[^p15_1]. Assumindo que $U$ é invertível (o que é verdade se $u_i$ formarem uma base de $K^n$), podemos expressar a matriz de mudança de base como:\n$$ P_{V,U} = U^{-1} V $$\n[^p15_1]. Esta fórmula fornece um método prático para calcular $P_{V,U}$ no contexto de $K^n$.

#### Composição de Mudanças de Base

Considere três bases de $E$: $U$, $V$ e $W$. Sejam $P_{V,U}$ a matriz de mudança de base de $U$ para $V$, e $P_{W,V}$ a matriz de mudança de base de $V$ para $W$. Qual é a matriz de mudança de base $P_{W,U}$ de $U$ para $W$?\nPodemos ver isso considerando a composição das aplicações identidade. A mudança de $U$ para $W$ pode ser vista como $id: (E, W) \to (E, U)$, que é a composição $id: (E, V) \to (E, U) \circ id: (E, W) \to (E, V)$. As matrizes correspondentes são $M_{V,U}(id) = P_{V,U}$ e $M_{W,V}(id) = P_{W,V}$. Como a composição de aplicações lineares corresponde à multiplicação das matrizes na ordem inversa (Proposição 4.2 [^p10_1]), temos:\n$M_{W,U}(id) = M_{V,U}(id) M_{W,V}(id)$.\nPortanto, a relação de composição para as matrizes de mudança de base é:\n$$ P_{W,U} = P_{V,U} P_{W,V} $$\nEsta relação é confirmada no texto [^p20_3], embora derivada lá através de uma notação alternativa envolvendo transpostas.

#### Exemplos Concretos

O texto fornece ilustrações práticas do cálculo e uso de matrizes de mudança de base.

*   **Exemplo 4.1:** Em $E = \mathbb{R}^2$, com a base canônica $U = (u_1=(1,0), u_2=(0,1))$ e a nova base $V = (v_1=(1,1), v_2=(-1,1))$ [^p16_1].\n    Expressando $v_1$ e $v_2$ na base $U$:\n    $v_1 = 1 u_1 + 1 u_2$\n    $v_2 = -1 u_1 + 1 u_2$\n    Portanto, a matriz de mudança de base de $U$ para $V$ é $P_{V,U} = \begin{pmatrix} 1 & -1 \\\\ 1 & 1 \end{pmatrix}$.\n    Sua inversa é $P_{U,V} = P_{V,U}^{-1} = \begin{pmatrix} 1/2 & 1/2 \\\\ -1/2 & 1/2 \end{pmatrix}$.\n    A relação de transformação de coordenadas é $x_U = P_{V,U} x_V$ e $x_V = P_{U,V} x_U$ [^p16_1].

*   **Exemplo 4.2:** Em $E = \mathbb{R}[X]_3$, o espaço dos polinômios de grau $\le 3$. A base das potências é $U = (1, x, x^2, x^3)$. A base de Bernstein de grau 3 é $V = (B_0^3(x), B_1^3(x), B_2^3(x), B_3^3(x))$, onde $B_k^3(x) = \binom{3}{k} (1-x)^{3-k} x^k$. O texto fornece a matriz $P_{V,U}$ expandindo os polinômios de Bernstein na base $U$ [^p16_2]:\n    $B_0^3(x) = (1-x)^3 = 1 - 3x + 3x^2 - x^3$\n    $B_1^3(x) = 3(1-x)^2 x = 3x - 6x^2 + 3x^3$\n    $B_2^3(x) = 3(1-x) x^2 = 3x^2 - 3x^3$\n    $B_3^3(x) = x^3$\n    Assim, as colunas de $P_{V,U}$ são $(1, -3, 3, -1)^T$, $(0, 3, -6, 3)^T$, $(0, 0, 3, -3)^T$, $(0, 0, 0, 1)^T$.\n    $$ P_{V,U} = \begin{pmatrix} 1 & 0 & 0 & 0 \\\\ -3 & 3 & 0 & 0 \\\\ 3 & -6 & 3 & 0 \\\\ -1 & 3 & -3 & 1 \end{pmatrix} $$\n    [^p16_2]. A matriz inversa $P_{U,V} = P_{V,U}^{-1}$ também é fornecida [^p16_2], permitindo converter as coordenadas de um polinômio da base $U$ para a base $V$. Por exemplo, para $p(x) = 2x^3 - x + 1$, cujas coordenadas em $U$ são $x_U = (1, -1, 0, 2)^T$, as coordenadas em $V$ são $x_V = P_{U,V} x_U$ [^p16_2].

### Conclusão

A **matriz de mudança de base** $P_{V,U}$ de uma base $U$ para uma base $V$ em um espaço vetorial $E$ é um conceito central em álgebra linear. Ela é definida pelas coordenadas dos novos vetores de base $V$ em termos da antiga base $U$, e pode ser interpretada como a matriz da aplicação identidade $id: E \to E$ em relação às bases $V$ (como base de entrada) e $U$ (como base de saída), i.e., $P_{V,U} = M_{V,U}(id)$. Sua principal aplicação é fornecer a relação entre as coordenadas de um vetor $x$ nas duas bases: $x_U = P_{V,U} x_V$. Esta matriz é sempre invertível, com $P_{V,U}^{-1} = P_{U,V}$. Além disso, as matrizes de mudança de base se compõem de maneira consistente: $P_{W,U} = P_{V,U} P_{W,V}$. A compreensão da matriz de mudança de base é crucial não apenas para transformar coordenadas de vetores, mas também, como explorado na Seção 4.4, para entender como a representação matricial de aplicações lineares se transforma sob mudança de base, levando ao conceito de **similaridade de matrizes** ($B = P^{-1}AP$) [^p17_1, ^p17_2, ^p18_2].

### Referências

[^p1_1]: Chapter 4, Page 113, Proposition 3.18 reference and definition of M(f).
[^p2_1]: Chapter 4, Page 114, Unique representation of vectors x and y in bases.
[^p4_1]: Chapter 4, Page 116, Definition 4.1, Matrix M(f) and representation M(x).
[^p8_2]: Chapter 4, Page 120, Definition 4.2, Notation M_{U,V}(f) and coordinate relation y_V = M_{U,V}(f) x_U.
[^p10_1]: Chapter 4, Page 122, Proposition 4.2, M(f o g) = M(f)M(g).
[^p12_1]: Chapter 4, Page 124, Proposition 4.3, Invertibility of P related to (v1,...,vn) being a basis. Proof using M(f)M(f^-1)=In.
[^p13_1]: Chapter 4, Page 125, Definition 4.3, Formal definition of change of basis matrix P=(a_ij) via v_j = sum a_ij u_i.
[^p13_2]: Chapter 4, Page 125, Interpretation of P as the matrix of identity id: E->E w.r.t bases (v_j) and (u_i). Column structure of P.
[^p13_3]: Chapter 4, Page 125, Coordinate transformation relation derived from P definition (implicit x_U = P x_V).
[^p13_4]: Chapter 4, Page 125, Inverse relation P^-1 for change of basis from V to U.
[^p14_1]: Chapter 4, Page 126, Derivation and check of coordinate transformation x_i = sum a_ij x\'_j for n=2, matrix form x = P x\'.
[^p14_2]: Chapter 4, Page 126, Matrix form v_j = U A^j for vectors in K^n.
[^p15_1]: Chapter 4, Page 127, Derivation V = UP and P = U^-1 V for vectors in K^n.
[^p15_2]: Chapter 4, Page 127, Definition 4.4, Notation P_{V,U} for change of basis matrix from U to V. Relation P_{U,V} = P_{V,U}^-1. Link to M_{V,U}(id).
[^p15_3]: Chapter 4, Page 127, Coordinate transformation rules x_U = P_{V,U} x_V and x_V = P_{V,U}^-1 x_U = P_{U,V} x_U.
[^p15_4]: Chapter 4, Page 127, Mention of contravariant coordinates and caveat.
[^p16_1]: Chapter 4, Page 128, Example 4.1, Change of basis in R^2. Calculation of P and P^-1, coordinate transformation equations.
[^p16_2]: Chapter 4, Page 128, Example 4.2, Change of basis from power basis to Bernstein basis in R[X]_3. Calculation of P_{V,U} and P_{U,V}, coordinate transformation example.
[^p17_1]: Chapter 4, Page 129, Proposition 4.5, Transformation rule for matrix of linear map f: E->F: M\'(f) = Q^-1 M(f) P. Explicit notation Mu\',v\'(f) = P_v\',v^-1 Mu,v(f) P_u\',u.
[^p17_2]: Chapter 4, Page 129, Corollary 4.6, Transformation rule for f: E->E: M\'(f) = P^-1 M(f) P. Explicit notation Mu\'(f) = P_u,u\' Mu(f) P_u\',u.
[^p18_1]: Chapter 4, Page 130, Example 4.3, Diagonalization example illustrating similarity transformation.
[^p18_2]: Chapter 4, Page 130, Definition 4.5, Definition of similar matrices B = P^-1 A P. Link to representing the same linear map.
[^p19_1]: Chapter 4, Page 131, Matrix notation extension V = U P^T (using row vectors implicitly or transpose).
[^p20_1]: Chapter 4, Page 132, Remark on avoiding transposition using row vector notation (v1 ... vn) = (u1 ... un) P.
[^p20_2]: Chapter 4, Page 132, Definition of product of row of vectors by column of scalars.
[^p20_3]: Chapter 4, Page 132, Proof of composition rule P_w,u = P_v,u P_w,v using transposed matrix notation.

<!-- END -->