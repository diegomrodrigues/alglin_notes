## Capítulo 4.2.1: Bilinearidade da Multiplicação Matricial e a Transposta de Aplicações Lineares

### Introdução

Como estabelecido anteriormente (Seção 4.1), existe uma correspondência fundamental entre aplicações lineares $f: E \to F$ entre espaços vetoriais de dimensão finita e matrizes $M(f)$ [^2], [^4]. Vimos que, dadas bases $\mathcal{U} = (u_j)_{j \in J}$ para $E$ e $\mathcal{V} = (v_i)_{i \in I}$ para $F$, a aplicação linear $f$ é unicamente determinada pelas imagens dos vetores da base, $f(u_j)$, cujas coordenadas em relação à base $\mathcal{V}$ formam as colunas da matriz $M(f)$ [^1], [^4]. Além disso, a composição de aplicações lineares $f: F \to G$ e $g: E \to F$ corresponde à multiplicação das matrizes que as representam, $M(f \circ g) = M(f)M(g)$ (Seção 4.2) [^6], [^7], [^8]. Este capítulo aprofunda essa conexão, explorando duas propriedades cruciais: a **bilinearidade** da multiplicação de matrizes, que permite transferir propriedades estruturais das aplicações lineares para o domínio matricial [^9], e o conceito da **aplicação transposta** $f^T$, que conecta uma aplicação linear ao seu comportamento nos espaços duais e é representada pela matriz transposta $A^T$ [^11], [^12].

### Conceitos Fundamentais

#### A Bilinearidade da Multiplicação Matricial e Suas Implicações

A correspondência entre composição de aplicações e multiplicação de matrizes é a chave para entender as propriedades algébricas da multiplicação matricial. A Proposição 4.1 formaliza isso [^9].

**Proposição 4.1 (Resumida):**
1.  Dados matrizes $A \in M_{m,n}(K)$, $B \in M_{n,p}(K)$, e $C \in M_{p,q}(K)$, temos $(AB)C = A(BC)$. Ou seja, a multiplicação de matrizes é **associativa**.
2.  Dados matrizes $A, B \in M_{m,n}(K)$, e $C, D \in M_{n,p}(K)$, e para todo $\lambda \in K$, temos:
    $(A + B)C = AC + BC$
    $A(C + D) = AC + AD$
    $(\lambda A)C = \lambda(AC)$
    $A(\lambda C) = \lambda(AC)$
    Portanto, a multiplicação de matrizes $\cdot: M_{m,n}(K) \times M_{n,p}(K) \to M_{m,p}(K)$ é **bilinear** [^9].

A prova dessas propriedades ilustra elegantemente como a conexão com aplicações lineares é explorada. Para qualquer matriz $A \in M_{m,n}(K)$, podemos definir uma aplicação linear $f_A: K^n \to K^m$ dada por $f_A(x) = Ax$, onde $x$ é um vetor coluna em $K^n$ [^9]. Verifica-se imediatamente que $f_A$ é linear e que a matriz que representa $f_A$ em relação às bases canônicas de $K^n$ e $K^m$ é precisamente $A$ [^9]. Como vimos na Seção 4.2, a fórmula (4) demonstra que $M(f_A \circ f_B) = M(f_A)M(f_B) = AB$ [^8], [^9].

A associatividade $(AB)C = A(BC)$ segue diretamente da associatividade da composição de funções: $(f_A \circ f_B) \circ f_C = f_A \circ (f_B \circ f_C)$ [^10]. Aplicando o mapeamento $M$, obtemos $M((f_A \circ f_B) \circ f_C) = M(f_A \circ f_B)M(f_C) = (AB)C$ e $M(f_A \circ (f_B \circ f_C)) = M(f_A)M(f_B \circ f_C) = A(BC)$, o que implica $(AB)C = A(BC)$ [^10].

A bilinearidade pode ser demonstrada de forma similar. Por exemplo, para provar $(A + B)C = AC + BC$, note que as aplicações lineares associadas satisfazem $f_{A+B} = f_A + f_B$ [^10]. Então:
$$ (A + B)C = M(f_{A+B})M(f_C) $$
Usando a propriedade de que $M$ preserva a estrutura de espaço vetorial (Proposição 4.2 [^10]), $M(f_{A+B}) = M(f_A + f_B) = M(f_A) + M(f_B) = A + B$. No entanto, a prova na página 122 [^10] segue uma linha ligeiramente diferente, focando na composição:
$$ (A + B)C = M(f_{A+B})M(f_C) = M(f_A + f_B)M(f_C) $$
$$ = M((f_A + f_B) \circ f_C) $$
Pela linearidade da composição (ou seja, $(g+h)\circ k = g \circ k + h \circ k$), temos:
$$ = M((f_A \circ f_C) + (f_B \circ f_C)) $$
Como $M$ é um isomorfismo de espaços vetoriais (Proposição 4.2 [^10]), $M(g+h) = M(g) + M(h)$, então:
$$ = M(f_A \circ f_C) + M(f_B \circ f_C) $$
$$ = M(f_A)M(f_C) + M(f_B)M(f_C) = AC + BC $$
As outras identidades de bilinearidade são verificadas de maneira análoga [^10].

> *"Isto nos permite transferir propriedades de aplicações lineares para matrizes."* [^9]

Uma consequência importante da associatividade e bilinearidade, juntamente com a existência de uma matriz identidade $I_n$, é que o espaço vetorial $M_n(K)$ das matrizes quadradas forma um **anel** (não comutativo) com unidade $I_n$. De fato, $M_n(K)$ é uma **álgebra associativa** sobre $K$ [^10].

#### A Transposta de uma Aplicação Linear e Espaços Duais

A noção de transposição não se limita a matrizes; ela tem uma contraparte conceitual importante no nível das aplicações lineares, envolvendo os **espaços duais**. Lembremos que o espaço dual $E^*$ de um espaço vetorial $E$ é o espaço de todas as formas lineares (funcionais lineares) $f^*: E \to K$. Se $E$ tem dimensão finita com base $\mathcal{U} = (u_1, \dots, u_n)$, uma forma linear $f^* \in E^*$ pode ser representada por um vetor linha de coeficientes $(f^*(u_1) \dots f^*(u_n))$ [^11].

Considere uma aplicação linear $f: E \to F$. Ela induz uma aplicação linear $f^T: F^* \to E^*$, denominada a **transposta** de $f$ [^11]. Note a inversão na direção: $f^T$ mapeia o dual de $F$ para o dual de $E$. A conexão com a matriz transposta é direta:

> Se $\mathcal{U} = (u_1, \dots, u_n)$ é uma base de $E$, $\mathcal{V} = (v_1, \dots, v_m)$ é uma base de $F$, e se $f$ é representada pela matriz $A = (a_{ij}) \in M_{m,n}(K)$ em relação a essas bases (onde $f(u_j) = \sum_{i=1}^m a_{ij} v_i$ [^4]), então a aplicação transposta $f^T$ é representada pela matriz transposta $A^T \in M_{n,m}(K)$ em relação às bases duais $\mathcal{V}^* = (v_1^*, \dots, v_m^*)$ de $F^*$ e $\mathcal{U}^* = (u_1^*, \dots, u_n^*)$ de $E^*$ [^11].

Vamos detalhar a derivação apresentada no contexto [^11], [^12]. Uma forma linear $\varphi \in F^*$ é representada em relação à base $\mathcal{V}$ pelo vetor linha $\lambda = (\varphi(v_1) \dots \varphi(v_m))$ [^11]. A aplicação $f^T$ é definida de tal forma que $f^T(\varphi)$ é a forma linear em $E^*$ representada, em relação à base $\mathcal{U}$, pelo vetor linha $\lambda A$ [^11].
Agora, consideremos a representação em relação às bases duais. A forma linear $\varphi \in F^*$ é representada pelo vetor coluna $\lambda^T$ em relação à base dual $\mathcal{V}^*$. A questão é qual vetor coluna representa $f^T(\varphi)$ em relação à base dual $\mathcal{U}^*$. Como $f^T(\varphi)$ é representado pelo vetor linha $\lambda A$ em relação à base $\mathcal{U}$, sua representação como vetor coluna em relação à base dual $\mathcal{U}^*$ é a transposta desse vetor linha, ou seja, $(\lambda A)^T$ [^11], [^12]. Usando a propriedade da transposta de um produto, $(\lambda A)^T = A^T \lambda^T$ [^12].

Portanto, a transformação nas coordenadas (representadas como vetores coluna nas bases duais) é dada por $\lambda^T \mapsto A^T \lambda^T$. Isso confirma que a matriz que representa $f^T: F^* \to E^*$ em relação às bases duais $\mathcal{V}^*$ e $\mathcal{U}^*$ é precisamente $A^T$ [^12].

Conceitualmente, a aplicação transposta $f^T: F^* \to E^*$ é definida pela composição [^12]:
$$ f^T(\varphi) = \varphi \circ f $$
para toda $\varphi \in F^*$. Como $\varphi: F \to K$ e $f: E \to F$, a composição $\varphi \circ f$ é uma aplicação de $E$ para $K$. Sendo $f$ e $\varphi$ lineares, a composição $\varphi \circ f$ também é linear, ou seja, $\varphi \circ f \in E^*$. Esta definição $f^T(\varphi) = \varphi \circ f$ é fundamental e independente de bases, conectando a ação de $f$ nos vetores com a ação de $f^T$ nos funcionais lineares [^12].

### Conclusão

Este capítulo destacou duas conexões profundas entre aplicações lineares e matrizes. Primeiramente, a **bilinearidade** da multiplicação de matrizes, uma consequência direta da linearidade da composição de aplicações, permite-nos usar a estrutura algébrica das matrizes para analisar e computar propriedades das transformações lineares de forma eficiente [^9], [^10]. Essa propriedade fundamenta a estrutura de anel (e álgebra) do espaço de matrizes quadradas. Em segundo lugar, o conceito de **aplicação transposta** $f^T: F^* \to E^*$ [^11], representada pela matriz transposta $A^T$ em relação às bases duais [^12], estabelece uma ligação fundamental entre uma aplicação linear e sua ação sobre funcionais lineares. Isso não apenas fornece uma interpretação conceitual para a transposta da matriz, mas também abre a porta para o estudo de relações importantes entre um espaço vetorial e seu dual, e como as aplicações lineares interagem com essa dualidade. Ambas as propriedades são ferramentas essenciais na análise e computação em álgebra linear avançada.

### Referências

[^1]: Página 113, parágrafo 1: A Proposição 3.18 mostra que uma aplicação linear $f: E \to F$ é unicamente determinada pelas imagens $f(u_j)$ dos vetores da base $(u_j)_{j \in J}$ de $E$.
[^2]: Página 113, parágrafo 2: A aplicação linear $f$ é completamente determinada pela matriz $M(f) = (a_{ij})_{(i,j) \in I \times J}$, onde $f(u_j) = \sum_{i \in I} a_{ij} v_i$.
[^3]: Página 114: Exemplo de representação matricial e cálculo de $f(x)$.
[^4]: Página 116, Definition 4.1: Definição formal da matriz $M(f)$ de uma aplicação linear $f: E \to F$ em relação às bases $(u_1, \dots, u_n)$ de $E$ e $(v_1, \dots, v_m)$ de $F$. A $j$-ésima coluna de $M(f)$ contém os coeficientes de $f(u_j)$ na base $(v_i)$.
[^5]: Página 117: Exemplo concreto de representação matricial para o operador de derivação $d$ e integração $\int$ em espaços de polinômios.
[^6]: Página 118, Seção 4.2: Introdução à composição de aplicações lineares $f: F \to G$ e $g: E \to F$, resultando em $f \circ g: E \to G$.
[^7]: Página 119: Derivação mostrando como as coordenadas $z_i$ de $z = f(g(x))$ são obtidas a partir das coordenadas $x_j$ de $x$.
[^8]: Página 120, Equação (4): $z_i = \sum_{j=1}^p c_{ij} x_j$, onde $c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}$, mostrando que a composição corresponde ao produto de matrizes $C = AB$.
[^9]: Página 121, Proposição 4.1: Enuncia a associatividade e bilinearidade da multiplicação de matrizes. Inicia a prova usando a associação entre matrizes $A$ e aplicações lineares $f_A(x) = Ax$. Afirma que $M(f_A \circ f_B) = M(f_A)M(f_B) = AB$.
[^10]: Página 122: Continua a prova da Proposição 4.1, demonstrando associatividade e bilinearidade (ex: $(A+B)C = AC+BC$) usando a correspondência com aplicações lineares e suas composições. Menciona que $M_n(K)$ é um anel (e álgebra associativa). Enuncia a Proposição 4.2 sobre o isomorfismo $M: \text{Hom}(E, F) \to M_{m,n}$.
[^11]: Página 123: Explica a representação de formas lineares $f^*: E \to K$ por vetores linha. Introduz a aplicação transposta $f^T: F^* \to E^*$ induzida por $f: E \to F$. Afirma que se $f$ é representada por $A$, então $f^T$ é representada por $A^T$ em relação às bases duais. Define $f^T(\varphi)$ como a forma linear representada pelo vetor linha $\lambda A$, onde $\lambda$ representa $\varphi$.
[^12]: Página 124: Mostra que $f^T(\varphi)$ é representada pelo vetor coluna $A^T \lambda^T$ em relação à base dual de $E^*$, confirmando que a matriz de $f^T$ é $A^T$. Apresenta a definição conceitual $f^T(\varphi) = \varphi \circ f$.

<!-- END -->